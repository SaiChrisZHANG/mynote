\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{13}{Non-convex Learning + Lasso}{}{Sai Zhang}{Combining the best of the two, we can use \textbf{Lasso plus Concave} method, with Lasso screening and concave component selecting variables, achieving a coordinated intrinsic two-scale learning.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}

We are facing a tradeoff:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Convex}} methods: have appealing \underline{prediction power and oracle inequalities}, but challenging to provide tight \underline{false sign rate control}
    \item \myhl[myred]{\textbf{Concave}} methods: have good \underline{variable selection} properties, but challenging to establish \underline{global} properties and risk properties
\end{itemize}

Here, we take advantage of the linearity of Lasso (convex \textit{and} concave) and try to combine it with concave regularization to get the best of both.

\section{Model Setup}
Again, consider a linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, where 
\begin{itemize}
    \item response vector ($n\times 1$): $\mathbf{y} = (y_1,\cdots,y_n)^{\prime}$
    \item design matrix ($n\times p$): $\mathbf{X} = (\mathbf{x}_1,\cdots,\mathbf{x}_p)$
\end{itemize}
here, we consider a scenario where
\begin{itemize}
    \item $\boldsymbol{\beta}_0 = (\beta_{0,1},\cdots,\beta_{0,p})^{\prime}$ is \textit{\textbf{sparse}} (with many 0 components)
\end{itemize}

%\newpage
%\bibliographystyle{plainnat}
%\bibliography{ref.bib}

\end{document}