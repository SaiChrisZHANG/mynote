\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{13}{Non-convex Learning + Lasso}{}{Sai Zhang}{Combining the best of the two, we can use \textbf{Lasso plus Concave} method, with Lasso screening and concave component selecting variables, achieving a coordinated intrinsic two-scale learning.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}

We are facing a tradeoff:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Convex}} methods: have appealing \underline{prediction power and oracle inequalities}, but challenging to provide tight \underline{false sign rate control}
    \item \myhl[myred]{\textbf{Concave}} methods: have good \underline{variable selection} properties, but challenging to establish \underline{global} properties and risk properties
\end{itemize}

Here, we take advantage of the linearity of Lasso (convex \textit{and} concave) and try to combine it with concave regularization to get the best of both.

\section{Model Setup}
Again, consider a linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, where 
\begin{itemize}
    \item response vector ($n\times 1$): $\mathbf{y} = (y_1,\cdots,y_n)^{\prime}$
    \item design matrix ($n\times p$): $\mathbf{X} = (\mathbf{x}_1,\cdots,\mathbf{x}_p)$, with each column rescaled to have $L_2-$norm $n^{1/2}$
\end{itemize}
here, we consider a scenario where
\begin{itemize}
    \item $\boldsymbol{\beta}_0 = (\beta_{0,1},\cdots,\beta_{0,p})^{\prime}$ is \textit{\textbf{sparse}} (with many 0 components)
    \item ultra-\textbf{high} dimensions: $\log p=O(n^a)$, for some $0<a<1$
\end{itemize}
and consider the penalized least squares 
\begin{equation}\label{eq:penalized-LS-l1concave}
    \min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
\end{equation}
where 
\begin{itemize}
    \item $\lambda_0 = c\left(\frac{\log p}{n}\right)^{1/2}$ for some $c>0$
    \item $p_{\lambda}(\boldsymbol{\beta}) = p_{\lambda}(\lvert\boldsymbol{\beta}\rvert ) = (p_{\lambda}(\lvert \beta_1 \rvert),\cdots,p_{\lambda}(\lvert \beta_p \rvert))^{\prime}$, with $\lvert\boldsymbol{\beta} \rvert = (\lvert \beta_1 \rvert,\cdots,\lvert \beta_p \rvert)^{\prime}$; the concave penalty $p_{\lambda}(t)$ is defined on $t\in\left[0,\infty \right)$, indexed by $\lambda\geq 0$, increasing in \textbf{\underline{both}} $t$ and $\lambda$, $p_{\lambda}(0)=0$
\end{itemize}
the 2 penalty components
\begin{itemize}
    \item $L_1-$component: minimum amount of regularization for \underline{\textit{removing noise}} in prediction 
    \item concave component $\lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1$: adapt model sparsity for \underline{\textit{variable selection}}
\end{itemize}

Under this set up, we can derive the hard-thresholding property as 
\begin{proposition}{Hard-Thresholding Property}{hard-thresholding-nonconvex}
    Assume the $p_{\lambda}(t)$, $t\geq 0$, is \myhl[myblue]{\textbf{increasing and concave}} with 
    \begin{itemize}
        \item $p_{\lambda}(t)\geq p_{H,\lambda}(t) = \frac{1}{2}\left[\lambda^2-(\lambda-t)^2_+\right]$ on $[0,\lambda]$
        \item $p'_{\lambda}\left((1-c_1)\lambda\right)\leq c_1\lambda$ for some $c_1\in [0,1)$
        \item $-p''_{\lambda}(t)$ decreasing on $[0,(1-c_1)\lambda]$
    \end{itemize}
    then any \underline{local minimizer} of \ref{eq:penalized-LS-l1concave} that is also a \underline{global minimizer} in each coordinate has the \textbf{{hard-thresholding}} feature that each component is either $0$ or of magnitude \textbf{larger} than $(1-c_1)\lambda$
\end{proposition}
Such property is shared by a wide class of concave penalties, including hard-thresholding penalty $p_{H,\lambda}(t)$ with $c_1=0$, $L_0-$penalty, and SICA (with suitable $c_1$).

\paragraph*{How to \textit{\underline{understand}} this proposition?} Let $\hat{\boldsymbol{\beta}}=\left(\hat{\beta}_1,\cdots,\hat{\beta}_p\right)^{\prime}$, then \myhl[myred]{\textbf{each} $\hat{\beta}_j$} is the glocal minimizer of the corresponding univariate penalized least-square problem along the $j-$th coordinate. These univariate problems share a common form with (generally) different scalars $z$ $$ \hat{\beta}(z) = \arg\min_{\beta\in\mathbb{R}}\left\{ \frac{1}{2}(z-\beta)^2 + \lambda_0\lvert\beta\rvert + p_{H,\lambda}(\lvert \beta \rvert) \right\} $$
after we rescale all covariates to have $L_2-$norm $n^{1/2}$. The solution to these univariate problems are $$ \hat{\beta}(z) = \mathrm{sgn}(z)(\lvert z \rvert-\lambda_0)\cdot\mathbf{1}_{\lvert z\rvert > \lambda+\lambda_0} $$
these solutions have the same feature as the hard-thresholded estimator: each component is either 0 or of magnitude larger than $\lambda$. This provides a better distinction between insignificant and significant covariates then soft-thresholding by $L_1$ penalty.


With the hard-thresholding property of Prop. \ref{prop:hard-thresholding-nonconvex}, we can prove a basic constraint for the global optimum $\hat{\boldsymbol{\beta}}$ on an event with significant probability \citep{fan2014asymptotic}
\begin{equation}\label{eq:basic_constrant}
    \lVert \boldsymbol{\delta}_2 \rVert _1 \leq 7 \lVert \boldsymbol{\delta}_1 \rVert _1
\end{equation}
where $\boldsymbol{\delta} = \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 = \left( \hat{\boldsymbol{\beta}}_1',\hat{\boldsymbol{\beta}}_2' \right)' - \left( {\boldsymbol{\beta}}_{0,1}',{\boldsymbol{\beta}}_{0,2}' \right)' = \left( \boldsymbol{\delta}^{\prime}_1,\boldsymbol{\delta}^{\prime}_2 \right)^{\prime}$, with $\boldsymbol{\delta}_1\in\mathbb{R}^s$. Where does this constraint come from? For the penalized least square quesion \ref{eq:penalized-LS-l1concave}
$$
\min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
$$
the global minimizer $\hat{\boldsymbol{\beta}}$ leads to 
\begin{align*}
    (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 =& (2n)^{-1}\lVert \mathbf{X}\boldsymbol{\beta}_0+ \boldsymbol{\epsilon} -\mathbf{X}\hat{\boldsymbol{\beta}} \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 \\
    =& (2n)^{-1}\lVert \boldsymbol{\epsilon} -\mathbf{X}(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}_0) \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 \\
    \leq &  (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}{\boldsymbol{\beta}_0} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1\\
    =& (2n)^{-1}\lVert {\boldsymbol{\epsilon}} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1
\end{align*}

then, plug in $\boldsymbol{\delta} = \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 $, we get
\begin{align*}
    (2n)^{-1}\lVert \boldsymbol{\epsilon} -\mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta}_0+\boldsymbol{\delta} \rVert _1 + \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1  &\leq  (2n)^{-1}\lVert {\boldsymbol{\epsilon}} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 \\
    (2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0 \lVert \boldsymbol{\beta}_0+\boldsymbol{\delta} \rVert _1 + \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1  &\leq  \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1
\end{align*}
since $\boldsymbol{\beta}_{0,2}=\mathbf{0}$, $\boldsymbol{\delta}_2=\boldsymbol{\beta}_{0,2}+\boldsymbol{\delta}_2$, we have $$ \lVert {\boldsymbol{\beta}_{0}} + \boldsymbol{\delta} \rVert _1 = \lVert {\boldsymbol{\beta}_{0,1}} + {\boldsymbol{\beta}_{0,2}} + \boldsymbol{\delta}_1 + \boldsymbol{\delta}_2 \rVert _1 = \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 + \boldsymbol{\delta}_2 \rVert _1 \leq  \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 \rVert _1 +  \lVert \boldsymbol{\delta}_2 \rVert _1 $$
hence
$$
(2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 \leq  \lambda_0 \lVert {\boldsymbol{\beta}_{0,1}} \rVert _1 - \lambda_0 \lVert \boldsymbol{\beta}_{0,1}+\boldsymbol{\delta}_1 \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
$$
and by the reverse triangle inequality $ \lVert {\boldsymbol{\beta}_{0,1}} \rVert _1 - \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 \rVert _1 \leq \lVert \boldsymbol{\delta}_1 \rVert _1 $, we get 
$$
(2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 \leq  \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
$$
If assume the distribution of the model error $\boldsymbol{\epsilon}$ as 
$$
\Pr \left( \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty} >\frac{\lambda_0}{2} \right) = O\left(p^{-c_0}\right)
$$
conditional on the event $\mathcal{E} = \left\{ \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty}\leq \lambda_0 /2 \right\}$, we have 
$$
 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 - \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 \geq -\frac{\lambda_0}{2}\lVert \boldsymbol{\delta} \rVert _1 + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 - \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 = \frac{\lambda_0}{2} \lVert \boldsymbol{\delta}_2 \rVert _1 - \frac{3\lambda_0}{2} \lVert \boldsymbol{\delta}_1 \rVert _1
$$
plug this result back, get
\begin{equation}\label{eq:basic_constrant_step1}
    \frac{1}{2n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \frac{\lambda_0}{2} \lVert \boldsymbol{\delta}_2 \rVert _1 \leq \frac{3\lambda_0}{2} \lVert \boldsymbol{\delta}_1 \rVert _1+ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
\end{equation}
Now, if we further impose 2 conditions:
\begin{itemize}
    \item {\myhl[myblue]{\textbf{eigenvalue condition}}}: for some positive constant $\kappa_0$ 
    \begin{align*}
        \min_{\lVert\boldsymbol{\delta}\rVert _2=1,\lVert\boldsymbol{\delta}\rVert _0<2s}\frac{1}{\sqrt{n}}\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2 &\geq \kappa_0 & \textbf{(A)} \\
        \kappa = \kappa(s,7) = \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1} \left\{ \frac{1}{\sqrt{n}}\frac{\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2}{\lVert\boldsymbol{\delta}_1\rVert _2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2} \right\} &> 0 & \textbf{(B)}
    \end{align*}
    where $\tilde{\boldsymbol{\delta}}_2$ is the subvector of $\boldsymbol{\delta}_2$ consisting of the components with the $s$ \textbf{\underline{largest}} absolute values. Here
    \begin{itemize}
        \item[-] Condition \textbf{(A)} is a mild sparse eigenvalue condition
        \item[-] Condition \textbf{(B)} combines the restricted eigenvalue assumptions in \citet{bickel2009simultaneous}\footnote{Introduced by \citet{candes2007dantzig} for studying the oracle inequalities for the Lasso estimator and Dantzig selector.}. The intuition is, for OLS estimation, $\mathbf{X}^{\prime}\mathbf{X}$ should be \textbf{\underline{positive definite}}, that is $$ \min_{\mathbf{0}\neq \boldsymbol{\delta}\in\mathbb{R}^p}\left\{ \frac{1}{\sqrt{n}} \frac{\lVert  \mathbf{X} \boldsymbol{\delta} \rVert _2}{\lVert \boldsymbol{\delta} \rVert _2} \right\} >0 $$
        however, when $p>n$, this condition \textbf{never} holds, hence we replace $\lVert \boldsymbol{\delta} \rVert _2$ with the $L_2-$norm of $\lVert \boldsymbol{\delta}_1 \rVert _2$, a subvector of $\boldsymbol{\delta}$ 
        $$ \kappa = \kappa(s,7) \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1}\left\{ \frac{1}{\sqrt{n}} \frac{\lVert  \mathbf{X} \boldsymbol{\delta} \rVert _2}{\lVert \boldsymbol{\delta} \rVert _2} \right\} >0 $$
        and for $L_q$ loss with $q\in (1,2]$, we further bound $ \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2$, which leads to condition \textbf{(B)}.
    \end{itemize}
    \item {\myhl[myblue]{\textbf{hard-thresholding condition}}}: The penalty $p_{\lambda}(t)$ satisfies the conditions of Prop. \ref{prop:hard-thresholding-nonconvex} with 
    \begin{align*}
        p'_{\lambda}\left\{ (1-c_1) \lambda \right\} &\leq \lambda_0/4 \\
        \min_{j=1,\cdots,s}\lvert\beta_{0,j}\rvert &> \max\left\{ (1-c_1)\lambda,2\kappa_0^{-1}p_{\lambda}^{1/2}(\infty) \right\}
    \end{align*} 
\end{itemize}

Now, look back at the condition \ref{eq:basic_constrant_step1}, we can upper-bound $ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1$ by $\frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1$. Consider 2 cases:
\begin{itemize}
    \item \textbf{Case 1}: $\lVert \hat{\boldsymbol{\beta}}_0 \rVert\geq s$. By the hard-thresholding condition, we have $\lvert\beta_{0,j}\rvert>(1-c_1)\lambda$ and $p'_{\lambda}\left\{ (1-c_1)\lambda \right\}\leq \lambda_0/4$. Hence, $\forall j=1,\cdots,s$, if $\hat{\beta}_j\neq 0$, we must have $\lvert \hat{\beta}_j \rvert >(1-c_1)\lambda$. And by the \underline{mean-value theorem}, we have 
    $$\lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j} \rvert) \rvert = p'_{\lambda}(b)(\lvert \hat{\beta}_{j} \rvert - \lvert \beta_{0,j} \rvert)\leq p'_{\lambda}(b)\lvert \delta_{0,j} \rvert$$ 
    where $b$ is between $\lvert \beta_{0,j} \rvert$ and $\lvert \hat{\beta}_j \rvert$, hence, $b>\lvert\beta_{0,j}\rvert>(1-c_1)\lambda$, by the concavity of $p_{\lambda}$, we have $p'(b)<p'((1-c_1)\lambda)\leq \lambda_0/4$, which leads to $\lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j} \rvert) \rvert$
\end{itemize}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}