\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{13}{Non-convex Learning + Lasso}{}{Sai Zhang}{Combining the best of the two, we can use \textbf{Lasso plus Concave} method, with Lasso screening and concave component selecting variables, achieving a coordinated intrinsic two-scale learning.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}

We are facing a tradeoff:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Convex}} methods: have appealing \underline{prediction power and oracle inequalities}, but challenging to provide tight \underline{false sign rate control}
    \item \myhl[myred]{\textbf{Concave}} methods: have good \underline{variable selection} properties, but challenging to establish \underline{global} properties and risk properties
\end{itemize}

Here, we take advantage of the linearity of Lasso (convex \textit{and} concave) and try to combine it with concave regularization to get the best of both.

\section{Model Setup}
Again, consider a linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, where 
\begin{itemize}
    \item response vector ($n\times 1$): $\mathbf{y} = (y_1,\cdots,y_n)^{\prime}$
    \item design matrix ($n\times p$): $\mathbf{X} = (\mathbf{x}_1,\cdots,\mathbf{x}_p)$, with each column rescaled to have $L_2-$norm $n^{1/2}$
\end{itemize}
here, we consider a scenario where
\begin{itemize}
    \item $\boldsymbol{\beta}_0 = (\beta_{0,1},\cdots,\beta_{0,p})^{\prime}$ is \textit{\textbf{sparse}} (with many 0 components)
    \item ultra-\textbf{high} dimensions: $\log p=O(n^a)$, for some $0<a<1$
\end{itemize}
and consider the penalized least squares 
\begin{equation}\label{eq:penalized-LS-l1concave}
    \min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
\end{equation}
where 
\begin{itemize}
    \item $\lambda_0 = c\left(\frac{\log p}{n}\right)^{1/2}$ for some $c>0$
    \item $p_{\lambda}(\boldsymbol{\beta}) = p_{\lambda}(\lvert\boldsymbol{\beta}\rvert ) = (p_{\lambda}(\lvert \beta_1 \rvert),\cdots,p_{\lambda}(\lvert \beta_p \rvert))^{\prime}$, with $\lvert\boldsymbol{\beta} \rvert = (\lvert \beta_1 \rvert,\cdots,\lvert \beta_p \rvert)^{\prime}$; the concave penalty $p_{\lambda}(t)$ is defined on $t\in\left[0,\infty \right)$, indexed by $\lambda\geq 0$, increasing in \textbf{\underline{both}} $t$ and $\lambda$, $p_{\lambda}(0)=0$
\end{itemize}
the 2 penalty components
\begin{itemize}
    \item $L_1-$component: minimum amount of regularization for \underline{\textit{removing noise}} in prediction 
    \item concave component $\lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1$: adapt model sparsity for \underline{\textit{variable selection}}
\end{itemize}

Under this set up, we can derive the hard-thresholding property as 
\begin{proposition}{Hard-Thresholding Property}{hard-thresholding-nonconvex}
    Assume the $p_{\lambda}(t)$, $t\geq 0$, is \myhl[myblue]{\textbf{increasing and concave}} with 
    \begin{itemize}
        \item $p_{\lambda}(t)\geq p_{H,\lambda}(t) = \frac{1}{2}\left[\lambda^2-(\lambda-t)^2_+\right]$ on $[0,\lambda]$
        \item $p'_{\lambda}\left((1-c_1)\lambda\right)\leq c_1\lambda$ for some $c_1\in [0,1)$
        \item $-p''_{\lambda}(t)$ decreasing on $[0,(1-c_1)\lambda]$
    \end{itemize}
    then any \underline{local minimizer} of \ref{eq:penalized-LS-l1concave} that is also a \underline{global minimizer} in each coordinate has the \textbf{{hard-thresholding}} feature that each component is either $0$ or of magnitude \textbf{larger} than $(1-c_1)\lambda$
\end{proposition}
Such property is shared by a wide class of concave penalties, including hard-thresholding penalty $p_{H,\lambda}(t)$ with $c_1=0$, $L_0-$penalty, and SICA (with suitable $c_1$).

\paragraph*{How to \textit{\underline{understand}} this proposition?} Let $\hat{\boldsymbol{\beta}}=\left(\hat{\beta}_1,\cdots,\hat{\beta}_p\right)^{\prime}$, then \myhl[myred]{\textbf{each} $\hat{\beta}_j$} is the glocal minimizer of the corresponding univariate penalized least-square problem along the $j-$th coordinate. These univariate problems share a common form with (generally) different scalars $z$ $$ \hat{\beta}(z) = \arg\min_{\beta\in\mathbb{R}}\left\{ \frac{1}{2}(z-\beta)^2 + \lambda_0\lvert\beta\rvert + p_{H,\lambda}(\lvert \beta \rvert) \right\} $$
after we rescale all covariates to have $L_2-$norm $n^{1/2}$. The solution to these univariate problems are $$ \hat{\beta}(z) = \mathrm{sgn}(z)(\lvert z \rvert-\lambda_0)\cdot\mathbf{1}_{\lvert z\rvert > \lambda+\lambda_0} $$
these solutions have the same feature as the hard-thresholded estimator: each component is either 0 or of magnitude larger than $\lambda$. This provides a better distinction between insignificant and significant covariates then soft-thresholding by $L_1$ penalty.


With the hard-thresholding property of Prop. \ref{prop:hard-thresholding-nonconvex}, we can prove a basic constraint for the global optimum $\hat{\boldsymbol{\beta}}$ on an event with significant probability \citep{fan2014asymptotic}
\begin{equation}\label{eq:basic_constrant}
    \lVert \boldsymbol{\delta}_2 \rVert _1 \leq 7 \lVert \boldsymbol{\delta}_1 \rVert _1
\end{equation}
where $\boldsymbol{\delta} = \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 = \left( \hat{\boldsymbol{\beta}}_1',\hat{\boldsymbol{\beta}}_2' \right)' - \left( {\boldsymbol{\beta}}_{0,1}',{\boldsymbol{\beta}}_{0,2}' \right)' = \left( \boldsymbol{\delta}^{\prime}_1,\boldsymbol{\delta}^{\prime}_2 \right)^{\prime}$, with $\boldsymbol{\delta}_1\in\mathbb{R}^s$. Where does this constraint come from? For the penalized least square quesion \ref{eq:penalized-LS-l1concave}
$$
\min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
$$
the global minimizer $\hat{\boldsymbol{\beta}}$ leads to 
\begin{align*}
    (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 =& (2n)^{-1}\lVert \mathbf{X}\boldsymbol{\beta}_0+ \boldsymbol{\epsilon} -\mathbf{X}\hat{\boldsymbol{\beta}} \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 \\
    =& (2n)^{-1}\lVert \boldsymbol{\epsilon} -\mathbf{X}(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}_0) \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 \\
    \leq &  (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}{\boldsymbol{\beta}_0} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1\\
    =& (2n)^{-1}\lVert {\boldsymbol{\epsilon}} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1
\end{align*}

then, plug in $\boldsymbol{\delta} = \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 $, we get
\begin{align*}
    (2n)^{-1}\lVert \boldsymbol{\epsilon} -\mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta}_0+\boldsymbol{\delta} \rVert _1 + \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1  &\leq  (2n)^{-1}\lVert {\boldsymbol{\epsilon}} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 \\
    (2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0 \lVert \boldsymbol{\beta}_0+\boldsymbol{\delta} \rVert _1 + \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1  &\leq  \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1
\end{align*}
since $\boldsymbol{\beta}_{0,2}=\mathbf{0}$, $\boldsymbol{\delta}_2=\boldsymbol{\beta}_{0,2}+\boldsymbol{\delta}_2$, we have $$ \lVert {\boldsymbol{\beta}_{0}} + \boldsymbol{\delta} \rVert _1 = \lVert {\boldsymbol{\beta}_{0,1}} + {\boldsymbol{\beta}_{0,2}} + \boldsymbol{\delta}_1 + \boldsymbol{\delta}_2 \rVert _1 = \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 + \boldsymbol{\delta}_2 \rVert _1 \leq  \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 \rVert _1 +  \lVert \boldsymbol{\delta}_2 \rVert _1 $$
hence
$$
(2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 \leq  \lambda_0 \lVert {\boldsymbol{\beta}_{0,1}} \rVert _1 - \lambda_0 \lVert \boldsymbol{\beta}_{0,1}+\boldsymbol{\delta}_1 \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
$$
and by the reverse triangle inequality $ \lVert {\boldsymbol{\beta}_{0,1}} \rVert _1 - \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 \rVert _1 \leq \lVert \boldsymbol{\delta}_1 \rVert _1 $, we get 
$$
(2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 \leq  \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
$$
If assume the distribution of the model error $\boldsymbol{\epsilon}$ as 
$$
\Pr \left( \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty} >\frac{\lambda_0}{2} \right) = O\left(p^{-c_0}\right)
$$
conditional on the event $\mathcal{E} = \left\{ \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty}\leq \lambda_0 /2 \right\}$, we have 
$$
 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 - \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 \geq -\frac{\lambda_0}{2}\lVert \boldsymbol{\delta} \rVert _1 + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 - \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 = \frac{\lambda_0}{2} \lVert \boldsymbol{\delta}_2 \rVert _1 - \frac{3\lambda_0}{2} \lVert \boldsymbol{\delta}_1 \rVert _1
$$
plug this result back, get
\begin{equation}\label{eq:basic_constrant_step1}
    \frac{1}{2n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \frac{\lambda_0}{2} \lVert \boldsymbol{\delta}_2 \rVert _1 \leq \frac{3\lambda_0}{2} \lVert \boldsymbol{\delta}_1 \rVert _1+ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
\end{equation}
Now, if we further impose 2 conditions:
\begin{itemize}
    \item {\myhl[myblue]{\textbf{Condition 1 (eigenvalue condition)}}}: for some positive constant $\kappa_0$ 
    \begin{align*}
        \min_{\lVert\boldsymbol{\delta}\rVert _2=1,\lVert\boldsymbol{\delta}\rVert _0<2s}\frac{1}{\sqrt{n}}\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2 &\geq \kappa_0 & \textbf{(A)} \\
        \kappa = \kappa(s,7) = \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1} \left\{ \frac{1}{\sqrt{n}}\frac{\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2}{\lVert\boldsymbol{\delta}_1\rVert _2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2} \right\} &> 0 & \textbf{(B)}
    \end{align*}
    where $\tilde{\boldsymbol{\delta}}_2$ is the subvector of $\boldsymbol{\delta}_2$ consisting of the components with the $s$ \textbf{\underline{largest}} absolute values. Here
    \begin{itemize}
        \item[-] Condition \textbf{(A)} is a mild sparse eigenvalue condition
        \item[-] Condition \textbf{(B)} combines the restricted eigenvalue assumptions in \citet{bickel2009simultaneous}\footnote{Introduced by \citet{candes2007dantzig} for studying the oracle inequalities for the Lasso estimator and Dantzig selector.}. The intuition is, for OLS estimation, $\mathbf{X}^{\prime}\mathbf{X}$ should be \textbf{\underline{positive definite}}, that is $$ \min_{\mathbf{0}\neq \boldsymbol{\delta}\in\mathbb{R}^p}\left\{ \frac{1}{\sqrt{n}} \frac{\lVert  \mathbf{X} \boldsymbol{\delta} \rVert _2}{\lVert \boldsymbol{\delta} \rVert _2} \right\} >0 $$
        however, when $p>n$, this condition \textbf{never} holds, hence we replace $\lVert \boldsymbol{\delta} \rVert _2$ with the $L_2-$norm of $\lVert \boldsymbol{\delta}_1 \rVert _2$, a subvector of $\boldsymbol{\delta}$ 
        $$ \kappa = \kappa(s,7) \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1}\left\{ \frac{1}{\sqrt{n}} \frac{\lVert  \mathbf{X} \boldsymbol{\delta} \rVert _2}{\lVert \boldsymbol{\delta} \rVert _2} \right\} >0 $$
        and for $L_q$ loss with $q\in (1,2]$, we further bound $ \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2$, which leads to condition \textbf{(B)}.
    \end{itemize}
    \item {\myhl[myblue]{\textbf{Condition 2 (hard-thresholding condition)}}}: The penalty $p_{\lambda}(t)$ satisfies the conditions of Prop. \ref{prop:hard-thresholding-nonconvex} with 
    \begin{align*}
        p'_{\lambda}\left\{ (1-c_1) \lambda \right\} &\leq \lambda_0/4 \\
        \min_{j=1,\cdots,s}\lvert\beta_{0,j}\rvert &> \max\left\{ (1-c_1)\lambda,2\kappa_0^{-1}p_{\lambda}^{1/2}(\infty) \right\}
    \end{align*} 
\end{itemize}

Now, look back at the condition \ref{eq:basic_constrant_step1}, we can upper-bound $ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1$ by $\frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1$. Consider 2 cases:
\begin{itemize}
    \item \textbf{Case 1}: $\lVert \hat{\boldsymbol{\beta}} \rVert _0\geq s$. By the hard-thresholding condition, we have $\lvert\beta_{0,j}\rvert>(1-c_1)\lambda$ and $p'_{\lambda}\left\{ (1-c_1)\lambda \right\}\leq \lambda_0/4$. Hence, for $ j=1,\cdots,s$, 
    \begin{itemize}
        \item[-] if $\hat{\beta}_j\neq 0$, we must have $\lvert \hat{\beta}_j \rvert >(1-c_1)\lambda$. And by the \underline{mean-value theorem}, we have 
        $$\lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j} \rvert) \rvert = p'_{\lambda}(b)(\lvert \hat{\beta}_{j} \rvert - \lvert \beta_{0,j} \rvert)\leq p'_{\lambda}(b)\lvert \delta_{0,j} \rvert$$ 
        where $b$ is between $\lvert \beta_{0,j} \rvert$ and $\lvert \hat{\beta}_j \rvert$, hence, $b>\lvert\beta_{0,j}\rvert>(1-c_1)\lambda$, by the concavity of $p_{\lambda}$, we have $p'(b)<p'((1-c_1)\lambda)\leq \lambda_0/4$, which leads to $\lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j} \rvert) \rvert\leq \frac{1}{4}\lambda_0 \lvert\delta_j \rvert$.
        \item[-] if $\hat{\beta}_j=0$, since $\lVert \hat{\boldsymbol{\beta}}_0 \rVert \geq s$, there must exist some $j'>s$ s.t. $\hat{\beta}_{j'}\neq 0$, similarly 
        \begin{align*}
            \lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j'} \rvert) \rvert &\leq \lvert p_{\lambda}\left(\lvert \beta_{0,j} \rvert\right)-p_{\lambda}\left( (1-c_1)\lambda \right) \rvert + \lvert p_{\lambda}\left(\lvert \hat{\beta}_{j'} \rvert\right)-p_{\lambda}\left( (1-c_1)\lambda \right) \rvert \\
            &= p'_{\lambda}(b_1)\left( \lvert \beta_{0,j} \rvert - (1-c_1)\lambda \right) + p'_{\lambda}(b_2)\left( \lvert \hat{\beta}_{j'} \rvert - (1-c_1)\lambda \right) \\
            &\leq p'_{\lambda}(b_1)\left( \lvert \beta_{0,j} \rvert - \underbrace{\lvert \hat{\beta}_j \rvert}_{=0} \right) +  p'_{\lambda}(b_2) \left( \lvert \hat{\beta}_{j'} \rvert - \underbrace{\lvert \beta_{0,j'} \rvert}_{=0} \right) \\
            &= p'_{\lambda}(b_1)\lvert \delta_j \rvert + p'_{\lambda}(b_2)\lvert \delta_{j'} \rvert \leq \frac{\lambda_0}{4}\left( \lvert \delta_j \rvert + \lvert \delta_{j'} \rvert \right)
        \end{align*}
    \end{itemize}
    together, we have $$ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 \leq \frac{1}{4}\lambda_0\lVert\boldsymbol{\delta}\rVert _1 \leq \frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1 $$
    \item \textbf{Case 2}: $\lVert \hat{\boldsymbol{\beta}} \rVert _0 = s-k$ for some $k\geq 1$. Then we must have $\lVert \boldsymbol{\delta} \rVert _0 \leq \lVert \hat{\boldsymbol{\beta}} \rVert _0 + \lVert \boldsymbol{\beta}_0 \rVert _0\leq s -k+s <2s$, and $\lVert \boldsymbol{\delta} \rVert _2 \geq \sqrt{k}\min_{j=1,\cdots,s}\lvert \beta_{0,j} \rvert$. Also, there are at least $k$ null estimates ($\hat{\beta}_j=0$), thus
    \begin{align*}
        \underbrace{\frac{1}{4n}\lVert \mathbf{X} \boldsymbol{\delta} \rVert^2_2 \geq \frac{\kappa_0^2}{4} \lVert \boldsymbol{\delta} \rVert^2_2}_{\text{Condition 1(A)}} \geq \underbrace{\frac{\kappa_0^2}{4}\left( \sqrt{k}\min_{j=1,\cdots,s}\lvert \beta_{0,j}\rvert \right)^2 \geq kp_{\lambda}(\infty)}_{\text{Condition 2}} \geq k p_{\lambda}(\lvert \beta_{0,j} \rvert)
    \end{align*}
    similar to Case 1, we have the desired upper bound 
    \begin{align*}
        \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 \leq  kp_{\lambda}(\infty) + \frac{1}{4}\lambda_0\lVert\boldsymbol{\delta}\rVert _1 \leq \frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1
    \end{align*}
\end{itemize}
Combining Case 1 and 2, we have $\lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 \leq \frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1$, plug this back in \ref{eq:basic_constrant_step1}, get 
\begin{align*}
    \frac{1}{n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 &\leq 3\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1+ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1\\
     & \leq 3\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 + \frac{1}{2n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{2}\lambda_0\underbrace{\lVert\boldsymbol{\delta} \rVert _1}_{= \lVert\boldsymbol{\delta}_1 \rVert _1 + \lVert\boldsymbol{\delta}_2 \rVert _1} \\
     & \leq 7\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1
\end{align*}
which leads to the constraint in \ref{eq:basic_constrant} and $\frac{1}{n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 \leq 7\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1$.

Now, look back at Condition 1(\textbf{B})
$$
\kappa = \kappa(s,7) = \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1} \left\{ \frac{1}{\sqrt{n}}\frac{\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2}{\lVert\boldsymbol{\delta}_1\rVert _2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2} \right\} > 0
$$
we have 
\begin{align*}
    \frac{1}{4}\kappa^2(s,7)\lVert\boldsymbol{\delta}_1\rVert^2_2 \leq \frac{1}{4}\kappa^2(s,7)\left( \lVert\boldsymbol{\delta}_1\rVert^2_2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert^2_2 \right)\leq \frac{1}{4n}\lVert\mathbf{X}\boldsymbol{\delta}\rVert^2_2\leq \underbrace{\frac{7}{4}\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 \leq \frac{7}{4}\lambda_0\sqrt{s} \lVert \boldsymbol{\delta}_1 \rVert _2 }_{\text{Cauchy-Schwartz inequality}}
\end{align*}
hence
\begin{align*}
    \lVert\boldsymbol{\delta}_1\rVert _2 &\leq \frac{7\lambda_0 \sqrt{s}}{\kappa^2(s,7)} & \lVert\boldsymbol{\delta}_1\rVert _1\leq \sqrt{s} \lVert\boldsymbol{\delta}_1\rVert _1 &\leq \frac{7\lambda_0 s}{\kappa^2(s,7)} & \lVert\boldsymbol{\delta}_2'\rVert _2 &\leq \frac{\sqrt{7\lambda_0 \sqrt{s} \lVert \boldsymbol{\delta}_1 \rVert _2}}{\kappa(s,7)}
\end{align*}
Notice that the $k-$th largest absolute component of $\boldsymbol{\delta}_2$ is bounded from above by $\lVert\boldsymbol{\delta}_2\rVert _1/k$, then for $\boldsymbol{\delta}_{2_s}$, the subvector of $\boldsymbol{\delta}_2$ consisting of components \textbf{excluding} those with the $s$ largest magnitudes, we have
$$
\lVert\boldsymbol{\delta}_{2_s}\rVert _2^2\leq  \sum^{p-s}_{k=s+1}\frac{1}{k^2} \lVert\boldsymbol{\delta}_2\rVert^2 _1 \leq s^{-1}\lVert\boldsymbol{\delta}_2\rVert _1^2 \Rightarrow \lVert\boldsymbol{\delta}_{2_s}\rVert _2 \leq \frac{1}{\sqrt{s}}\lVert\boldsymbol{\delta}_2\rVert _1  \overset{\ref{eq:basic_constrant}}{\leq} \frac{7}{\sqrt{s}}\lVert\boldsymbol{\delta}_1\rVert _1 \overset{\text{C-S}}{\leq} 7\lVert\boldsymbol{\delta}_1\rVert _2
$$
since $\boldsymbol{\delta}_{2_s}$ and $\boldsymbol{\delta}_{2}'$ are a partition of $\boldsymbol{\delta}$, we have 
\begin{align*}
    \lVert\boldsymbol{\delta}_2\rVert _2 \leq \lVert\boldsymbol{\delta}_{2_s}\rVert _2 + \lVert\boldsymbol{\delta}_2'\rVert _2 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _2 + \frac{\sqrt{7\lambda_0 \sqrt{s} \lVert \boldsymbol{\delta}_1 \rVert _2}}{\kappa(s,7)}\leq 7 \lVert\boldsymbol{\delta}_1\rVert _2 &\leq \frac{56\lambda_0 \sqrt{s}}{\kappa^2(s,7)} 
\end{align*}


Hence, we have the following theorem:
\begin{theorem}{1}{}
    Assume that Condition 1 and 2 and the model error bound $\Pr \left( \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty} >\frac{\lambda_0}{2} \right) = O\left(p^{-c_0}\right)$ 
\end{theorem}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}