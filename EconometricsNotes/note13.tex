\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{13}{Non-convex Learning + Lasso}{}{Sai Zhang}{Combining the best of the two, we can use \textbf{Lasso plus Concave} method, with Lasso screening and concave component selecting variables, achieving a coordinated intrinsic two-scale learning.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}

We are facing a tradeoff:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Convex}} methods: have appealing \underline{prediction power and oracle inequalities}, but challenging to provide tight \underline{false sign rate control}
    \item \myhl[myred]{\textbf{Concave}} methods: have good \underline{variable selection} properties, but challenging to establish \underline{global} properties and risk properties
\end{itemize}

Here, we take advantage of the linearity of Lasso (convex \textit{and} concave) and try to combine it with concave regularization to get the best of both.

\section{Model Setup}
Again, consider a linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, where 
\begin{itemize}
    \item response vector ($n\times 1$): $\mathbf{y} = (y_1,\cdots,y_n)^{\prime}$
    \item design matrix ($n\times p$): $\mathbf{X} = (\mathbf{x}_1,\cdots,\mathbf{x}_p)$, with each column rescaled to have $L_2-$norm $n^{1/2}$
\end{itemize}
here, we consider a scenario where
\begin{itemize}
    \item $\boldsymbol{\beta}_0 = (\beta_{0,1},\cdots,\beta_{0,p})^{\prime}$ is \textit{\textbf{sparse}} (with many 0 components)
    \item ultra-\textbf{high} dimensions: $\log p=O(n^a)$, for some $0<a<1$
\end{itemize}
and consider the penalized least squares 
\begin{equation}\label{eq:penalized-LS-l1concave}
    \min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
\end{equation}
where 
\begin{itemize}
    \item $\lambda_0 = c\left(\frac{\log p}{n}\right)^{1/2}$ for some $c>0$
    \item $p_{\lambda}(\boldsymbol{\beta}) = p_{\lambda}(\lvert\boldsymbol{\beta}\rvert ) = (p_{\lambda}(\lvert \beta_1 \rvert),\cdots,p_{\lambda}(\lvert \beta_p \rvert))^{\prime}$, with $\lvert\boldsymbol{\beta} \rvert = (\lvert \beta_1 \rvert,\cdots,\lvert \beta_p \rvert)^{\prime}$; the concave penalty $p_{\lambda}(t)$ is defined on $t\in\left[0,\infty \right)$, indexed by $\lambda\geq 0$, increasing in \textbf{\underline{both}} $t$ and $\lambda$, $p_{\lambda}(0)=0$
\end{itemize}
the 2 penalty components
\begin{itemize}
    \item $L_1-$component: minimum amount of regularization for \underline{\textit{removing noise}} in prediction 
    \item concave component $\lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1$: adapt model sparsity for \underline{\textit{variable selection}}
\end{itemize}

Under this set up, we can derive the hard-thresholding property as 
\begin{proposition}{Hard-Thresholding Property}{hard-thresholding-nonconvex}
    Assume the $p_{\lambda}(t)$, $t\geq 0$, is \myhl[myblue]{\textbf{increasing and concave}} with 
    \begin{itemize}
        \item $p_{\lambda}(t)\geq p_{H,\lambda}(t) = \frac{1}{2}\left[\lambda^2-(\lambda-t)^2_+\right]$ on $[0,\lambda]$
        \item $p'_{\lambda}\left((1-c_1)\lambda\right)\leq c_1\lambda$ for some $c_1\in [0,1)$
        \item $-p''_{\lambda}(t)$ decreasing on $[0,(1-c_1)\lambda]$
    \end{itemize}
    then any \underline{local minimizer} of \ref{eq:penalized-LS-l1concave} that is also a \underline{global minimizer} in each coordinate has the \textbf{{hard-thresholding}} feature that each component is either $0$ or of magnitude \textbf{larger} than $(1-c_1)\lambda$
\end{proposition}

%\newpage
%\bibliographystyle{plainnat}
%\bibliography{ref.bib}

\end{document}