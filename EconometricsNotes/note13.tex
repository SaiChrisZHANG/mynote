\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{13}{Non-convex Learning + Lasso}{}{Sai Zhang}{Combining the best of the two, we can use \textbf{Lasso plus Concave} method, with Lasso screening and concave component selecting variables, achieving a coordinated intrinsic two-scale learning.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}

We are facing a tradeoff:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Convex}} methods: have appealing \underline{prediction power and oracle inequalities}, but challenging to provide tight \underline{false sign rate control}
    \item \myhl[myred]{\textbf{Concave}} methods: have good \underline{variable selection} properties, but challenging to establish \underline{global} properties and risk properties
\end{itemize}

Here, we take advantage of the linearity of Lasso (convex \textit{and} concave) and try to combine it with concave regularization to get the best of both.

\section{Model Setup}
Again, consider a linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, where 
\begin{itemize}
    \item response vector ($n\times 1$): $\mathbf{y} = (y_1,\cdots,y_n)^{\prime}$
    \item design matrix ($n\times p$): $\mathbf{X} = (\mathbf{x}_1,\cdots,\mathbf{x}_p)$, with each column rescaled to have $L_2-$norm $n^{1/2}$
\end{itemize}
here, we consider a scenario where
\begin{itemize}
    \item $\boldsymbol{\beta}_0 = (\beta_{0,1},\cdots,\beta_{0,p})^{\prime}$ is \textit{\textbf{sparse}} (with many 0 components)
    \item ultra-\textbf{high} dimensions: $\log p=O(n^a)$, for some $0<a<1$
\end{itemize}
and consider the penalized least squares 
\begin{equation}\label{eq:penalized-LS-l1concave}
    \min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
\end{equation}
where 
\begin{itemize}
    \item $\lambda_0 = c\left(\frac{\log p}{n}\right)^{1/2}$ for some $c>0$
    \item $p_{\lambda}(\boldsymbol{\beta}) = p_{\lambda}(\lvert\boldsymbol{\beta}\rvert ) = (p_{\lambda}(\lvert \beta_1 \rvert),\cdots,p_{\lambda}(\lvert \beta_p \rvert))^{\prime}$, with $\lvert\boldsymbol{\beta} \rvert = (\lvert \beta_1 \rvert,\cdots,\lvert \beta_p \rvert)^{\prime}$; the concave penalty $p_{\lambda}(t)$ is defined on $t\in\left[0,\infty \right)$, indexed by $\lambda\geq 0$, increasing in \textbf{\underline{both}} $t$ and $\lambda$, $p_{\lambda}(0)=0$
\end{itemize}
the 2 penalty components
\begin{itemize}
    \item $L_1-$component: minimum amount of regularization for \underline{\textit{removing noise}} in prediction 
    \item concave component $\lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1$: adapt model sparsity for \underline{\textit{variable selection}}
\end{itemize}

Under this set up, we can derive the hard-thresholding property as 
\begin{proposition}{Hard-Thresholding Property}{hard-thresholding-nonconvex}
    Assume the $p_{\lambda}(t)$, $t\geq 0$, is \myhl[myblue]{\textbf{increasing and concave}} with 
    \begin{itemize}
        \item $p_{\lambda}(t)\geq p_{H,\lambda}(t) = \frac{1}{2}\left[\lambda^2-(\lambda-t)^2_+\right]$ on $[0,\lambda]$
        \item $p'_{\lambda}\left((1-c_1)\lambda\right)\leq c_1\lambda$ for some $c_1\in [0,1)$
        \item $-p''_{\lambda}(t)$ decreasing on $[0,(1-c_1)\lambda]$
    \end{itemize}
    then any \underline{local minimizer} of \ref{eq:penalized-LS-l1concave} that is also a \underline{global minimizer} in each coordinate has the \textbf{{hard-thresholding}} feature that each component is either $0$ or of magnitude \textbf{larger} than $(1-c_1)\lambda$
\end{proposition}
Such property is shared by a wide class of concave penalties, including hard-thresholding penalty $p_{H,\lambda}(t)$ with $c_1=0$, $L_0-$penalty, and SICA (with suitable $c_1$).

\paragraph*{How to \textit{\underline{understand}} this proposition?} Let $\hat{\boldsymbol{\beta}}=\left(\hat{\beta}_1,\cdots,\hat{\beta}_p\right)^{\prime}$, then \myhl[myred]{\textbf{each} $\hat{\beta}_j$} is the glocal minimizer of the corresponding univariate penalized least-square problem along the $j-$th coordinate. These univariate problems share a common form with (generally) different scalars $z$ $$ \hat{\beta}(z) = \arg\min_{\beta\in\mathbb{R}}\left\{ \frac{1}{2}(z-\beta)^2 + \lambda_0\lvert\beta\rvert + p_{H,\lambda}(\lvert \beta \rvert) \right\} $$
after we rescale all covariates to have $L_2-$norm $n^{1/2}$. The solution to these univariate problems are $$ \hat{\beta}(z) = \mathrm{sgn}(z)(\lvert z \rvert-\lambda_0)\cdot\mathbf{1}_{\lvert z\rvert > \lambda+\lambda_0} $$
these solutions have the same feature as the hard-thresholded estimator: each component is either 0 or of magnitude larger than $\lambda$. This provides a better distinction between insignificant and significant covariates then soft-thresholding by $L_1$ penalty.


With the hard-thresholding property of Prop. \ref{prop:hard-thresholding-nonconvex}, we can prove a basic constraint for the global optimum $\hat{\boldsymbol{\beta}}$ on an event with significant probability \citep{fan2014asymptotic}
\begin{equation}\label{eq:basic_constrant}
    \lVert \boldsymbol{\delta}_2 \rVert _1 \leq 7 \lVert \boldsymbol{\delta}_1 \rVert _1
\end{equation}
where $\boldsymbol{\delta} = \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 = \left( \hat{\boldsymbol{\beta}}_1',\hat{\boldsymbol{\beta}}_2' \right)' - \left( {\boldsymbol{\beta}}_{0,1}',{\boldsymbol{\beta}}_{0,2}' \right)' = \left( \boldsymbol{\delta}^{\prime}_1,\boldsymbol{\delta}^{\prime}_2 \right)^{\prime}$, with $\boldsymbol{\delta}_1\in\mathbb{R}^s$. Where does this constraint come from? For the penalized least square quesion \ref{eq:penalized-LS-l1concave}
$$
\min_{\boldsymbol{\beta}\in\mathbb{R}^p}\left\{ (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta} \rVert _1 + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1  \right\}
$$
the global minimizer $\hat{\boldsymbol{\beta}}$ leads to 
\begin{align*}
    (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 =& (2n)^{-1}\lVert \mathbf{X}\boldsymbol{\beta}_0+ \boldsymbol{\epsilon} -\mathbf{X}\hat{\boldsymbol{\beta}} \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 \\
    =& (2n)^{-1}\lVert \boldsymbol{\epsilon} -\mathbf{X}(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}_0) \rVert ^2_2 + \lambda_0 \lVert \hat{\boldsymbol{\beta}} \rVert _1 + \lVert p_{\lambda}(\hat{\boldsymbol{\beta}}) \rVert _1 \\
    \leq &  (2n)^{-1}\lVert \mathbf{y}-\mathbf{X}{\boldsymbol{\beta}_0} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1\\
    =& (2n)^{-1}\lVert {\boldsymbol{\epsilon}} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1
\end{align*}

then, plug in $\boldsymbol{\delta} = \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 $, we get
\begin{align*}
    (2n)^{-1}\lVert \boldsymbol{\epsilon} -\mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \lambda_0 \lVert \boldsymbol{\beta}_0+\boldsymbol{\delta} \rVert _1 + \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1  &\leq  (2n)^{-1}\lVert {\boldsymbol{\epsilon}} \rVert ^2_2 + \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 \\
    (2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0 \lVert \boldsymbol{\beta}_0+\boldsymbol{\delta} \rVert _1 + \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1  &\leq  \lambda_0 \lVert {\boldsymbol{\beta}_0} \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1
\end{align*}
since $\boldsymbol{\beta}_{0,2}=\mathbf{0}$, $\boldsymbol{\delta}_2=\boldsymbol{\beta}_{0,2}+\boldsymbol{\delta}_2$, we have $$ \lVert {\boldsymbol{\beta}_{0}} + \boldsymbol{\delta} \rVert _1 = \lVert {\boldsymbol{\beta}_{0,1}} + {\boldsymbol{\beta}_{0,2}} + \boldsymbol{\delta}_1 + \boldsymbol{\delta}_2 \rVert _1 = \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 + \boldsymbol{\delta}_2 \rVert _1 \leq  \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 \rVert _1 +  \lVert \boldsymbol{\delta}_2 \rVert _1 $$
hence
$$
(2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 \leq  \lambda_0 \lVert {\boldsymbol{\beta}_{0,1}} \rVert _1 - \lambda_0 \lVert \boldsymbol{\beta}_{0,1}+\boldsymbol{\delta}_1 \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
$$
and by the reverse triangle inequality $ \lVert {\boldsymbol{\beta}_{0,1}} \rVert _1 - \lVert {\boldsymbol{\beta}_{0,1}} + \boldsymbol{\delta}_1 \rVert _1 \leq \lVert \boldsymbol{\delta}_1 \rVert _1 $, we get 
$$
(2n)^{-1}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 \leq  \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 + \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
$$
If assume the distribution of the model error $\boldsymbol{\epsilon}$ as 
$$
\Pr \left( \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty} >\frac{\lambda_0}{2} \right) = O\left(p^{-c_0}\right)
$$
conditional on the event $\mathcal{E} = \left\{ \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty}\leq \lambda_0 /2 \right\}$, we have 
$$
 - n^{-1}\boldsymbol{\epsilon}^{\prime}\mathbf{X}\boldsymbol{\delta} + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 - \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 \geq -\frac{\lambda_0}{2}\lVert \boldsymbol{\delta} \rVert _1 + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 - \lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 = \frac{\lambda_0}{2} \lVert \boldsymbol{\delta}_2 \rVert _1 - \frac{3\lambda_0}{2} \lVert \boldsymbol{\delta}_1 \rVert _1
$$
plug this result back, get
\begin{equation}\label{eq:basic_constrant_step1}
    \frac{1}{2n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \frac{\lambda_0}{2} \lVert \boldsymbol{\delta}_2 \rVert _1 \leq \frac{3\lambda_0}{2} \lVert \boldsymbol{\delta}_1 \rVert _1+ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 
\end{equation}
Now, if we further impose 2 conditions:
\begin{itemize}
    \item {\myhl[myblue]{\textbf{Condition 1 (eigenvalue condition)}}}: for some positive constant $\kappa_0$ 
    \begin{align*}
        \min_{\lVert\boldsymbol{\delta}\rVert _2=1,\lVert\boldsymbol{\delta}\rVert _0<2s}\frac{1}{\sqrt{n}}\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2 &\geq \kappa_0 & \textbf{(A)} \\
        \kappa = \kappa(s,7) = \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1} \left\{ \frac{1}{\sqrt{n}}\frac{\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2}{\lVert\boldsymbol{\delta}_1\rVert _2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2} \right\} &> 0 & \textbf{(B)}
    \end{align*}
    where $\tilde{\boldsymbol{\delta}}_2$ is the subvector of $\boldsymbol{\delta}_2$ consisting of the components with the $s$ \textbf{\underline{largest}} absolute values. Here
    \begin{itemize}
        \item[-] Condition \textbf{(A)} is a mild sparse eigenvalue condition
        \item[-] Condition \textbf{(B)} combines the restricted eigenvalue assumptions in \citet{bickel2009simultaneous}\footnote{Introduced by \citet{candes2007dantzig} for studying the oracle inequalities for the Lasso estimator and Dantzig selector.}. The intuition is, for OLS estimation, $\mathbf{X}^{\prime}\mathbf{X}$ should be \textbf{\underline{positive definite}}, that is $$ \min_{\mathbf{0}\neq \boldsymbol{\delta}\in\mathbb{R}^p}\left\{ \frac{1}{\sqrt{n}} \frac{\lVert  \mathbf{X} \boldsymbol{\delta} \rVert _2}{\lVert \boldsymbol{\delta} \rVert _2} \right\} >0 $$
        however, when $p>n$, this condition \textbf{never} holds, hence we replace $\lVert \boldsymbol{\delta} \rVert _2$ with the $L_2-$norm of $\lVert \boldsymbol{\delta}_1 \rVert _2$, a subvector of $\boldsymbol{\delta}$ 
        $$ \kappa = \kappa(s,7) \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1}\left\{ \frac{1}{\sqrt{n}} \frac{\lVert  \mathbf{X} \boldsymbol{\delta} \rVert _2}{\lVert \boldsymbol{\delta} \rVert _2} \right\} >0 $$
        and for $L_q$ loss with $q\in (1,2]$, we further bound $ \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2$, which leads to condition \textbf{(B)}.
    \end{itemize}
    \item {\myhl[myblue]{\textbf{Condition 2 (hard-thresholding condition)}}}: The penalty $p_{\lambda}(t)$ satisfies the conditions of Prop. \ref{prop:hard-thresholding-nonconvex} with 
    \begin{align*}
        p'_{\lambda}\left\{ (1-c_1) \lambda \right\} &\leq \lambda_0/4 \\
        \min_{j=1,\cdots,s}\lvert\beta_{0,j}\rvert &> \max\left\{ (1-c_1)\lambda,2\kappa_0^{-1}p_{\lambda}^{1/2}(\infty) \right\}
    \end{align*} 
\end{itemize}

Now, look back at the condition \ref{eq:basic_constrant_step1}, we can upper-bound $ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1$ by $\frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1$. Consider 2 cases:
\begin{itemize}
    \item \textbf{Case 1}: $\lVert \hat{\boldsymbol{\beta}} \rVert _0\geq s$. By the hard-thresholding condition, we have $\lvert\beta_{0,j}\rvert>(1-c_1)\lambda$ and $p'_{\lambda}\left\{ (1-c_1)\lambda \right\}\leq \lambda_0/4$. Hence, for $ j=1,\cdots,s$, 
    \begin{itemize}
        \item[-] if $\hat{\beta}_j\neq 0$, we must have $\lvert \hat{\beta}_j \rvert >(1-c_1)\lambda$. And by the \underline{mean-value theorem}, we have 
        $$\lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j} \rvert) \rvert = p'_{\lambda}(b)(\lvert \hat{\beta}_{j} \rvert - \lvert \beta_{0,j} \rvert)\leq p'_{\lambda}(b)\lvert \delta_{0,j} \rvert$$ 
        where $b$ is between $\lvert \beta_{0,j} \rvert$ and $\lvert \hat{\beta}_j \rvert$, hence, $b>\lvert\beta_{0,j}\rvert>(1-c_1)\lambda$, by the concavity of $p_{\lambda}$, we have $p'(b)<p'((1-c_1)\lambda)\leq \lambda_0/4$, which leads to $\lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j} \rvert) \rvert\leq \frac{1}{4}\lambda_0 \lvert\delta_j \rvert$.
        \item[-] if $\hat{\beta}_j=0$, since $\lVert \hat{\boldsymbol{\beta}}_0 \rVert \geq s$, there must exist some $j'>s$ s.t. $\hat{\beta}_{j'}\neq 0$, similarly 
        \begin{align*}
            \lvert p_{\lambda}(\lvert \beta_{0,j} \rvert)-p_{\lambda}(\lvert \hat{\beta}_{j'} \rvert) \rvert &\leq \lvert p_{\lambda}\left(\lvert \beta_{0,j} \rvert\right)-p_{\lambda}\left( (1-c_1)\lambda \right) \rvert + \lvert p_{\lambda}\left(\lvert \hat{\beta}_{j'} \rvert\right)-p_{\lambda}\left( (1-c_1)\lambda \right) \rvert \\
            &= p'_{\lambda}(b_1)\left( \lvert \beta_{0,j} \rvert - (1-c_1)\lambda \right) + p'_{\lambda}(b_2)\left( \lvert \hat{\beta}_{j'} \rvert - (1-c_1)\lambda \right) \\
            &\leq p'_{\lambda}(b_1)\left( \lvert \beta_{0,j} \rvert - \underbrace{\lvert \hat{\beta}_j \rvert}_{=0} \right) +  p'_{\lambda}(b_2) \left( \lvert \hat{\beta}_{j'} \rvert - \underbrace{\lvert \beta_{0,j'} \rvert}_{=0} \right) \\
            &= p'_{\lambda}(b_1)\lvert \delta_j \rvert + p'_{\lambda}(b_2)\lvert \delta_{j'} \rvert \leq \frac{\lambda_0}{4}\left( \lvert \delta_j \rvert + \lvert \delta_{j'} \rvert \right)
        \end{align*}
    \end{itemize}
    together, we have $$ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 \leq \frac{1}{4}\lambda_0\lVert\boldsymbol{\delta}\rVert _1 \leq \frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1 $$
    \item \textbf{Case 2}: $\lVert \hat{\boldsymbol{\beta}} \rVert _0 = s-k$ for some $k\geq 1$. Then we must have $\lVert \boldsymbol{\delta} \rVert _0 \leq \lVert \hat{\boldsymbol{\beta}} \rVert _0 + \lVert \boldsymbol{\beta}_0 \rVert _0\leq s -k+s <2s$, and $\lVert \boldsymbol{\delta} \rVert _2 \geq \sqrt{k}\min_{j=1,\cdots,s}\lvert \beta_{0,j} \rvert$. Also, there are at least $k$ null estimates ($\hat{\beta}_j=0$), thus
    \begin{align*}
        \underbrace{\frac{1}{4n}\lVert \mathbf{X} \boldsymbol{\delta} \rVert^2_2 \geq \frac{\kappa_0^2}{4} \lVert \boldsymbol{\delta} \rVert^2_2}_{\text{Condition 1(A)}} \geq \underbrace{\frac{\kappa_0^2}{4}\left( \sqrt{k}\min_{j=1,\cdots,s}\lvert \beta_{0,j}\rvert \right)^2 \geq kp_{\lambda}(\infty)}_{\text{Condition 2}} \geq k p_{\lambda}(\lvert \beta_{0,j} \rvert)
    \end{align*}
    similar to Case 1, we have the desired upper bound 
    \begin{align*}
        \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 \leq  kp_{\lambda}(\infty) + \frac{1}{4}\lambda_0\lVert\boldsymbol{\delta}\rVert _1 \leq \frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1
    \end{align*}
\end{itemize}
Combining Case 1 and 2, we have $\lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1 \leq \frac{1}{4n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{4}\lambda_0\lVert\boldsymbol{\delta} \rVert _1$, plug this back in \ref{eq:basic_constrant_step1}, get 
\begin{align*}
    \frac{1}{n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 + \lambda_0\lVert \boldsymbol{\delta}_2 \rVert _1 &\leq 3\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1+ \lVert p_{\lambda}({\boldsymbol{\beta}_0}) \rVert _1 -  \lVert p_{\lambda}( \boldsymbol{\beta}_0+\boldsymbol{\delta}) \rVert _1\\
     & \leq 3\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 + \frac{1}{2n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2+\frac{1}{2}\lambda_0\underbrace{\lVert\boldsymbol{\delta} \rVert _1}_{= \lVert\boldsymbol{\delta}_1 \rVert _1 + \lVert\boldsymbol{\delta}_2 \rVert _1} \\
     & \leq 7\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1
\end{align*}
which leads to the constraint in \ref{eq:basic_constrant} and $\frac{1}{n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert ^2_2 \leq 7\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1$.

Now, look back at Condition 1(\textbf{B})
$$
\kappa = \kappa(s,7) = \min_{\boldsymbol{\delta}\neq \mathbf{0}, \lVert\boldsymbol{\delta}_2\rVert _1 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _1} \left\{ \frac{1}{\sqrt{n}}\frac{\lVert\mathbf{X}\boldsymbol{\delta}\rVert _2}{\lVert\boldsymbol{\delta}_1\rVert _2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert _2} \right\} > 0
$$
we have 
\begin{align*}
    \frac{1}{4}\kappa^2(s,7)\lVert\boldsymbol{\delta}_1\rVert^2_2 \leq \frac{1}{4}\kappa^2(s,7)\left( \lVert\boldsymbol{\delta}_1\rVert^2_2 \vee \lVert\tilde{\boldsymbol{\delta}}_2\rVert^2_2 \right)\leq \frac{1}{4n}\lVert\mathbf{X}\boldsymbol{\delta}\rVert^2_2\leq \underbrace{\frac{7}{4}\lambda_0 \lVert \boldsymbol{\delta}_1 \rVert _1 \leq \frac{7}{4}\lambda_0\sqrt{s} \lVert \boldsymbol{\delta}_1 \rVert _2 }_{\text{Cauchy-Schwartz inequality}}
\end{align*}
hence
\begin{align}\label{eq:L2bound_loss_subvector}
    \lVert\boldsymbol{\delta}_1\rVert _2 &\leq \frac{7\lambda_0 \sqrt{s}}{\kappa^2(s,7)} & \lVert\boldsymbol{\delta}_1\rVert _1\leq \sqrt{s} \lVert\boldsymbol{\delta}_1\rVert _1 &\leq \frac{7\lambda_0 s}{\kappa^2(s,7)} & \lVert\boldsymbol{\delta}_2'\rVert _2 &\leq \frac{\sqrt{7\lambda_0 \sqrt{s} \lVert \boldsymbol{\delta}_1 \rVert _2}}{\kappa(s,7)}
\end{align}

Notice that the $k-$th largest absolute component of $\boldsymbol{\delta}_2$ is bounded from above by $\lVert\boldsymbol{\delta}_2\rVert _1/k$, then for $\boldsymbol{\delta}_{2_s}$, the subvector of $\boldsymbol{\delta}_2$ consisting of components \textbf{excluding} those with the $s$ largest magnitudes, we have
$$
\lVert\boldsymbol{\delta}_{2_s}\rVert _2^2\leq  \sum^{p-s}_{k=s+1}\frac{1}{k^2} \lVert\boldsymbol{\delta}_2\rVert^2 _1 \leq s^{-1}\lVert\boldsymbol{\delta}_2\rVert _1^2 \Rightarrow \lVert\boldsymbol{\delta}_{2_s}\rVert _2 \leq \frac{1}{\sqrt{s}}\lVert\boldsymbol{\delta}_2\rVert _1  \overset{\ref{eq:basic_constrant}}{\leq} \frac{7}{\sqrt{s}}\lVert\boldsymbol{\delta}_1\rVert _1 \overset{\text{C-S}}{\leq} 7\lVert\boldsymbol{\delta}_1\rVert _2
$$
since $\boldsymbol{\delta}_{2_s}$ and $\boldsymbol{\delta}_{2}'$ are a partition of $\boldsymbol{\delta}$, we have 
\begin{align}\label{eq:L2bound_loss_subvector2}
    \lVert\boldsymbol{\delta}_2\rVert _2 \leq \lVert\boldsymbol{\delta}_{2_s}\rVert _2 + \lVert\boldsymbol{\delta}_2'\rVert _2 \leq 7 \lVert\boldsymbol{\delta}_1\rVert _2 + \frac{\sqrt{7\lambda_0 \sqrt{s} \lVert \boldsymbol{\delta}_1 \rVert _2}}{\kappa(s,7)}\leq  \frac{56\lambda_0 \sqrt{s}}{\kappa^2(s,7)} 
\end{align}

Together, for the estimation loss $\boldsymbol{\delta}= \hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0$, we have
\begin{itemize}
    \item \myhl[myblue]{\textbf{$L_2$-covar-loss-correlation}}: $\frac{1}{n}\lVert \mathbf{X}\boldsymbol{\delta} \rVert^2_2 \leq 7\lambda_0\lVert \boldsymbol{\delta}_1 \rVert _1 \leq \frac{(7\lambda_0)^2s}{\kappa^2(s,7)}\Rightarrow \frac{1}{\sqrt{n}}\lVert \mathbf{X}\boldsymbol{\delta} \rVert _2 \leq \frac{7\lambda_0 \sqrt{s}}{\kappa(s,7)}$
    \item \myhl[myblue]{\textbf{$L_2$-loss}}: $\lVert\boldsymbol{\delta}\rVert _2\leq \lVert\boldsymbol{\delta}_1\rVert _2+\lVert\boldsymbol{\delta}_2\rVert _2\leq \frac{63\lambda_0\sqrt{s}}{\kappa^2(s,7)}$
    \item \myhl[myblue]{\textbf{$L_q$-loss}}: $\lVert\boldsymbol{\delta}\rVert _q \leq \left(s^{(2-q)/2}\lVert\boldsymbol{\delta}_1\rVert ^q_2\right)^{1/q} = s^{(2-q)/2q}\lVert\boldsymbol{\delta}_1\rVert _2 \leq s^{(2-q)/2q}\frac{7\lambda_0 \sqrt{s}}{\kappa^2(s,7)} = \frac{7\lambda_0 s^{1/q}}{\kappa^2(s,7)} $ \sidenotes{$\leftarrow$by Holder's\\ inequality}
    
\end{itemize}
Define the \myhl[myblue]{\textbf{number of falsely discovered signs}} as\footnote{\textbf{Stronger} than the total number of false positives and false negatives.}
    $$
    \mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right) = \left\vert \left\{ j=1,\cdots,p: \mathrm{sgn}\left(\hat{\beta}_j\right) \neq \mathrm{sgn}\left(\beta_{0,j}\right) \right\} \right\vert
    $$
we know from Prop.\ref{prop:hard-thresholding-nonconvex} that $\lvert\hat{\beta}_j\rvert>(1-c_1)\lambda$ and from Condition 2 that $ \lvert \beta_{0,j}\rvert>(1-c_1)\lambda  $, then if $\mathrm{sgn}(\hat{\beta}_j)\neq \mathrm{sgn}(\beta_{0,j})$, we must have $ \lvert \delta_j \rvert = \lvert \hat{\beta}_j - \beta_{0,j} \rvert \geq (1-c_1)\lambda $. Therefore, it follows that 
$$
\lVert \boldsymbol{\delta} \rVert _2 \geq \left(\mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right)\right)^{1/2}(1-c_1)\lambda
$$
hence 
$$
\mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right) \leq \frac{\lVert \boldsymbol{\delta} \rVert ^2_2}{(1-c_1)^2\lambda^2} \leq \left(\frac{63}{1-c_1}\right)^2\left(\frac{\lambda_0}{\lambda}\right)^2\frac{s}{\kappa^4(s,7)}
$$
The results above are all conditional on the event $\mathcal{E} = \left\{ \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty}\leq \lambda_0 /2 \right\}$, hence hold simultaneously with proability $1-O(p^{-c_0})$.

Altogether, we have the following theorem:
\begin{theorem}{Properties of the Global Minimizer $\hat{\boldsymbol{\beta}}$}{global_minimizer_property}
    Assume that Condition 1 and 2 and the model error bound $\Pr \left( \left\Vert n^{-1}\mathbf{X}'\boldsymbol{\epsilon}  \right\Vert _{\infty} >\frac{\lambda_0}{2} \right) = O\left(p^{-c_0}\right)$ , and $p_{\lambda}(t)$ is continuously differentiable. Then the global minimizer $\hat{\boldsymbol{\beta}}$ of \ref{eq:penalized-LS-l1concave} has the hard-thresholding property stated in Prop.\ref{prop:hard-thresholding-nonconvex}, and, with probability $1-O(p^{-c_0})$, statisfies simultaneously that
    \begin{align}
        \frac{1}{\sqrt{n}}\left\Vert \mathbf{X}\left( \hat{\boldsymbol{\beta}} -\boldsymbol{\beta}_0 \right) \right\Vert _2 &= O(\kappa^{-1}\lambda_0 s^{1/2}) \\
        \left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert _q &= O(\kappa^{-2}\lambda_0 s^{1/q}), & q\in[1,2]\\
        \mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right) &= O\left( \kappa^{-4} \left(\frac{\lambda_0}{\lambda}\right)^2 s \right)
    \end{align}

    If in addition $\lambda \geq \frac{56\lambda_0\sqrt{s}}{(1-c_1)\kappa^2}$, then with probability $1-O(p^{-c_0})$, we also have that 
    \begin{align*}
        \mathrm{sgn}\left(\hat{\boldsymbol{\beta}}\right) &= \mathrm{sgn}\left(\boldsymbol{\beta}_0\right) & \left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert _{\infty} = O\left(\lambda_0\left\Vert \left(\frac{1}{n}\mathbf{X}^{\prime}_1\mathbf{X}_1 \right)^{-1} \right\Vert _{\infty}\right)
    \end{align*}
    where $\mathbf{X}_1$ is the $n\times s$ submatrix of $\mathbf{X}$ corresponding to $s$ nonzero regression coefficients $\beta_{0,j}$.
\end{theorem}
The proof of the second part follows as such: by assuming $\lambda \geq \frac{56\lambda_0\sqrt{s}}{(1-c_1)\kappa^2}$, from Condition 2, we have $\min_{j=1,\cdots,s}\lvert \beta_{0,j}\rvert > \frac{56\lambda_0\sqrt{s}}{\kappa^2(s,7)}$, combined with \ref{eq:L2bound_loss_subvector}, we know that
$$
\mathrm{sgn}\left( \hat{\beta}_j \right) = \mathrm{sgn}\left(\beta_{0,j}\right),\ \forall j=1,\cdots, s
$$
by a simple contradiction argument. In view of \ref{eq:L2bound_loss_subvector2} and the hard-thresholding feature of $\hat{\boldsymbol{\beta}} = \left( \hat{\boldsymbol{\beta}}_{0,1}^{\prime},\hat{\boldsymbol{\beta}}_{0,2}^{\prime} \right)^{\prime}$, with $\hat{\boldsymbol{\beta}}_{0,1} = \left(\hat{\beta}_1,\cdots,\hat{\beta}_s\right)^{\prime}$, a similar contradiction argument leads to $\hat{\boldsymbol{\beta}}_{0,2}=\mathbf{0}$. Together, we have the 
sign consistency: $\mathrm{sgn}\left(\hat{\boldsymbol{\beta}}\right) = \mathrm{sgn}\left(\boldsymbol{\beta}_0\right)$. Under this result, applying Theorem 1 of \citet{lv2009unified}, the estimation $\hat{\boldsymbol{\beta}}_{0,1}$ solves the following equation for $\boldsymbol{\gamma} \in \mathbb{R}^s$
$$
\boldsymbol{\gamma} = \tilde{\boldsymbol{\beta}}_{0,1}-\left(n^{-1}\mathbf{X}^{\prime}_1\mathbf{X}_1\right)^{-1}\mathbf{b}
$$
where 
\begin{itemize}
    \item $\mathbf{X}_1$ is the $n\times s$ submatrix of $\mathbf{X}$ corresponding to the $s$ non-zero regression coefficients $\beta_{0,j}$
    \item $\mathbf{b} = \left\{ \lambda_0 \mathbf{1}_s +p'_{\lambda}(\lvert \boldsymbol{\gamma} \rvert)  \right\} \circ \mathrm{sgn}(\tilde{\boldsymbol{\beta}}_{0,1})-n^{-1}\mathbf{X}'_1\boldsymbol{\epsilon}$, with componentwise derivative and product.
\end{itemize}
From the concavity and monotonicity of $p_{\lambda}(t)$ and Condition 2, we have 
$$
0 \leq p_{\lambda}'(t)\leq p_{\lambda}'\left\{ (1-c_1)\lambda \right\} \leq \lambda_0/4
$$
this gives that each component of $\hat{\boldsymbol{\beta}}_{0,1}$ has magnitude larger than $(1-c_1)\lambda$. Since $\lVert n^{-1}\mathbf{X}'_1\boldsymbol{\epsilon} \rVert _{\infty} \leq \lVert n^{-1}\mathbf{X}'\boldsymbol{\epsilon} \rVert _{\infty}\leq \frac{\lambda_0}{2}$ on the event $\mathcal{E}$, hence we have
\begin{align*}
    \mathrm{sgn}(\mathbf{b}) &= \mathrm{sgn}(\tilde{\boldsymbol{\beta}}_{0,1}), & \frac{\lambda_0}{2}&\leq \lVert \mathbf{b}\rVert _{\infty}\leq \frac{7\lambda_0}{4}
\end{align*}
which completes the proof for Theorem \ref{thm:global_minimizer_property}.

\paragraph*{How to understand Theorem \ref{thm:global_minimizer_property}?} 
\begin{itemize}
    \item False sign rate $\mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right) = O\left( \kappa^{-4} \left(\frac{\lambda_0}{\lambda}\right)^2 s \right)$  is asymptotically vanishing when $\lambda_0/\lambda\rightarrow 0$, outperforming Lasso, whose false sign rate is generally bounded by $O\left( \lambda_{\max} \right)$ with $\lambda_{\max}$ being the largest eigenvalue of Gram matrix $n^{-1}\mathbf{X}^{\prime}\mathbf{X}$; also outperforming concave method, whose false sign rate is generally of order $O(1)$. When \textbf{signal strength is stronger} and \textbf{$\lambda$ is chosen suitably}, sign consistency is stronger as well.
    \item Convergence rates of $\frac{1}{\sqrt{n}}\left\Vert \mathbf{X}\left( \hat{\boldsymbol{\beta}} -\boldsymbol{\beta}_0 \right) \right\Vert _2$ and $\left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert _q$ are the same as those in \citet{bickel2009simultaneous} for the $L_1$-component, and are consistent with the concave component of \citet{zhang2012general}. The bounds $O(\kappa^{-1}\lambda_0 s^{1/2}),O(\kappa^{-2}\lambda_0 s^{1/q})$ depend only on the universal regularization parameter $\lambda_0 = c\sqrt{\frac{\log p}{n}}$ for $L_1$-component, and are independent of $\lambda$ for concave component.
    \item The $L_{\infty}-$bound $\left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert _{\infty} = O\left(\lambda_0\left\Vert \left(\frac{1}{n}\mathbf{X}^{\prime}_1\mathbf{X}_1 \right)^{-1} \right\Vert _{\infty}\right)$ involves $\left\Vert \left(\frac{1}{n}\mathbf{X}^{\prime}_1\mathbf{X}_1 \right)^{-1} \right\Vert _{\infty}$, which is bounded from above by $\sqrt{s}\left\Vert \left(\frac{1}{n}\mathbf{X}^{\prime}_1\mathbf{X}_1 \right)^{-1} \right\Vert _2 \leq \sqrt{s}\kappa_0^{-2}$ and can be \textbf{dimension-free} in certain scenarios.
    \item \myhl[myblue]{\textbf{\underline{Oracle property}}}: Under all conditions of Theorem \ref{thm:global_minimizer_property} hold, and let $\tilde{\boldsymbol{\beta}}$ be the refitted least-squares estimator given by covariates in $\mathrm{supp}\left( \hat{\boldsymbol{\beta}} \right)$, with $\hat{\boldsymbol{\beta}}$ being the estimator in Theorem \ref{thm:global_minimizer_property}. The with probability $1-O(p^{-c_0})$, $\tilde{\boldsymbol{\beta}}$ equals the oracle estimator, and has the oracle property if the oracle estimaor is asymptotic normal.
\end{itemize}

\begin{theorem}{Further Properties of the Global Minimizer $\hat{\boldsymbol{\beta}}$}{global_minimizer_property_2}
    Under the same regularity conditions, the global minimizer $\hat{\boldsymbol{\beta}}$ in Theorem \ref{thm:global_minimizer_property} satisfies that $\forall \tau >0$
    \begin{align}
        \mathbb{E}\left\{ \frac{1}{n}\left\Vert \mathbf{X}\left( \hat{\boldsymbol{\beta}} -\boldsymbol{\beta}_0 \right) \right\Vert^2 _2 \right\} =& O\left(\kappa^{-2}\lambda^2_0s +m_{2,\tau}+\gamma \lambda_0p^{-c_0}\right) \\
        \mathbb{E}\left\{ \left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert^q _q \right\} = &O\left[ \kappa^{-2q}\lambda_0^q s + (2-q)\lambda_0^{-1}m_{2,\tau}+(q-1)\lambda^{-2}_0m_{4,\tau} + \left( (2-q)\gamma + (q-1)\gamma^2 \right)p^{-c_0} \right] \\
        \mathbb{E}\left\{\mathrm{FS}\left(\hat{\boldsymbol{\beta}} \right) \right\} =& O\left[ \kappa^{-4}\left( \frac{\lambda_0}{\lambda} \right)^s s +\lambda^{-2}m_{2,\tau} + \left(\frac{\gamma\lambda_0}{\lambda^2}+s\right)p^{-c_0} \right]
    \end{align}
    where $m_{q,\tau}=\mathbb{E}\left( \lvert \epsilon_0 \rvert^q \mathbf{1}_{ \{\lvert \epsilon_0 \rvert>\tau \} } \right)$ denotes the tail moment and $\gamma = \lVert \boldsymbol{\beta}_0 \rVert _1 + s\lambda_0^{-1}p_{\lambda}(\infty) + \tau^2 \lambda_0^{-1}$. If in addition $\lambda\geq 56(1-c_1)^{-1}\kappa^{-2}\lambda_0 \sqrt{s}$, then we  have 
    \begin{align*}
        \mathbb{E}\left\{\mathrm{FS}\left(\hat{\boldsymbol{\beta}} \right) \right\} =& O\left\{ \lambda^{-2} \right\} \\
        \mathbb{E}\left\{ \left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert _{\infty} \right\} =& O\left\{ \lambda_0 \lVert (n^{-1}\mathbf{X}'_1\mathbf{X}_1)^{-1} \rVert _{\infty} + \lambda_0^{-1}m_{2,\tau} + \gamma p^{-c_0} \right\}
    \end{align*}

\end{theorem}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}