\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{19}{Community Detection}{}{Sai Zhang}{.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Stochastic Block Model (Abbe et al., 2015)}

Consider an undirected graph $G$, with nodes $V$ and edges $E$. Let 
    \begin{itemize}
        \item $n$ be a positive integer: the number of \textbf{vertices}
        \item $k$ be a positive integer: the number of \textbf{communities}
        \item $p=\left(p_1,\cdots,p_k\right)$ be a probability vector on $\left\{ 1,\cdots,k \right\} \coloneq [k]$: the \textbf{prior} on the $k$ communities
        \item $\mathbf{W}$ be a $k\times k$ symmetric matrix with entries $ W_{ij}\in [0,1]$: the matrix of \textbf{connectivity probabilities}
    \end{itemize}
then we have
\begin{definition}{Stochastic Block Model}{stoch_block_model}
    The pair $(\mathbf{X},G)$ is drawn under $SBM(n,p,\mathbf{W})$ if $\mathbf{X}$ is an $n$ dimensional random vector with i.i.d. components distributed under $p$, and $G$ is an $n-$vertex simple graph 
    where vertices $i$ and $j$ are connected with probability $W_{X_i,X_j}$, \myhl[myred]{\textbf{independently}} of other pairs of vertices. And the \textbf{community} sets can be defined by 
    \begin{equation*}
        \Omega_i = \Omega_i(\mathbf{X}) \coloneq \left\{ v\in [n]: X_v=i \right\}, i\in [k]
    \end{equation*}
\end{definition}
Immediately, we can define the symmetry of SBM as:
\begin{definition}{Symmetric SBM}{symmetric_sbm}
    An SBM is called symmetric if
    \begin{itemize}
        \item $p$ is \textbf{uniform}
        \item $\mathbf{W}$ takes the same value \textbf{on the diagonal} and the same value \textbf{off the diagonal} 
    \end{itemize}
    $(\mathbf{X},G)$ is drawn under $SSBM(n,k,A,B)$ if $p=\left\{ 1/k \right\}^k$ and $\mathbf{W}$ takes avlue $A$ on the diagonal and $B$ off the diagonal.
\end{definition}

\subsection{Recovery}
The goal of community detection is to recover the labels $\mathbf{X}$ by observing $G$, up to some level of accuracy. First, define \myhl[myred]{\textbf{agreement}} as 
\begin{definition}{Agreement of Communities}{community_agreement}
    The agreement between two community vectors $\mathbf{x,y}\in [k]^n$ is obtained by maximizing the common components between $\mathbf{x}$ and any relabelling of $\mathbf{y}$, that is 
    \begin{equation*}
        A(\mathbf{x,y}) = \max_{\pi\in S_k}\frac{1}{n}\sum^n_{i=1}\mathbf{1}\left[x_i = \pi(y_i)\right]
    \end{equation*}
    where $S_k$ is the group of permutations on $[k]$.
\end{definition}
The \textbf{relabelling} permutation is used to handle symmetric communities such as in SSBM, as it is impossible to recover the actual labels in this case.
But it's possible to recover the \textbf{partition}. There are 2 types of partition recovery we consider 

\paragraph*{Exact Recovery} 
First, consider the case of \myhl[myred]{\textbf{exact recovery}}:
\begin{definition}{Exact Recovery}{exact_recovery}
   Let $(\mathbf{X},G)\sim SBM(n,p,W)$, the exact recovery is solved if there exists an algorithm that takes $G$ as an input and outpus $\hat{\mathbf{X}} = \hat{\mathbf{X}}(G)$ such that $\mathbb{P}\left\{ A(\mathbf{X},\hat{\mathbf{X}}) =1 \right\} = 1-o_p(1)$ 
\end{definition}
In the SSBM case, algorithms that guarantee $$A(\mathbf{X},\hat{\mathbf{X}}) \rightarrow \frac{1}{k}$$ would be trivial.

\paragraph*{Weak Recovery} 
On the other hand, we the case of \myhl[myred]{\textbf{weak recovery}} defined as 
\begin{definition}{Weak Recovery}{weak_recovery}
    Weak recovery or detection is solved $SSBM(n,k,A,B)$ if for $(\mathbf{X},G)\sim SSBM(n,k,A,B)$, then $\exists \epsilon >0 $ and an algorithm that takes $G$ as an input and outputs $\hat{\mathbf{X}}$ such that 
    $$
    \mathbb{P}\left\{ A(\mathbf{X},\hat{\mathbf{X}})\geq \frac{1}{k} + \epsilon \right\} = 1-o(1)
    $$
\end{definition}

\subsection{Example: SSBM(n,2)}
Let's look at the example of $SSBM(n,2,\alpha\frac{\log n}{n},\beta \frac{\log n}{n})$, where 
\begin{itemize}
    \item $n$: number of vertices (assumed to be even for simplicity)
    \item for each $v\in [n]$, a binary label $X_v$ is attached s.t. $$ \left\vert \left\{ v\in [n]: X_v =1 \right\} \right\vert = n/2 $$
    \item for each pair of distinct nodes $u,v\in[n]$, an edge is placed with probability
    \begin{itemize}
        \item $\alpha \frac{\log n}{n}$ if $X_u=X_v$
        \item $\beta\frac{\log n}{n}$ if $X_u \neq X_v$
    \end{itemize}
    where edges are placed independently conditionally on the vertex labels
    \item WLOG, $\alpha > \beta$
\end{itemize}
then we have the following theorem 
\begin{theorem}{Exact Recovery in $SSBM(n,2,\alpha \log (n)/n,\beta \log(n)/n)$}{exact_recover_ssbm_n2}
    \begin{itemize}
        \item Exact recovery in $SSBM(n,2,\alpha \log (n)/n,\beta \log(n)/n)$ is solvable and efficiently so if $\left\vert \sqrt{\alpha} -\sqrt{\beta} \right\vert > \sqrt{2}$ nad unsolvable if $\left\vert \sqrt{\alpha} -\sqrt{\beta} \right\vert < \sqrt{2}$
        \item Exact recovery of the ground truth assignment of the partition $(A,B)$ is also achieveable, that is: if 
        $$ \frac{\alpha+\beta}{2} - \sqrt{\alpha\beta} >1 $$
        i.e. 
        $$ \alpha+\beta >2,\ \left(\alpha-\beta\right)^2 > 4\left(\alpha+\beta\right) - 4 $$
        the maximum likelihood estimator exactly recovers the communities (up to a global flip), with high probability.
    \end{itemize}
\end{theorem}
See \citet{abbe2017community} for the proof of this theorem. 

In summary, for a graph structure $G=(V,E)$ represented by adjacency matrix $\mathbf{X}_{n\times n}$, Stochastic Block Model (SBM)
\begin{itemize}
    \item assumes that there is a symmetric matrix $\mathbf{P} = \left\{ p_{ij} \right\} \in \mathbb{R}^{k\times k}$, for $k \ll n$ and a map $C:\left\{1,\cdots,n\right\} \rightarrow \left\{1,\cdots,k\right\}$, s.t. $\Pr\left(\mathbf{X}_{ij}=1\right) = \mathbf{P}_{C(i),C(i)}$
    \item Define $\boldsymbol{\Pi} = \left(\pi_1,\cdots,\pi_n\right)'\in \mathbb{R}^{n\times k}$ where $\boldsymbol{\Pi}_{ij}=1$ if $C(i)=j$, and $\boldsymbol{\Pi}_{ij}=0$ otherwise
    \item Let $\mathbf{H}= \mathbb{E}(\mathbf{X})$ be the probability matrix, then $\mathbf{H} = \boldsymbol{\Pi}\mathbf{P}\boldsymbol{\Pi}'$
    \item A variant of SBM is degree corrected SBM which incorporates the degree heterogeneity.
    \begin{itemize}
        \item each node is assigned a parameter $\theta_i >0$ such that $\Pr \left(\mathbf{X}_{ij}=1\right) = \theta_i\theta_j \mathbf{P}_{C(i),C(j)}$
        \item $\mathbf{H} = \boldsymbol{\Theta\Pi}\mathbf{P}\boldsymbol{\Pi'\Theta}$, where $\boldsymbol{\Theta} = \mathrm{diag} \left(\theta_1,\cdots,\theta_n\right)$
    \end{itemize}
\end{itemize}

\section{SIMPLE Model (Fan et al., 2022)}
In SBM, each $\pi_i\in\left\{e_1,\cdots,e_K \right\}$ with $e_k$ a one entry vector whose $k$-th component is one. But what if each node $i$ can belong to $K$ different communities? We generalize $\pi_i$ to be a compositional vector, and interpret it as community membership profile for node $i$, then 
$$
\Pr\left(\mathbf{X}_{ij}=1\right) = \theta_i\theta_j \sum^K_{k=1} \sum^K_{l=1} \pi_i(k)\pi_j(l)p_{kl}
$$
and $\mathbf{H} = \boldsymbol{\Theta\Pi}\mathbf{P}\boldsymbol{\Pi'\Theta}$. Now, consider a new statistical tests for testing whether any given pair of nodes share the same membership profiles, and providing the associated $p$-values.

\subsection{Problem Setting}
For an undirected graph $G=(V,E)$ with $n$ nodes, let $\mathbf{X}=\left\{x_{ij}\right\}\in \mathbb{R}^{n\times n}$ be the \textbf{symmetric} adjacency matrix. Under a probabilistic model, assume $x_{ij}$ is an independent realization from a Bernoulli random variable for all upper triangular entries of random matrix $\mathbf{X}$.
Consider the adjacency matrix with the deterministic-random latent structure $$ \mathbf{X=H+W} $$
where 
\begin{itemize}
    \item $\mathbf{H} = \left\{ h_{ij} \right\} \in \mathbb{R}^{n\times n}$ is the deterministic mean matrix of low rank $K\geq 1$
    \item $\mathbf{W} = \left\{ w_{ij} \right\} \in \mathbb{R}^{n\times n}$ is a symmetric random matrix with zero mean and independent entries on and above the diagonal
\end{itemize}

Assume $V$ is decomposed into $K$ disjoint latent communities $$ C_1, \cdots, C_K $$ where each node $i$ is associated with the community membership probability vector $$\boldsymbol{\pi}_i = \left( \pi_i(1),\cdots,\pi_i(K) \right)' \in \mathbb{R}^K$$
s.t. $$ \Pr(i\in C_k) = \pi_i(k),\ k=1,\cdots,K $$ here, $K$ is unknown but bounded away from $\infty$.

\subsection{Hypothesis Testing}
For any given pair of nodes $i\neq j\in V$, the goal is to infer whether they share the same community identity with quantified uncertainty level based on adjacency matrix $\mathbf{X}$, the hypothesis is
\begin{align*}
    H_0 &: \pi_i=\pi_j & H_1: &\pi_i\neq \pi_j
\end{align*}
More explicitly, consider the DCMM (Degree Corrected Mixed Membership) model as the underlying network model, s.t. the probability of a link between nodes $i$ and $j$ can be written as 
\begin{align*}
    \Pr(\mathbf{X}_{ij}=1) = \theta_i \theta_j \sum^K_{k=1}\sum^K_{l=1}\pi_i(k)\pi_j(l)p_{kl}
\end{align*}
and 
$$
\mathbf{H} = \boldsymbol{\Theta\Pi}\mathbf{P}\boldsymbol{\Pi'\Theta}
$$
in matrix form, where $\boldsymbol{\Pi} = \left(\pi_1,\cdots,\pi_n\right)'\in \mathbb{R}^{n\times k}$ and $\boldsymbol{\Theta} = \mathrm{diag}\left(\theta_1,\cdots,\theta_n\right)$. Consider 
\begin{itemize}
    \item No degree homogeneity: $\boldsymbol{\Theta} = \sqrt{\theta}\mathbf{I}_n$, then $\mathbf{H} = \theta \boldsymbol{\Pi}\mathbf{P}\boldsymbol{\Pi}'$. If we eigen-decompose $\mathbf{H} = \mathbf{VDV}'$ where $\mathbf{D} = \mathrm{diag}\left(d_1,\cdots,d_K\right)$ with 
    $\left\vert d_1 \right\vert \geq \left\vert d_2 \right\vert \geq \cdots \left\vert d_K \right\vert > 0$ is the matrix of all $K$ non-zero eigenvalues and $V=\left(v_1,\cdots, v_K\right)\in \mathbb{R}^{n\times K}$ is the eigenvectors.
    \begin{itemize}
        \item the column space spanned by $\boldsymbol{\Pi}$ is the same as the eigenspace spanned by the top $K$ eigenvectors of matrix $\mathbf{H}$
        \item mean matrix $\mathbf{H}$ is \textbf{not} observable: replace it with adjacency matrix $\mathbf{X}$ and conduct eigen-decomposition to get eigenvalues $\hat{d}_1,\cdots,\hat{d}_n$ and eigenvectors $\hat{v}_1,\cdots,\hat{v}_n$. We assume that 
        $$ \left\vert \hat{d}_1 \right\vert \geq \left\vert \hat{d}_2 \right\vert \geq \cdots \geq \left\vert \hat{d}_n \right\vert $$
        and let $\hat{\mathbf{V}} = \left(\hat{v}_1,\cdots,\hat{v}_K\right) \in \mathbf{R}^{n\times K}$.
    \end{itemize}
\end{itemize}

\paragraph*{Without degree heterogeneity} first, consider the case where $\boldsymbol{\Theta}=\sqrt{\theta}\mathbf{I}_n$ and $\mathbb{E}\left(\mathbf{X}\right) = \mathbf{H} = \theta\boldsymbol{\Pi}\mathbf{P}\boldsymbol{\Pi}'$. If $\pi_i = \pi_j$, then nodes $i$ and $j$ are exchangeable and $\mathbf{V}(i)=\mathbf{V}(j)$. The test statistic for membership information of node $i$ and $j$ is given as 
$$
T_{ij} = \left[\hat{\mathbf{V}}(i) - \hat{\mathbf{V}}(j)\right]'\boldsymbol{\Sigma}_1^{-1} \left[\hat{\mathbf{V}}(i) - \hat{\mathbf{V}}(j)\right]
$$
where $\boldsymbol{\Sigma}_1^{-1} = \mathrm{Cov}\left[(e_i - e_j)' \mathbf{WVD}^{-1}\right]$ is the asymptotic variance of $\left[\hat{\mathbf{V}}(i) - \hat{\mathbf{V}}(j)\right]$. The regularity conditions are 
\begin{itemize}
    \item[\textbf{C1}] $\exists c_0 >0$ s.t. $$ \min \left\{ \frac{\left\vert d_i \right\vert}{\left\vert d_j \right\vert}: 1\leq i\leq j\leq K, d_i\neq -d_j \right\} \geq 1+c_0 $$
    \item[\textbf{C2}] $\exists c_0\in (0,1), c_2\in [0,1/2), c_1\in (0,1-2c_2)$ s.t. $\lambda_k\left(\boldsymbol{\Pi'\Pi}\right) \geq c_0 n$, $\lambda_K(\mathbf{P})\geq n^{-c_2}$ and $\theta \geq n^{-c_1}$
    \item[\textbf{C3}] as $n\rightarrow \infty$, all the eigenvalues of $\theta^{-1}\mathbf{D}\boldsymbol{\Sigma}_1\mathbf{D}$ are bounded away from $0$ and $\infty$
\end{itemize}

and the test statistics follow the theorem 
\begin{theorem}{Test Statistics Distribution}{SIMPLE-stat-dist}
    Under Condition \textbf{C1} and \textbf{C2}, and $\boldsymbol{\Theta} = \sqrt{\theta}\mathbf{I}_n$,
    \begin{itemize}
        \item If \textbf{C3} holds too, then under the null
        $$ H_0: T_{ij}\xrightarrow{\mathcal{D}} \chi^2_K $$
        as $n\rightarrow\infty$, where $\chi^2_K$ is the chi-square distribution with $K$ degrees of freedom
        \item under the alternative,
        \begin{itemize}
            \item if $n^{1/2-c_2}\sqrt{\theta}\left\Vert \pi_i -\pi_j \right\Vert \rightarrow \infty$, then for arbitrarily large constant $C>0$, we have $$\Pr\left(T_{ij}>C\right) \xrightarrow{n\rightarrow\infty} 1$$
            \item in addition, if Condition \textbf{C3} holds, $c_2=0$, $\left\Vert \pi_i-\pi_j \right\Vert \sim \frac{1}{\sqrt{n\theta}}$, and 
            $$ \left[\mathbf{V}(i)-\mathbf{V}(j)\right]'\boldsymbol{\Sigma}^{-1}_1 \left[\mathbf{V}(i)-\mathbf{V}(j)\right] \rightarrow \mu $$, then 
            $$
            T_{ij} \xrightarrow{\mathcal{D}} \chi^2_{K}(\mu)
            $$
            as $n\rightarrow \infty$, where $\chi^2_{K}(\mu)$ is a noncentral chi-square distribution with mean $\mu$ and $K$ degrees of freedom.
        \end{itemize}
    \end{itemize}
\end{theorem}
Under the joint null $H_{0,ij}: \pi_i=\pi j,\forall 1\leq i\neq j\leq n$, a uniform version of Thm.\ref{thm:SIMPLE-stat-dist} is 
$$
\lim_{n\rightarrow\infty} \sup_{1\leq i\neq j \leq n} \left\vert \Pr \left(T_{ij}\leq x \right) - \Pr \left( X\leq x \right) \right\vert =0,\forall x \in \mathbf{R}
$$
where $X\sim \chi^2_{K}$. But the test statistic $T_{ij}$ is not directly applicable since the population parameters $K$ and $\boldsymbol{\Sigma}_1$. For consistent 
estimators satisfying the following condition 
\begin{align*}
    \Pr(\hat{K}=K)&= 1-o(1)\\
    \theta^{-1}\left\Vert \mathbf{D}\left(\hat{\mathbf{S}}_1 - \boldsymbol{\Sigma}_1 \right) \mathbf{D} \right\Vert _2 &= o(1) 
\end{align*}
then the asymptotic results in Thm. \ref{thm:SIMPLE-stat-dist} holds.

\paragraph*{With degree heterogeneity}
Define componentwise ratio 
\begin{align*}
    Y(i,k) &= \frac{\hat{v}_k (i)}{\hat{v}_1 (i)}, & 1\leq i <n,&2\leq k\leq K
\end{align*}
where $\hat{v}_k(i)$ is the $i$-th component of $k$-th eigenvector of $\mathbf{X}$. Due to the \textbf{exchangeability} of nodes $i$ and $j$,
under the null it holds that 
\begin{align*}
    \frac{v_k(j)}{v_1(j)} &= \frac{v_k(j)}{v_1(j)}, & 2\leq k&\leq K
\end{align*}
Denote $\mathbf{Y}_i = \left(Y(i,2),\cdots,Y(i,K)\right)'$, the new test statistics is proposed as 
$$
G_{ij} =\left(\mathbf{Y}_i - \mathbf{Y}_j\right)'\boldsymbol{\Sigma}_2^{-1}\left(\mathbf{Y}_i-\mathbf{Y}_j\right)
$$
where $\boldsymbol{\Sigma}_2$ is the asymptotic variance of $\mathbf{Y}_i -\mathbf{Y}_j$, which is much harder to derive and estimate. So we need to 
impose four other conditions in addition to Condition C1-C3: 
\begin{itemize}
    \item[\textbf{C4}] $\exists c_2 \in [0,1/2), c_3\in (0,1-2c_2), c_4>0, c_5\in (0,1)$ s.t. 
    \begin{align*}
        \lambda_K(\mathbf{P}) &\geq n^{-c_2} & \min_{1\leq k\leq K} \left\vert \mathcal{N}_k \right\vert & \geq c_5 n & \theta_{\max} &\leq c_4 \theta_{\min} & \theta^2_{\min} &\geq n^{-c_3}
    \end{align*}
    \item[\textbf{C5}] $\mathbf{P} = (p_{kl})$ is positive definite, irreducible and has unit diagonal entries, moreover 
    $$
        n \min_{1\leq k \leq K,t=i,j} \mathrm{Var}\left(\mathbf{e}'_t \mathbf{W v_k}\right) \sim n\theta^2_{\max} \rightarrow \infty
    $$
    \item[\textbf{C6}] all the eigenvalues of $$ \left(n\theta^2_{\max}\right)^{-1} \mathbf{D}\mathrm{Cov}(f)\mathbf{D} $$ are bounded away from 0 and $\infty$
    \item[\textbf{C7}] Let $\eta_1$ be the first right singular vector of $\mathbf{P}\boldsymbol{\Pi'\Theta^2\Pi}$, it holds that 
    \begin{align*}
        \min_{1\leq k\leq K} \eta_1(k)&>0 & \frac{\max_{1\leq k\leq K}\eta_1(k)}{\min_{1\leq k\leq K}\eta_1(k)} &\leq C
    \end{align*}
    for some positive $C$, where $\eta_1(k)$ is the $k$-th entry of $\eta_1$.
\end{itemize}

Then we have 
\begin{theorem}{Test Statistic Distribution with Degree Heterogeneity}{stat_dist_hetero}
    Under Condition \textbf{C1, C4-C7}, with degree heterogeneity, 
    \begin{itemize}
        \item under the null, $$ G_{ij} \xrightarrow{\mathcal{D}} \chi^2_{K-1} $$
        \item under the alternative with $\lambda_2 \left(\boldsymbol{\pi}_i\boldsymbol{\pi}_i' + \boldsymbol{\pi}_i \boldsymbol{\pi}_j'\right) \gg \frac{1}{n^{1-2_c\theta^2_{\min}}}$, for any arbitrarily large constant $C>0$, $$ \Pr(G_{ij}>C)\xrightarrow{n\rightarrow\infty} 1 $$
    \end{itemize}
\end{theorem}
notice that $K$ and $\boldsymbol{\Sigma}_2$ are both unknown, we must have 
\begin{itemize}
    \item for estimator $\hat{\mathbf{S}}_2$ of $\boldsymbol{\Sigma}_2$, we need $$\left(n\theta^2_{\max}\right)^{-1} \left\Vert \mathbf{D}\left(\hat{\mathbf{S}}_2 -\boldsymbol{\Sigma}^2\right)\mathbf{D} \right\Vert _2 = o_p(1)$$ replace $\boldsymbol{\Sigma}_2$ with $\hat{\mathbf{S}}_2$
    \item for $K$, under Condition \textbf{C1}, and $\left\vert d_K \right\vert \gg \sqrt{\log(n)}\alpha_n$ and $\alpha_n\geq n^{c_5}$ for some positive constant $c_5$, a consistent thresholding estimator is defined 
    $$
    \hat{K} = \left\vert \left\{ \hat{d}_i: \hat{d}_i^2 > 2.01(\log n) \check{d}_n,i\in[n] \right\} \right\vert 
    $$
    where the constant 2.01 can be replace with any other constant slightly larger than 2, and $$\check{d}_n = \max_{1\leq l\leq n} \sum^n_{j=1}\mathbf{X}_{lj}$$ is the maximum degree of the network. For $\hat{K}$ to be consistent, we need 
    \begin{itemize}
        \item Condition \textbf{C1} holds
        \item $\left\vert d_K \right\vert \gg \sqrt{\log(n)}\alpha_n$, where $\alpha_n\geq n^{c_5}$ for some constant $c_5>0$
    \end{itemize}
\end{itemize}

\section{Rank Inference via Residual Subsampling}
Again, consider $n\times n$ symmetric random matrix $\tilde{\mathbf{X}}$ and its decomposition 
\begin{equation*}
    \tilde{\mathbf{X}} = \mathbf{H+W}
\end{equation*}
where
\begin{itemize}
    \item $\mathbf{H} = \mathbb{E}\left(\tilde{\mathbf{X}}\right)$ with some fixed but unknown rank $K \ll n$, it can be eigen-decomposed as $$ \mathbf{H} = \mathbf{VDV}' $$ where $\mathbf{D} = \mathrm{diag}(d_1,\cdots,d_K)$ are the non-zero eigenvalues of $\mathbf{H}$ in decreasing magnitude and $\mathbf{V}= \left(\mathbf{v}_1,\cdots,\mathbf{v}_K\right)$ are the corresponding eigenvectors
    \item $\mathbf{W}$ has bounded and independent entries on and above the diagonals
\end{itemize}

for a simple case (networks with self loops), when the observed data matrix $\mathbf{X} = \tilde{\mathbf{X}} = \mathbf{H+W}$, then we have 
\begin{equation*}
    \begin{aligned}
        \frac{\sum^n_{i=1}w_{ii}}{\sqrt{\sum^n_{i=1}\mathbb{E}w_{ii}^2}}&\xrightarrow{d}\mathcal{N}(0,1) \\
        \frac{\sum^n_{i=1}\mathbb{E}w_{ii}^2}{\sum^n_{i=1}w^2_{ii}} &\xrightarrow{p} 1
    \end{aligned} \Rightarrow \frac{\sum^n_{i=1}w_{ii}}{\sqrt{\sum^n_{i=1}w_{ii}^2}}\xrightarrow{d}\mathcal{N}(0,1)
\end{equation*}

Let $\hat{\mathbf{V}}\hat{\mathbf{D}}\hat{\mathbf{V}}' = \sum^{K_0}_{k=1}\hat{d}_k \hat{\mathbf{v}}_k \hat{\mathbf{v}}_k'$ be an estimate of $\mathbf{H}$ with rank $K=K_0$, then 
\begin{equation*}
    \hat{\mathbf{W}} = \left(\hat{w}_{ij}\right) = \mathbf{X} - \sum^{K_0}_{k=1}\hat{d}_k \hat{\mathbf{v}}_k \hat{\mathbf{v}}_k'
\end{equation*}
a test of the form 
$$
\tilde{T}_n = \frac{\sum^n_{i=1}\hat{\mathbf{w}}_{ij}}{\sqrt{\sum^n_{i=1}\hat{\mathbf{w}}_{ij}^2}}
$$
can be used. But what if $\mathrm{diag}(\mathbf{X})=0$, that is, the network doesn't contain \textbf{self loops}?

\paragraph*{Network without self loops} In the absence of self loops, the observed data matrix $\mathbf{X}$ takes the form $$ \tilde{\mathbf{X}} - \mathrm{diag}\left(\tilde{\mathbf{X}}\right) $$ and thus $\hat{\mathbf{W}}$ actually estimates $$\mathbf{W} - \mathrm{diag}\left(\tilde{\mathbf{X}}\right)$$
also, the entries of $\hat{\mathbf{W}}$ are all correlated: aggregating too many entries will cause too much noise accumulation and invalidate the normality assumption.

\paragraph*{Rank inference via residual subsampling (RIRS)} Define i.i.d. Bernoulli random variables $Y_{ij}$ with $ \Pr \left(Y_{ij}=1\right) = \frac{1}{m},1\leq i<j \leq n$, then the universal RIRS test takes the form 
$$ 
T_n = \frac{\sqrt{m} \sum_{i\neq j}\hat{\mathbf{w}}_{ij}\mathbf{Y}_{ij}}{\sqrt{2\sum_{i\neq j}\hat{\mathbf{w}}^2_{ij}}}
$$
$m$ controls (on average) how many entries of the residual matrix to use in calculating the statistic,  and it needs to diverge fast for CLT, but not too fast to avoid noise accumulation

\paragraph*{Conditions} there are several conditions necessary for this method
\begin{itemize}
    \item[\textbf{C1}] $\mathbf{W}$ is symmetric, upper triangular entries and diagonals independent and bounded. $\mathbb{E}\mathbf{w}_{ij}=0,i\neq j$
    \item[\textbf{C2}] $\exists c_0$ s.t. $\frac{\left\vert d_i \right\vert }{\left\vert d_j \right\vert}\geq 1+c_0$, $\forall 1\leq i<j \leq K, d_i\neq -d_j$
    \item[\textbf{C3}] $\exists \theta_n>0$, $\theta_n\xrightarrow{n\rightarrow\infty} 0$
    \item[\textbf{C4}]
    \item[\textbf{C5}]  
\end{itemize}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}