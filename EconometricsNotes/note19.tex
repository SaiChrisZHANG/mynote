\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{19}{Community Detection}{}{Sai Zhang}{.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Stochastic Block Model (Abbe et al., 2015)}

Consider an undirected graph $G$, with nodes $V$ and edges $E$. Let 
    \begin{itemize}
        \item $n$ be a positive integer: the number of \textbf{vertices}
        \item $k$ be a positive integer: the number of \textbf{communities}
        \item $p=\left(p_1,\cdots,p_k\right)$ be a probability vector on $\left\{ 1,\cdots,k \right\} \coloneq [k]$: the \textbf{prior} on the $k$ communities
        \item $\mathbf{W}$ be a $k\times k$ symmetric matrix with entries $ W_{ij}\in [0,1]$: the matrix of \textbf{connectivity probabilities}
    \end{itemize}
then we have
\begin{definition}{Stochastic Block Model}{stoch_block_model}
    The pair $(\mathbf{X},G)$ is drawn under $SBM(n,p,\mathbf{W})$ if $\mathbf{X}$ is an $n$ dimensional random vector with i.i.d. components distributed under $p$, and $G$ is an $n-$vertex simple graph 
    where vertices $i$ and $j$ are connected with probability $W_{X_i,X_j}$, \myhl[myred]{\textbf{independently}} of other pairs of vertices. And the \textbf{community} sets can be defined by 
    \begin{equation*}
        \Omega_i = \Omega_i(\mathbf{X}) \coloneq \left\{ v\in [n]: X_v=i \right\}, i\in [k]
    \end{equation*}
\end{definition}
Immediately, we can define the symmetry of SBM as:
\begin{definition}{Symmetric SBM}{symmetric_sbm}
    An SBM is called symmetric if
    \begin{itemize}
        \item $p$ is \textbf{uniform}
        \item $\mathbf{W}$ takes the same value \textbf{on the diagonal} and the same value \textbf{off the diagonal} 
    \end{itemize}
    $(\mathbf{X},G)$ is drawn under $SSBM(n,k,A,B)$ if $p=\left\{ 1/k \right\}^k$ and $\mathbf{W}$ takes avlue $A$ on the diagonal and $B$ off the diagonal.
\end{definition}

\subsection{Recovery}
The goal of community detection is to recover the labels $\mathbf{X}$ by observing $G$, up to some level of accuracy. First, define \myhl[myred]{\textbf{agreement}} as 
\begin{definition}{Agreement of Communities}{community_agreement}
    The agreement between two community vectors $\mathbf{x,y}\in [k]^n$ is obtained by maximizing the common components between $\mathbf{x}$ and any relabelling of $\mathbf{y}$, that is 
    \begin{equation*}
        A(\mathbf{x,y}) = \max_{\pi\in S_k}\frac{1}{n}\sum^n_{i=1}\mathbf{1}\left[x_i = \pi(y_i)\right]
    \end{equation*}
    where $S_k$ is the group of permutations on $[k]$.
\end{definition}
The \textbf{relabelling} permutation is used to handle symmetric communities such as in SSBM, as it is impossible to recover the actual labels in this case.
But it's possible to recover the \textbf{partition}. There are 2 types of partition recovery we consider 

\paragraph*{Exact Recovery} 
First, consider the case of \myhl[myred]{\textbf{exact recovery}}:
\begin{definition}{Exact Recovery}{exact_recovery}
   Let $(\mathbf{X},G)\sim SBM(n,p,W)$, the exact recovery is solved if there exists an algorithm that takes $G$ as an input and outpus $\hat{\mathbf{X}} = \hat{\mathbf{X}}(G)$ such that $\mathbb{P}\left\{ A(\mathbf{X},\hat{\mathbf{X}}) =1 \right\} = 1-o_p(1)$ 
\end{definition}
In the SSBM case, algorithms that guarantee $$A(\mathbf{X},\hat{\mathbf{X}}) \rightarrow \frac{1}{k}$$ would be trivial.

\paragraph*{Weak Recovery} 
On the other hand, we the case of \myhl[myred]{\textbf{weak recovery}} defined as 
\begin{definition}{Weak Recovery}{weak_recovery}
    Weak recovery or detection is solved $SSBM(n,k,A,B)$ if for $(\mathbf{X},G)\sim SSBM(n,k,A,B)$, then $\exists \epsilon >0 $ and an algorithm that takes $G$ as an input and outputs $\hat{\mathbf{X}}$ such that 
    $$
    \mathbb{P}\left\{ A(\mathbf{X},\hat{\mathbf{X}})\geq \frac{1}{k} + \epsilon \right\} = 1-o(1)
    $$
\end{definition}

\subsection{Example: SSBM(n,2)}
Let's look at the example of $SSBM(n,2,\alpha\frac{\log n}{n},\beta \frac{\log n}{n})$, where 
\begin{itemize}
    \item $n$: number of vertices (assumed to be even for simplicity)
    \item for each $v\in [n]$, a binary label $X_v$ is attached s.t. $$ \left\vert \left\{ v\in [n]: X_v =1 \right\} \right\vert = n/2 $$
    \item for each pair of distinct nodes $u,v\in[n]$, an edge is placed with probability
    \begin{itemize}
        \item $\alpha \frac{\log n}{n}$ if $X_u=X_v$
        \item $\beta\frac{\log n}{n}$ if $X_u \neq X_v$
    \end{itemize}
    where edges are placed independently conditionally on the vertex labels
    \item WLOG, $\alpha > \beta$
\end{itemize}
then we have the following theorem 
\begin{theorem}{Exact Recovery in $SSBM(n,2,\alpha \log (n)/n,\beta \log(n)/n)$}{exact_recover_ssbm_n2}
    \begin{itemize}
        \item Exact recovery in $SSBM(n,2,\alpha \log (n)/n,\beta \log(n)/n)$ is solvable and efficiently so if $\left\vert \sqrt{\alpha} -\sqrt{\beta} \right\vert > \sqrt{2}$ nad unsolvable if $\left\vert \sqrt{\alpha} -\sqrt{\beta} \right\vert < \sqrt{2}$
        \item Exact recovery of the ground truth assignment of the partition $(A,B)$ is also achieveable, that is: if 
        $$ \frac{\alpha+\beta}{2} - \sqrt{\alpha\beta} >1 $$
        i.e. 
        $$ \alpha+\beta >2,\ \left(\alpha-\beta\right)^2 > 4\left(\alpha+\beta\right) - 4 $$
        the maximum likelihood estimator exactly recovers the communities (up to a global flip), with high probability.
    \end{itemize}
\end{theorem}
See \citet{abbe2017community} for the proof of this theorem. 

In summary, for a graph structure $G=(V,E)$ represented by adjacency matrix $\mathbf{X}_{n\times n}$, Stochastic Block Model (SBM)
\begin{itemize}
    \item assumes that there is a symmetric matrix $\mathbf{P} = \left\{ p_{ij} \right\} \in \mathbb{R}^{k\times k}$, for $k \ll n$ and a map $C:\left\{1,\cdots,n\right\} \rightarrow \left\{1,\cdots,k\right\}$, s.t. $\Pr\left(\mathbf{X}_{ij}=1\right) = \mathbf{P}_{C(i),C(i)}$
    \item Define $\boldsymbol{\Pi} = \left(\pi_1,\cdots,\pi_n\right)'\in \mathbb{R}^{n\times k}$ where $\boldsymbol{\Pi}_{ij}=1$ if $C(i)=j$, and $\boldsymbol{\Pi}_{ij}=0$ otherwise
    \item Let $\mathbf{H}= \mathbb{E}(\mathbf{X})$ be the probability matrix, then $\mathbf{H} = \boldsymbol{\Pi}\mathbf{P}\boldsymbol{\Pi}'$
    \item A variant of SBM is degree corrected SBM which incorporates the degree heterogeneity.
    \begin{itemize}
        \item each node is assigned a parameter $\theta_i >0$ such that $\Pr \left(\mathbf{X}_{ij}=1\right) = \theta_i\theta_j \mathbf{P}_{C(i),C(j)}$
        \item $\mathbf{H} = \boldsymbol{\Theta\Pi}\mathbf{P}\boldsymbol{\Pi'\Theta}$, where $\boldsymbol{\Theta} = \mathrm{diag} \left(\theta_1,\cdots,\theta_n\right)$
    \end{itemize}
\end{itemize}

\section{SIMPLE Model (Fan et al., 2022)}
In SBM, each $\pi_i\in\left\{e_1,\cdots,e_K \right\}$ with $e_k$ a one entry vector whose $k$-th component is one. But what if each node $i$ can belong to $K$ different communities? We generalize $\pi_i$ to be a compositional vector, and interpret it as community membership profile for node $i$, then 
$$
\Pr\left(\mathbf{X}_{ij}=1\right) = \theta_i\theta_j \sum^K_{k=1} \sum^K_{l=1} \pi_i(k)\pi_j(l)p_{kl}
$$
and $\mathbf{H} = \boldsymbol{\Theta\Pi}\mathbf{P}\boldsymbol{\Pi'\Theta}$. Now, consider a new statistical tests for testing whether any given pair of nodes share the same membership profiles, and providing the associated $p$-values.

\subsection{Problem Setting}
For an undirected graph $G=(V,E)$ with $n$ nodes, let $\mathbf{X}=\left\{x_{ij}\right\}\in \mathbb{R}^{n\times n}$ be the \textbf{symmetric} adjacency matrix. Under a probabilistic model, assume $x_{ij}$ is an independent realization from a Bernoulli random variable for all upper triangular entries of random matrix $\mathbf{X}$.
Consider the adjacency matrix with the deterministic-random latent structure $$ \mathbf{X=H+W} $$
where 
\begin{itemize}
    \item $\mathbf{H} = \left\{ h_{ij} \right\} \in \mathbb{R}^{n\times n}$ is the deterministic mean matrix of low rank $K\geq 1$
    \item $\mathbf{W} = \left\{ w_{ij} \right\} \in \mathbb{R}^{n\times n}$ is a symmetric random matrix with zero mean and independent entries on and above the diagonal
\end{itemize}

Assume $V$ is decomposed into $K$ disjoint latent communities $$ C_1, \cdots, C_K $$ where each node $i$ is associated with the community membership probability vector $$\boldsymbol{\pi}_i = \left( \pi_i(1),\cdots,\pi_i(K) \right)' \in \mathbb{R}^K$$
s.t. $$ \Pr(i\in C_k) = \pi_i(k),\ k=1,\cdots,K $$ here, $K$ is unknown but bounded away from $\infty$.

\subsection{Hypothesis Testing}
For any given pair of nodes $i\neq j\in V$, the goal is to infer whether they share the same community identity with quantified uncertainty level based on adjacency matrix $\mathbf{X}$, the hypothesis is
\begin{align*}
    H_0 &: \pi_i=\pi_j & H_1: &\pi_i\neq \pi_j
\end{align*}
More explicitly, consider the DCMM (Degree Corrected Mixed Membership) model as the underlying network model, s.t. the probability of a link between nodes $i$ and $j$ can be written as 
\begin{align*}
    \Pr(\mathbf{X}_{ij}=1) = \theta_i \theta_j \sum^K_{k=1}\sum^K_{l=1}\pi_i(k)\pi_j(l)p_{kl}
\end{align*}
and 
$$
\mathbf{H} = \boldsymbol{\Theta\Pi}\mathbf{P}\boldsymbol{\Pi'\Theta}
$$
in matrix form, where $\boldsymbol{\Pi} = \left(\pi_1,\cdots,\pi_n\right)'\in \mathbb{R}^{n\times k}$ and $\boldsymbol{\Theta} = \mathrm{diag}\left(\theta_1,\cdots,\theta_n\right)$. Consider 
\begin{itemize}
    \item No degree homogeneity: $\boldsymbol{\Theta} = \sqrt{\theta}\mathbf{I}_n$, then $\mathbf{H} = \theta \boldsymbol{\Pi}\mathbf{P}\boldsymbol{\Pi}'$. If we eigen-decompose $\mathbf{H} = \mathbf{VDV}'$ where $\mathbf{D} = \mathrm{diag}\left(d_1,\cdots,d_K\right)$ with 
    $\left\vert d_1 \right\vert \geq \left\vert d_2 \right\vert \geq \cdots \left\vert d_K \right\vert > 0$ is the matrix of all $K$ non-zero eigenvalues and $V=\left(v_1,\cdots, v_K\right)\in \mathbb{R}^{n\times K}$ is the eigenvectors.
    \begin{itemize}
        \item the column space spanned by $\boldsymbol{\Pi}$ is the same as the eigenspace spanned by the top $K$ eigenvectors of matrix $\mathbf{H}$
        \item mean matrix $\mathbf{H}$ is \textbf{not} observable: replace it with adjacency matrix $\mathbf{X}$ and conduct eigen-decomposition to get eigenvalues $\hat{d}_1,\cdots,\hat{d}_n$ and eigenvectors $\hat{v}_1,\cdots,\hat{v}_n$. We assume that 
        $$ \left\vert \hat{d}_1 \right\vert \geq \left\vert \hat{d}_2 \right\vert \geq \cdots \geq \left\vert \hat{d}_n \right\vert $$
        and let $\hat{\mathbf{V}} = \left(\hat{v}_1,\cdots,\hat{v}_K\right) \in \mathbf{R}^{n\times K}$.
    \end{itemize}
\end{itemize}

\paragraph*{Without degree heterogeneity} first, consider the case where $\boldsymbol{\Theta}=\sqrt{\theta}\mathbf{I}_n$ and $\mathbb{E}\left(\mathbf{X}\right) = \mathbf{H} = \theta\boldsymbol{\Pi}\mathbf{P}\boldsymbol{\Pi}'$. If $\pi_i = \pi_j$, then nodes $i$ and $j$ are exchangeable and $\mathbf{V}(i)=\mathbf{V}(j)$. The test statistic for membership information of node $i$ and $j$ is given as 
$$
T_{ij} = \left[\hat{\mathbf{V}}(i) - \hat{\mathbf{V}}(j)\right]'\boldsymbol{\Sigma}_1^{-1} \left[\hat{\mathbf{V}}(i) - \hat{\mathbf{V}}(j)\right]
$$
where $\boldsymbol{\Sigma}_1^{-1} = \mathrm{Cov}\left[\right]$

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}