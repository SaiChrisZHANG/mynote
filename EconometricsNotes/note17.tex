\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{17}{False Discovery Rate (FDR) and Knockoffs}{}{Sai Zhang}{Constructing knockoff variables to control FDR when estimating regression coefficients.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Consider the classical linear regression setting
$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$
where $\boldsymbol{\beta}\in\mathbb{R}^p$ is the unknown vector of coefficients and $\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})$. In a high-dimensional problem, we would like to just select a subset of all variables $\hat{S}\subset \left\{ 1,\cdots,p \right\}$ s.t. conditional on $\left\{\mathbf{X}_j\right\}_{j\in\hat{S}}$, $\mathbf{y}$ is \textbf{independent} of all other variables, we can define the \myhl[myblue]{\textbf{False Discovery Rate}} \textbf{(FDR)} in can be defined as 
\begin{definition}{False Discovery Rate (FDR)}{FDR}
    \begin{align*}
        \mathrm{FDR} = \mathbb{E}(\mathrm{FDP}) = \mathbb{E}\left[ \frac{\lvert \hat{\mathcal{S}}\cap \mathcal{H}_0 \rvert}{ \lvert \hat{\mathcal{S}} \rvert } \textcolor{myred}{ =\frac{\#\left\{ j:j\in\hat{\mathcal{S}}\setminus \mathcal{S}\right\} }{\#\left\{ j:j\in\hat{\mathcal{S}}\right\} }} \right]
    \end{align*}
    where $\mathcal{H}_0\subset \left\{1,\cdots,p\right\}$ is the set of \myhl[myred]{\textbf{{null}}} variables: $\mathbf{X}_j$ is {\textbf{null}} iff $\mathbf{Y}$ is independent of $\mathbf{X}_j$ conditional on the other variables $\mathbf{X}_{-j}=\left\{\mathbf{X}_1,\cdots,\mathbf{X}_p\right\}\setminus \left\{\mathbf{X}_j\right\}$.
\end{definition}

In this note, we consider a series of knockoff-based methods to control FDR. They all follow a common procedure:
\begin{itemize}
    \item \textbf{\underline{Step 1}}: Construct Knockoffs 
    \item \textbf{\underline{Step 2}}: Calculate test statistics for both original and knockoff variables
    \item \textbf{\underline{Step 3}}: Calculate a threshold for the test statistics, controling for a desired FDR level
    \item \textbf{\underline{Step 4}}: Select variables that pass the threshold
\end{itemize}

\section{Barber and Candes (2015)}
\paragraph{Constructing the knockoffs}
\citet{Barber2015} construct the knockoffs by the following procedure
\begin{itemize}
    \item Calculate the Gram matrix $\boldsymbol{\Sigma}=\mathbf{X'X}$ for the normalized original variables, where $\Sigma_{jj}=\left\Vert \mathbf{X}_j \right\Vert^2_2 = 1$
    \item Construct the knockoffs $\tilde{\mathbf{X}}$ s.t. 
    \begin{align*}
        \tilde{\mathbf{X}}'\tilde{\mathbf{X}} &= \boldsymbol{\Sigma} & {\mathbf{X}}'\tilde{\mathbf{X}} = \boldsymbol{\Sigma} -\mathrm{diag}\left\{\mathbf{s}\right\}
    \end{align*}
    where $\mathbf{s}\in\mathbb{R}^p_{+}$ is a p-dimensional non-negative vector (larger $s_j$ indicates higher power) and
    \begin{itemize}
        \item $\tilde{\mathbf{X}}$ exhibits the \myhl[myblue]{\textbf{same}} covariance structrue as the original design $\mathbf{X}$
        \item The correlation between distinct original variables and knockoffs are the same as between the originals: $$ \mathbf{X}_j'\tilde{\mathbf{X}}_k = \mathbf{X}_j'{\mathbf{X}}_k,\ \forall j\neq k $$
        \item The correlation between the original variables and their own knockoffs is \myhl[myblue]{\textbf{less than 1}} $$ \mathbf{X}_j'\tilde{\mathbf{X}}_j = \Sigma_{jj}-s_j = 1-s_j $$
    \end{itemize}
    To construct such knockoffs, 
    \begin{itemize}
        \item Given a proper $\mathbf{s}$, if $n \geq  2p$, then  $$ \tilde{\mathbf{X}} = \mathbf{X}(\mathbf{I}-\boldsymbol{\Sigma}^{-1}\mathrm{diag}\left\{\mathbf{s}\right\}) + \tilde{\mathbf{U}}\mathbf{C} $$ where $\tilde{\mathbf{U}}\in \mathbb{R}^{n\times p}$ is an \myhl[myblue]{\textbf{orthonormal}} matrix s.t. $\tilde{\mathbf{U}}'\mathbf{X}=\mathbf{0}$ and $\mathbf{C'C}=2\mathrm{diag}\left\{\mathbf{s}\right\} - \mathrm{diag}\left\{\mathbf{s}\right\}\boldsymbol{\Sigma}^{-1}\mathrm{diag}\left\{\mathbf{s}\right\} \geq \mathbf{0}$
        \item A sufficient and necessary condition for $\tilde{\mathbf{X}}$ to exist: $\mathrm{diag}\left\{\mathbf{s}\right\} \leq 2\boldsymbol{\Sigma}$
    \end{itemize}
    2 types of knockoffs can be constructed, following these procedures
    \begin{itemize}
        \item[T1] \underline{\textbf{Equi-correlated}} knockoffs: set $s_j=2\lambda_{\min}(\boldsymbol{\Sigma}) \wedge 1$ for all $j$, then $\langle \mathbf{X}_j,\tilde{\mathbf{X}}_j \rangle = 1-2\lambda_{\min}(\boldsymbol{\Sigma}) \wedge 1$ for all $j$. This is essentially minimizing $\left\vert \langle \mathbf{X}_j,\tilde{\mathbf{X}}_j \rangle \right\vert$ 
        \item[T2] \underline{\textbf{SDP}} knockoffs: solve the convex problem \begin{align*}
            \arg\min_{\mathbf{x}} \sum_j (1-s_j) & & s.t. 0 \leq s_j \leq 1, \mathrm{diag}\left\{ \mathbf{s}\right\}\leq 2\boldsymbol{\Sigma}
        \end{align*}
        which is essentially minimizing the average of $ \langle \mathbf{X}_j,\tilde{\mathbf{X}}_j \rangle $ 
    \end{itemize}
\end{itemize}

\paragraph{Calculate test statistics} Define and calculate test statistics $W_j$ for each $\beta_j\in\left\{ 1,\cdots,p \right\}$ using $\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix}$:
\begin{itemize}
    \item the test statistic $W_j$ should be constructed s.t. large positive values are evidence against the null hypothesis $\beta_j =0$, for example, consider a Lasso on $\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix}$
\end{itemize}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}