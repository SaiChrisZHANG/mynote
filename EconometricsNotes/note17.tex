\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{17}{False Discovery Rate (FDR) and Knockoffs}{}{Sai Zhang}{Constructing knockoff variables to control FDR when estimating regression coefficients.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Consider the classical linear regression setting
$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$
where $\boldsymbol{\beta}\in\mathbb{R}^p$ is the unknown vector of coefficients and $\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})$. In a high-dimensional problem, we would like to just select a subset of all variables $\hat{S}\subset \left\{ 1,\cdots,p \right\}$ s.t. conditional on $\left\{\mathbf{X}_j\right\}_{j\in\hat{S}}$, $\mathbf{y}$ is \textbf{independent} of all other variables, we can define the \myhl[myblue]{\textbf{False Discovery Rate}} \textbf{(FDR)} in can be defined as 
\begin{definition}{False Discovery Rate (FDR)}{FDR}
    \begin{align*}
        \mathrm{FDR} = \mathbb{E}(\mathrm{FDP}) = \mathbb{E}\left[ \frac{\lvert \hat{\mathcal{S}}\cap \mathcal{H}_0 \rvert}{ \lvert \hat{\mathcal{S}} \rvert } \textcolor{myred}{ =\frac{\#\left\{ j:j\in\hat{\mathcal{S}}\setminus \mathcal{S}\right\} }{\#\left\{ j:j\in\hat{\mathcal{S}}\right\} }} \right]
    \end{align*}
    where $\mathcal{H}_0\subset \left\{1,\cdots,p\right\}$ is the set of \myhl[myred]{\textbf{{null}}} variables: $\mathbf{X}_j$ is {\textbf{null}} iff $\mathbf{Y}$ is independent of $\mathbf{X}_j$ conditional on the other variables $\mathbf{X}_{-j}=\left\{\mathbf{X}_1,\cdots,\mathbf{X}_p\right\}\setminus \left\{\mathbf{X}_j\right\}$.
\end{definition}

In this note, we consider a series of knockoff-based methods to control FDR. They all follow a common procedure:
\begin{itemize}
    \item \textbf{\underline{Step 1}}: Construct Knockoffs 
    \item \textbf{\underline{Step 2}}: Calculate test statistics for both original and knockoff variables
    \item \textbf{\underline{Step 3}}: Calculate a threshold for the test statistics, controling for a desired FDR level
    \item \textbf{\underline{Step 4}}: Select variables that pass the threshold
\end{itemize}

\section{Barber and Candes (2015)}
\paragraph{Constructing the knockoffs}
\citet{Barber2015} construct the knockoffs by the following procedure
\begin{itemize}
    \item Calculate the Gram matrix $\boldsymbol{\Sigma}=\mathbf{X'X}$ for the normalized original variables, where $\Sigma_{jj}=\left\Vert \mathbf{X}_j \right\Vert^2_2 = 1$
    \item Construct the knockoffs $\tilde{\mathbf{X}}$ s.t. 
    \begin{align*}
        \tilde{\mathbf{X}}'\tilde{\mathbf{X}} &= \boldsymbol{\Sigma} & {\mathbf{X}}'\tilde{\mathbf{X}} = \boldsymbol{\Sigma} -\mathrm{diag}\left\{\mathbf{s}\right\}
    \end{align*}
    where $\mathbf{s}\in\mathbb{R}^p_{+}$ is a p-dimensional non-negative vector (larger $s_j$ indicates higher power) and
    \begin{itemize}
        \item $\tilde{\mathbf{X}}$ exhibits the \myhl[myblue]{\textbf{same}} covariance structrue as the original design $\mathbf{X}$
        \item The correlation between distinct original variables and knockoffs are the same as between the originals: $$ \mathbf{X}_j'\tilde{\mathbf{X}}_k = \mathbf{X}_j'{\mathbf{X}}_k,\ \forall j\neq k $$
        \item The correlation between the original variables and their own knockoffs is \myhl[myblue]{\textbf{less than 1}} $$ \mathbf{X}_j'\tilde{\mathbf{X}}_j = \Sigma_{jj}-s_j = 1-s_j $$
    \end{itemize}
    To construct such knockoffs, 
    \begin{itemize}
        \item Given a proper $\mathbf{s}$, if $n \geq  2p$, then  $$ \tilde{\mathbf{X}} = \mathbf{X}(\mathbf{I}-\boldsymbol{\Sigma}^{-1}\mathrm{diag}\left\{\mathbf{s}\right\}) + \tilde{\mathbf{U}}\mathbf{C} $$ where $\tilde{\mathbf{U}}\in \mathbb{R}^{n\times p}$ is an \myhl[myblue]{\textbf{orthonormal}} matrix s.t. $\tilde{\mathbf{U}}'\mathbf{X}=\mathbf{0}$ and $\mathbf{C'C}=2\mathrm{diag}\left\{\mathbf{s}\right\} - \mathrm{diag}\left\{\mathbf{s}\right\}\boldsymbol{\Sigma}^{-1}\mathrm{diag}\left\{\mathbf{s}\right\} \geq \mathbf{0}$
        \item A sufficient and necessary condition for $\tilde{\mathbf{X}}$ to exist: $\mathrm{diag}\left\{\mathbf{s}\right\} \leq 2\boldsymbol{\Sigma}$
    \end{itemize}
    2 types of knockoffs can be constructed, following these procedures
    \begin{itemize}
        \item[T1] \underline{\textbf{Equi-correlated}} knockoffs: set $s_j=2\lambda_{\min}(\boldsymbol{\Sigma}) \wedge 1$ for all $j$, then $\langle \mathbf{X}_j,\tilde{\mathbf{X}}_j \rangle = 1-2\lambda_{\min}(\boldsymbol{\Sigma}) \wedge 1$ for all $j$. This is essentially minimizing $\left\vert \langle \mathbf{X}_j,\tilde{\mathbf{X}}_j \rangle \right\vert$ 
        \item[T2] \underline{\textbf{SDP}} knockoffs: solve the convex problem \begin{align*}
            \arg\min_{\mathbf{x}} \sum_j (1-s_j) & & s.t. 0 \leq s_j \leq 1, \mathrm{diag}\left\{ \mathbf{s}\right\}\leq 2\boldsymbol{\Sigma}
        \end{align*}
        which is essentially minimizing the average of $ \langle \mathbf{X}_j,\tilde{\mathbf{X}}_j \rangle $ 
    \end{itemize}
\end{itemize}

\paragraph{Calculate test statistics} Define and calculate test statistics $W_j$ for each $\beta_j\in\left\{ 1,\cdots,p \right\}$ using $\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix}$:
\begin{itemize}
    \item the test statistic $W_j$ should be constructed s.t. large positive values are evidence against the null hypothesis $\beta_j =0$, for example, consider a Lasso on $\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix}$ $$ \hat{\beta}(\lambda) = \arg\min_{\mathbf{b}}\left\{ \frac{1}{2}\left\Vert \mathbf{y}-\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix} \mathbf{b} \right\Vert^2_2 + \lambda\left\Vert \mathbf{b} \right\Vert \right\} _1 $$ where $\lambda$ is the point on the Lasso path at which the feature enters the model as $$ Z_j = \sup \left\{ \lambda: \hat{\beta}_j(\lambda)\neq 0 \right\} $$ and set $W_j = (Z_j \vee \tilde{Z}_j)\cdot \begin{cases} +1, &Z_j>\tilde{Z}_j \\ -1, & Z_j<\tilde{Z}_j \end{cases}$\footnote{Other choices of $W_j$ are $W_j =\left\vert\mathbf{X}'_j\mathbf{y} \right\vert - \left\vert\tilde{\mathbf{X}}'_j\mathbf{y} \right\vert $, or $\left\vert \hat{\beta}^{\mathrm{LS}}_j \right\vert-\left\vert \hat{\beta}^{\mathrm{LS}}_{j+p} \right\vert$} 
    \item In general, the statistics $W$ should satisfy the \myhl[myblue]{\textbf{sufficient}} property and \myhl[myblue]{\textbf{anti-symmetry}} property:
    \begin{definition}{Property of Test Statistics $W_j$}{statistic_property}
        The test statistic $W_j$ is said to obey
        \begin{itemize}
            \item the \myhl[myred]{\textbf{sufficient}} property if $\mathbf{W}$ depends \underline{\textit{only}} on the Gram matrix and on feature-response inner products, that is $$ \mathbf{W} = f\left( \begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix}'\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix},\begin{bmatrix} \mathbf{X} & \tilde{\mathbf{X}} \end{bmatrix}'\mathbf{y} \right) $$
            \item the \myhl[myred]{\textbf{antisymmetry}} property if swapping the original $\mathbf{X}_j$ and its knockoff $\tilde{\mathbf{X}}_j$ has the effect of \textbf{switching the sign} of $W_j$, that is $$ W_j(Z_j,\tilde{Z}_j)= -W_j (\tilde{Z}_j,Z_j) $$
        \end{itemize}
    \end{definition}
\end{itemize}

\paragraph*{Calculate a threshold for the test statistics} After defining the test statistic, we then
\begin{itemize}
    \item Let $q$ be the target FDR, define the data-dependent threshold $T$ as $$ T =\min\left\{ t\in\mathcal{W}: \frac{\# \left\{j:W_j\leq -t \right\}}{\# \left\{j:W_j\geq t \right\} \vee 1} \leq  q \right\} $$ where $\mathcal{W}=\left\{ \left\vert W_j \right\vert: j=1,\cdots,p \right\} \setminus \left\{ 0 \right\}$ is the set of unique non-zero values attained by $\lvert W_j \rvert$'s.
    \begin{figure}[ht]
        \centering
        \includegraphics[width = 0.6 \textwidth]{figures/note17_visualizingFDR.png}
        \caption{Visualizing Test Statistic Thresholding}\label{fig:fdr_test_threshold_viz}
    \end{figure}
\end{itemize}

\paragraph*{Variable selection} after building the threshold, 
\begin{itemize}
    \item for each $j=1,\cdots,p$, reject $H_{0,j}:\beta_j=0$ if $W_j\geq T$, the knockoff filter selects the model $$ \hat{S} = \left\{ j:W_j\geq T \right\} $$
\end{itemize}

\subsection{Intuition and Theory}
\paragraph*{Why knockoffs work?}
\begin{itemize}
    \item $\mathbf{W}$ is constructed (\myhl[myblue]{\textbf{antisymmetry}} and \myhl[myblue]{\textbf{sufficiency}}) such that the signs of the $W_j$'s are i.i.d. random for the null 
    \item for any threshold $t$, we have $$ \# \left\{ j:\beta_j=0,W_j\geq t \right\} \overset{d}{=} \# \left\{ j:\beta_j=0,W_j\leq -t \right\} $$, and the false discovery proportion (FDP) can be estimated as
    \begin{align*}
        \frac{\# \left\{ j:\beta_j=0,W_j\geq t \right\}}{\max\left( \# \left\{ j:W_j\geq t \right\},1 \right)} &\simeq \frac{\# \left\{ j:\beta_j=0,W_j\leq -t \right\}}{\max\left( \# \left\{ j:W_j\geq t \right\},1 \right)}\\
        &\leq  \frac{\# \left\{ j:W_j\leq -t \right\}}{\max\left( \# \left\{ j:W_j\geq t \right\},1 \right)} \coloneq \widehat{\mathrm{FDP}}(t)
    \end{align*}
    then the knockoff procedure can be interpreted as finding a threshold via $T=\min\left\{t\in \mathcal{W}:\widehat{\mathrm{FDR}}(t)\leq q \right\}$
\end{itemize}
The knockoff procedure essentially controls a quantity \myhl[myblue]{\textbf{nearly equal}} to the FDR. To control the FDR \myhl[myred]{\textbf{exactly}}, we have, \underline{textbf{knockoff+}} , a more conservative modification of the knockoff procedure, where the threshold is 
$$
T = \min\left\{ t\in\mathcal{W}: \frac{\textcolor{myred}{1+}\# \left\{ j:W_j\leq -t \right\}}{\max\left( \# \left\{ j:W_j\geq t \right\},1 \right)} \leq q \right\}
$$
the $\textcolor{myred}{+1}$ part makes it harder to reject the null:
\begin{align*}
    \mathrm{FDP} &= \frac{\# \left\{ j: \beta_j=0, W_j\geq -T \right\}}{\# \left\{ j: W_j\geq T \right\} \vee 1}\cdot \frac{1+ \# \left\{ j: \beta_j=0, W_j\leq -T \right\}}{1+\# \left\{ j: \beta_j=0, W_j\leq -T \right\}}\\
    & \leq \frac{1+ \# \left\{ j: W_j\leq -T \right\}}{\# \left\{ j: W_j\geq T \right\}\vee 1} \cdot \frac{\# \left\{ j: \beta_j=0, W_j\geq T \right\}}{1+\# \left\{ j: \beta_j=0, W_j\leq -T \right\}}\\
    & \leq  q \cdot 1
\end{align*}

Then, we have the following theorem
\begin{theorem}{Property of the Knockoff Method}{knockoff_property}
    For any $q\in[0,1]$, the \myhl[myblue]{\textbf{knockoff}} method satisfies
    $$
    \mathbb{E}\left[ \frac{\# \left\{ j: \beta_j=0, j\in \hat{S}\right\}}{\# \left\{ j: j\in\hat{S} \right\} + q^{-1}} \right]\leq q
    $$
    and the \myhl[myred]{\textbf{knockoff+}} method satisfies
    $$
    \mathbb{E}\left[ \frac{\# \left\{ j: \beta_j=0, j\in \hat{S}\right\}}{\# \left\{ j: j\in\hat{S} \right\}} \right]\leq q
    $$
    in both cases, teh expectation is taken over the Gaussian noise in the model, while treating original variables $\mathbf{X}$ and knockoffs $\tilde{\mathbf{X}}$ as fixed
\end{theorem}

\section{Candes et al. (2018)}
Another way of constructing knockoffs, introduced by \citet{candes2018panning}, is by a swapping method:  for the family of random variables $\mathbf{X}=(\mathbf{X}_1,\cdots,\mathbf{X}_p)$ are a new family of random variables $\tilde{\mathbf{X}} = (\tilde{\mathbf{X}}_1,\cdots,\tilde{\mathbf{X}}_p)$ constructed with the following 2 properties
\begin{itemize}
    \item[(a)] for any subset $S \subset \left\{1,\cdots,p \right\}$, $$ (\mathbf{X},\tilde{\mathbf{X}})_{\mathrm{swap}(S)} \overset{\mathrm{d}}{=}(\mathbf{X},\tilde{\mathbf{X}}) $$
    \item[(b)] $\tilde{\mathbf{X}} \ind \mathbf{Y}\mid \mathbf{X}$ if there is a response $\mathbf{Y}$
\end{itemize}
Suppose $\mathbf{X}\sim \mathcal{N}(0,\boldsymbol{\Sigma})$, then $(\mathbf{X},\tilde{\mathbf{X}})_{\mathrm{swap}(S)}$ satisfies $ (\mathbf{X},\tilde{\mathbf{X}})_{\mathrm{swap}(S)} \overset{\mathrm{d}}{=}(\mathbf{X},\tilde{\mathbf{X}}) $ if 
\begin{align*}
    (\mathbf{X},\tilde{\mathbf{X}})_{\mathrm{swap}(S)} \overset{\mathrm{d}}{=}(\mathbf{X},\tilde{\mathbf{X}}) &\sim \mathcal{N}(0,\mathbf{G}), & \text{where } \mathbf{G}&=\begin{pmatrix}
        \boldsymbol{\Sigma} & \boldsymbol{\Sigma}-\mathrm{diag}(s)\\
        \boldsymbol{\Sigma}-\mathrm{diag}(s) & \boldsymbol{\Sigma}
    \end{pmatrix}
\end{align*}
where $\mathrm{diag}(s)$ is any \textbf{diagonal matrix} s.t. $\boldsymbol{G}$ is \myhl[myblue]{\textbf{positive semidefinite}}. For $\mathbf{P}$, the permutation matrix encoding the swap,
$$
\mathbf{PGP} = \mathbf{G}
$$


\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}