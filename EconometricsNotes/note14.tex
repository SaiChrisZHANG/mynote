\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{14}{Regularization Methods in Thresholded Parameter Space}{}{Sai Zhang}{The connections and differences of all regularization methods and some interesting phase transition phenomena.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Model Setup}
Now, consider a generalized linear model (GLM) linking a $p$-dimensional predictor $\mathbf{x}$ to a scalar response $Y$. With canonical link, the conditional distribution of $Y$ given $\mathbf{x}$ has density
$$
f(y;\theta,\phi) = \exp\left[ y\theta-b(\theta)+c(y,\phi) \right]
$$
where $\theta = \mathbf{x}'\boldsymbol{\beta}$ with $\boldsymbol{\beta}$ a $p-$dimensional regression coefficient vector, $b(\dot)$ and $c(\cdot,\cdot)$ are know functions and $\phi$ is dispersion parameter. Again, $\boldsymbol{\beta}=\left( \beta_{0,1},\cdots,\beta_{0,p} \right)'$ is sparse with many zero components, and $\log p = O(n^a)$ for some $0<a<1$.

The penalized negative log-likelihood is
$$
Q_n(\boldsymbol{\beta}) = -n^{-1}\left[ \mathbf{y}'\mathbf{X}\boldsymbol{\beta} - \mathbf{1}'\mathbf{b}(\mathbf{X}\boldsymbol{\beta}) \right] + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1
$$
where
\begin{itemize}
    \item $\mathbf{y}=\left( y_1,\cdots,y_n \right)'$, $\mathbf{X}=\left(\mathbf{x}_1,\cdots,\mathbf{x}_n\right)'$, each column of $\mathbf{X}$ is rescaled to have $L_2$-norm $\sqrt{n}$
    \item $\mathbf{b}(\boldsymbol{\theta}) = \left( b(\theta_1),\cdots,b(\theta_n) \right)'$ with $\boldsymbol{\theta} = \left(\theta_1,\cdots,\theta_n\right)'$
    \item $\lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1 = \sum^p_{j=1}p_{\lambda}(\lvert \beta_j \rvert)$
\end{itemize}

Next, define \textbf{robust spark} $\kappa_c$
\begin{definition}{Robust spark $\kappa_c$}{robust_spark}
    The robust spark $\kappa_c$ of the $n\times p$ design matrix $\mathbf{X}$ is defined as the smallest possible positive integer s.t. there exists an $n\times \kappa_c$ submatrix of $\frac{1}{\sqrt{n}}\mathbf{X}$ having a singular value less than a given positive constant $c$ \citep{zheng2014high}, and $$\kappa_c\leq n+1$$
\end{definition}
Bounding sparse model size can control collinearity and ensure model identifiability and stability, and as $c\rightarrow 0+$, $\kappa_c$ approaches the spark. Robust spark can be some large number diverging with $n$:
\begin{proposition}{Order of $\kappa_c$}{order_robust_spark}
    Assume $\log p=o(n)$ and that the rows of the $n\times p$ random design matrix $\mathbf{X}$ are i.i.d. as $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$, where $\boldsymbol{\Sigma}$ has smallest eigenvalue bounded from below by some positive constant. Then there exist positive constants $c$ and $\tilde{c}$ s.t. with asymptotic probability one, $\kappa_c \geq \frac{\tilde{c}n}{\log p} $
\end{proposition}

Next, we define a thresholded parameter space
\begin{definition}{Thresholded parameter space}{thresholded_param_space}
    $$
    \mathcal{B}_{\tau,c} = \left\{ \boldsymbol{\beta}\in\mathbb{R}^p: \lVert \boldsymbol{\beta} \rVert _0 <\frac{\kappa_c}{2}\text{, and for each $j$, $\beta_j=0$ or $\lvert \beta_j \rvert \geq \tau$} \right\}
    $$
    where $\boldsymbol{\beta} = \left( \beta_1,\cdots,\beta_p \right)'$. $\tau$ is some positive threshold on parameter magnitude:
\end{definition}
Here, $\tau$ is very important:
\begin{itemize}
    \item $\tau$ is key to distinguishing between important covariates and noise covariates for the purpose of variable selection 
    \item $\tau$ typically needs to satisfy $\tau\sqrt{n/\log p}\xrightarrow{n\rightarrow\infty}\infty$
\end{itemize}

It turns out that the solution to the regularizaiton problem has the (very natural) hard-thresholding property:
\begin{proposition}{Hard-thresholding property}
    For the $L_0$-penalty $p_{\lambda}(t) = \lambda\mathbf{1}_{t\neq 0}$, the global minimizer $\hat{\boldsymbol{\beta}} = \left(\hat{\beta}_1,\cdots,\hat{\beta}_p\right)'$ of the regularization problem over $\mathbb{R}^p$ satisfies that each component $\hat{\beta}_j$ is either 0 or has magnitude larger than some positive threshold
\end{proposition}
This hard-thresholding property is shared by many other penalties such as SICA penalties. This property guarantees sparcity of the model: weak signals are generally difficult to stand out comparing to noise variables due to impact of high dimensionality

\section{Asymptotic Equivalence of Regularization Methods}
For a universal $\lambda=c_0\sqrt{\log p /n}$ with $c_0>0$ and $p$ implicitly as $n \vee p$, consider 2 key events:
\begin{align*}
    \mathcal{E}&=\left\{ \lVert n^{-1} \mathbf{X}'\boldsymbol{\epsilon} \rVert _{\infty} \leq \lambda/2 \right\} & \mathcal{E}_0 &= \left\{ \lVert n^{-1}\mathbf{X}'_{\alpha_0}\boldsymbol{\epsilon} \rVert _{\infty} \leq c_0 \sqrt{\log n/n}  \right\} 
\end{align*}
where $\boldsymbol{\epsilon}=\mathbf{y}-\mathbb{E}\mathbf{y}$, $\mathbf{X}_{\alpha}$ is a submatrix of $\mathbf{X}$ consisting of columns in $\alpha$. Here, let $\alpha_0= \mathrm{supp}\left(\boldsymbol{\beta}_0 \right)$ (non-zero variables in the true model).

For this setting, consider the following technical conditions:
\begin{itemize}
    \item[C1] \myhl[myblue]{\textbf{Error tail distribution}}: $\Pr (\mathcal{E}^c) = O(p^{-c_1})$ and $\Pr (\mathcal{E}^c_0) = O(n^{-c_1})$ for some positive constant $c_1$ that can be sufficiently large for large enough $c_0$
    \item[C2] \myhl[myblue]{\textbf{Bounded variance}}: $b(\theta)$ satisfies that $c_2\leq b''(\theta)\leq c_2^{-1}$ in its domain, where $c_2$ is some positive constant
    \item[C3] \myhl[myblue]{\textbf{Concave penalty function}}: $p_{\lambda}(t)$ is increasing and concave in $t\in [0,\infty)$ with $p_{\lambda}(0)=0$, and is differentiable with $p'_{\lambda}(0+) = c_3\lambda$ for some positive constant $c_3$\footnote{A wide class of penalties, including $L_1$-penalty in Lasso, SCAD, MCP and SICA, satisfy this condition.}
    \item[C4] \myhl[myblue]{\textbf{Ultra-high dimensionality}}: $\log p=O(n^a)$ for some constant $a\in (0,1)$
    \item[C5] \myhl[myblue]{\textbf{True parameter vector}}: $s=o(n^{1-a})$ and $\exists c>0$ s.t. the \textbf{robust spark} $\kappa_c > 2s$. Moreover, $\min_{1\leq j\leq s}\lvert \beta_{0,j}\rvert \gg \sqrt{\log p/n}$
\end{itemize}

Given these 5 conditions, we have that the global minimizer $\hat{\boldsymbol{\beta}} = \arg \min_{\boldsymbol{\beta}\in \mathcal{B}_{\tau}}Q_n(\boldsymbol{\beta})$ exists and satisfies oracle inequalities:
\begin{theorem}{Oracle Inequalities}{oracle_inequal}
    Assume that Condition 1-5 hold and $\tau$ is chosen s.t. $\tau < \min_{1\leq j \leq s}\lvert \beta_{0,j} \rvert$ and $\lambda = c_0\sqrt{\log p/n} = o(\tau)$, then the global minimizer exists, and any such global minimizer satisfies that with probability at least $1-O(p^{-c_1})$, it holds simultaneously that 
    \begin{itemize}
        \item \myhl[myblue]{\textbf{False sign}}: $$FS(\hat{\boldsymbol{\beta}}) \leq \frac{Cs \lambda^2 \tau^{-2}}{1-C\lambda^2\tau^{-2}}$$
        \item \myhl[myblue]{\textbf{Estimation losses}}: 
        \begin{align*}
            \lVert \hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0\rVert _q &\leq C\lambda s^{1/q} (1-C\lambda^2\tau^{-2})^{-1/q} &\forall q\in [1,2]\\
            \lVert \hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0\rVert _{\infty} &\leq C\lambda s^{1/2} (1-C\lambda^2\tau^{-2})^{-1/2}
        \end{align*}
        \item \myhl[myblue]{\textbf{Prediction loss}}: $$\frac{1}{\sqrt{n}} \lVert \mathbf{X}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0\right)\rVert _2 \leq C\lambda s^{1/2} (1-C\lambda^2\tau^{-2})^{-1/2}  $$
    \end{itemize}
    where $C$ is some positive constant.
\end{theorem}
\paragraph*{How to understand Thm.\ref{thm:oracle_inequal}} 
\begin{itemize}
    \item These results hold uniformly over the set of all possible global minimizers
    \item $c_1$ in probability bound can be chosen arbitrarily large, affecting \textbf{only} $C$
    \item $FS\left(\hat{\boldsymbol{\beta}}\right) = o(s)$ since $\lambda = o(\tau)$, while $\lVert\hat{\boldsymbol{\beta}}\rVert _0 = O(\phi_{\max}s)$ where $\phi_{\max}$ is the largest eigenvalue of $\frac{1}{n}\mathbf{X}'\mathbf{X}$
    \item $\forall q \in[1,2]$, the convergence rates of estimation losses
    \begin{align*}
        \lVert \hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0\rVert _q &= O\left\{ s^{1/q} \sqrt{\frac{\log p}{n}} \right\} \\
        \frac{1}{\sqrt{n}} \lVert \mathbf{X}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0\right)\rVert _2 &= O\left( \sqrt{\frac{s \log p}{n}} \right)
    \end{align*}
    are consistent with Lasso.
\end{itemize}
We also have a sign consistency result:
\begin{theorem}{Sign Consistency and Oracle Inequalities}{sign_consistency}
    Assume the same conditions of Thm.\ref{thm:oracle_inequal}, further assume $\min_{1\leq j \leq s}\lvert \beta_{0,j} \rvert \geq 2\tau $ and $\lambda = c_0\sqrt{\log p/n} = o(s^{-1/2}\tau)$, and $\gamma_n = o\left(\tau \sqrt{\frac{n}{s\log n}}\right)$, then any global minimizer $\hat{\boldsymbol{\beta}}$ defined satisfies that with probability at least $1-O(n^{-c_1})$, it holds simultaneously that 
    \begin{itemize}
        \item \myhl[myblue]{\textbf{Sign consistency}}: $\mathrm{sgn}\left( \hat{\boldsymbol{\beta}} \right)=\mathrm{sgn}\left( \boldsymbol{\beta}_0 \right)$
        \item \myhl[myblue]{\textbf{Estimation and prediction losses}}: If the penalty function further satisfies $p'_{\lambda}(\tau) = O\left(\frac{\log n}{n}\right)$, then $\forall q\in [1,2]$,
        \begin{align*}
            \left\Vert \hat{\boldsymbol{\beta}} -\boldsymbol{\beta}_0\right\Vert _q &\leq Cs^{1/q}\sqrt{\frac{\log n}{n}} & \left\Vert \hat{\boldsymbol{\beta}} -\boldsymbol{\beta}_0\right\Vert _{\infty} &\leq C\gamma_n^*\sqrt{\frac{\log n}{n}} & n^{-1}D\left( \hat{\boldsymbol{\beta}} \right) \leq C\frac{s\log n}{n} 
        \end{align*}
        where $\gamma^*_n$ is a constant showing the behavior of $\lVert \left[\frac{1}{n} \mathbf{X}'_{\alpha_0}\mathbf{H}\left(\boldsymbol{\beta}_1,\cdots,\boldsymbol{\beta}_n\right)\mathbf{X}_{\alpha_0} \right]^{-1} \rVert _{\infty}$ in a small neighborhood of $\boldsymbol{\beta}_0$, $D(\hat{\boldsymbol{\beta}})$ is the Kullback-Leibler divergence, and $C$ is some positive constant
    \end{itemize}
\end{theorem}

\paragraph*{How to understand Thm.\ref{thm:sign_consistency}} Consider a linear model, where
\begin{align*}
    \gamma_n^* &= \left\Vert \left( \frac{1}{n}\mathbf{X}'_{\alpha_0}\mathbf{X}_{\alpha_0} \right)^{-1} \right\Vert _{\infty} \leq \sqrt{s} \left\Vert \left( \frac{1}{n}\mathbf{X}'_{\alpha_0}\mathbf{X}_{\alpha_0} \right)^{-1} \right\Vert _2 \leq \frac{\sqrt{s}}{c} & \gamma_n & = \sup_{\alpha \subset \left\{ s+1,\cdots,p \right\},\lvert\alpha\rvert\leq s } \left\Vert \frac{1}{n}\mathbf{X}'_{\alpha_0}\mathbf{X}_{\alpha} \right\Vert _{\infty}
\end{align*} 
when all ture covariates are orthogonal to each other, $\gamma_n^*=1$ and  
$$
\lVert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0\rVert _{\infty}\leq C \sqrt{\frac{\log n}{n}}
$$
within a logarithmic factor $\log n$ or oracle rate. Meanwhile, the penalty function condition $p'_{\lambda}(\tau) = O\left(\frac{\log n}{n}\right)$ can be easily satisfied by concave penalties such as SCAD and SICA, having convergence rates improved with $\log n$ in place of $\log p$.

\paragraph{Phase transition phenomenon}
Combining Thm.\ref{thm:oracle_inequal} and \ref{thm:sign_consistency}, it's shown that
\begin{itemize}
    \item for \myhl[myred]{\underline{$ p=O(n^a)$}}, Lasso and concave regularization methods are \myhl[myred]{\textbf{asymptotically equivalent}}, having the same convergence rates in the oracle inequalities, with a logarithmic factor of $\log n$
    \item for \myhl[myred]{\underline{$\log p=O(n^a)$}}, concave regularization methods are \myhl[myred]{\textbf{asymptotically equivalent}} and still enjoy the same convergence rates in the oracle inequalities, with a logarithmic factor of $\log n$\footnote{For Lasso, the condition $p'_{\lambda}(\tau) = O\left(\frac{\log n}{n}\right)$ and the choice of $\lambda = c_0\sqrt{\frac{\log p}{n}}$ are \myhl[myred]{\textbf{incompatible}} with each other in this ultra-high dimensional case, and the convergence rates for Lasso \myhl[myred]{(of $\log p$)} are slower than those for concave regularization methods.}.
\end{itemize}
A phase diagram on how the performance of regularization methods, in the thresholded parameter space, evolves with dimensionality and penalty function.

Further, we have the following \textbf{\underline{oracle risk inequalities}} of the global minimizer
\begin{theorem}{Oracle Risk Inequalities}{oracle_risk_inequality}
    Assume that conditions of Thm.\ref{thm:sign_consistency} hold and \textbf{the fourth moments of errors $\mathbb{E}\epsilon_i^4$ are \myhl[myblue]{uniformly} \myhl[myblue]{bounded}} . Then any global minimizer $\hat{\boldsymbol{\beta}}$ defined satisfies that 
    \begin{itemize}
        \item \myhl[myblue]{\textbf{Sign risk}} $$ \mathbb{E}\left[\mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right)\right] = \frac{1}{p_{\lambda}(\tau)}\left[ \left(\left\Vert p_{\lambda}\left(\boldsymbol{\beta}_0\right) \right\Vert _1 + s\lambda^2 \right) O(n^{-c_1})+ O(p^{-c_1/2})\kappa_c \right] $$
        \item \myhl[myblue]{\textbf{Estimation and prediction risks}}: If the pnnalty function further satisfies $p'_{\lambda}(\tau) = O\left(\sqrt{\frac{\log n}{n}}\right)$, then $\forall q\in[1,2]$
        \begin{align*}
            \mathbb{E}\left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert^q_q &\leq Cs\left(\frac{log n}{n}\right)^{q/2} & \mathbb{E}\left\Vert \hat{\boldsymbol{\beta}}-\boldsymbol{\beta}_0 \right\Vert _{\infty} &\leq C\gamma_n^*\sqrt{\frac{\log n}{n}} & \mathbb{E}\left[\frac{1}{n}D\left(\hat{\boldsymbol{\beta}}\right)\right] &\leq Cs\frac{\log n}{n}
        \end{align*}
        where $C$ is some positive constant.
    \end{itemize}
\end{theorem}
\paragraph*{How to understand Thm.\ref{thm:oracle_risk_inequality}}
\begin{itemize}
    \item $\mathbb{E}\left[\mathrm{FS}\left(\hat{\boldsymbol{\beta}}\right)\right]$ converges to 0 at a polynomial rate of $n$
    \item Consistent with the risk bounds $O\left(\frac{s\log n}{n}\right)$ of the regularized estimators under the $L_2$-loss in wavelets setting with orthogonal design 
    \item No additional cost in risk bounds for generalizing to the ultra-high dimensional nonlinear model setting of GLM
\end{itemize}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}