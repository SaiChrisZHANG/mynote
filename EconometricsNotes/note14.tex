\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{14}{Regularization Methods in Thresholded Parameter Space}{}{Sai Zhang}{The connections and differences of all regularization methods and some interesting phase transition phenomena.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Model Setup}
Now, consider a generalized linear model (GLM) linking a $p$-dimensional predictor $\mathbf{x}$ to a scalar response $Y$. With canonical link, the conditional distribution of $Y$ given $\mathbf{x}$ has density
$$
f(y;\theta,\phi) = \exp\left[ y\theta-b(\theta)+c(y,\phi) \right]
$$
where $\theta = \mathbf{x}'\boldsymbol{\beta}$ with $\boldsymbol{\beta}$ a $p-$dimensional regression coefficient vector, $b(\dot)$ and $c(\cdot,\cdot)$ are know functions and $\phi$ is dispersion parameter. Again, $\boldsymbol{\beta}=\left( \beta_{0,1},\cdots,\beta_{0,p} \right)'$ is sparse with many zero components, and $\log p = O(n^a)$ for some $0<a<1$.

%\newpage
%\bibliographystyle{plainnat}
%\bibliography{ref.bib}

\end{document}