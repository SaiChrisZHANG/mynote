\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{14}{Regularization Methods in Thresholded Parameter Space}{}{Sai Zhang}{The connections and differences of all regularization methods and some interesting phase transition phenomena.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Model Setup}
Now, consider a generalized linear model (GLM) linking a $p$-dimensional predictor $\mathbf{x}$ to a scalar response $Y$. With canonical link, the conditional distribution of $Y$ given $\mathbf{x}$ has density
$$
f(y;\theta,\phi) = \exp\left[ y\theta-b(\theta)+c(y,\phi) \right]
$$
where $\theta = \mathbf{x}'\boldsymbol{\beta}$ with $\boldsymbol{\beta}$ a $p-$dimensional regression coefficient vector, $b(\dot)$ and $c(\cdot,\cdot)$ are know functions and $\phi$ is dispersion parameter. Again, $\boldsymbol{\beta}=\left( \beta_{0,1},\cdots,\beta_{0,p} \right)'$ is sparse with many zero components, and $\log p = O(n^a)$ for some $0<a<1$.

The penalized negative log-likelihood is
$$
Q_n(\boldsymbol{\beta}) = -n^{-1}\left[ \mathbf{y}'\mathbf{X}\boldsymbol{\beta} - \mathbf{1}'\mathbf{b}(\mathbf{X}\boldsymbol{\beta}) \right] + \lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1
$$
where
\begin{itemize}
    \item $\mathbf{y}=\left( y_1,\cdots,y_n \right)'$, $\mathbf{X}=\left(\mathbf{x}_1,\cdots,\mathbf{x}_n\right)'$, each column of $\mathbf{X}$ is rescaled to have $L_2$-norm $\sqrt{n}$
    \item $\mathbf{b}(\boldsymbol{\theta}) = \left( b(\theta_1),\cdots,b(\theta_n) \right)'$ with $\boldsymbol{\theta} = \left(\theta_1,\cdots,\theta_n\right)'$
    \item $\lVert p_{\lambda}(\boldsymbol{\beta}) \rVert _1 = \sum^p_{j=1}p_{\lambda}(\lvert \beta_j \rvert)$
\end{itemize}

Next, define \textbf{robust spark} $\kappa_c$
\begin{definition}{Robust spark $\kappa_c$}{robust_spark}
    The robust spark $\kappa_c$ of the $n\times p$ design matrix $\mathbf{X}$ is defined as the smallest possible positive integer s.t. there exists an $n\times \kappa_c$ submatrix of $\frac{1}{\sqrt{n}}\mathbf{X}$ having a singular value less than a given positive constant $c$ \citep{zheng2014high}, and $$\kappa_c\leq n+1$$
\end{definition}
Bounding sparse model size can control collinearity and ensure model identifiability and stability, and as $c\rightarrow 0+$, $\kappa_c$ approaches the spark. Robust spark can be some large number diverging with $n$:
\begin{proposition}{Order of $\kappa_c$}{order_robust_spark}
    Assume $\log p=o(n)$ and that the rows of the $n\times p$ random design matrix $\mathbf{X}$ are i.i.d. as $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$, where $\boldsymbol{\Sigma}$ has smallest eigenvalue bounded from below by some positive constant. Then there exist positive constants $c$ and $\tilde{c}$ s.t. with asymptotic probability one, $\kappa_c \geq \frac{\tilde{c}n}{\log p} $
\end{proposition}

Next, we define a thresholded parameter space
\begin{definition}{Thresholded parameter space}{thresholded_param_space}
    $$
    \mathcal{B}_{\tau,c} = \left\{ \boldsymbol{\beta}\in\mathbb{R}^p: \lVert \boldsymbol{\beta} \rVert _0 <\frac{\kappa_c}{2}\text{, and for each $j$, $\beta_j=0$ or $\lvert \beta_j \rvert \geq \tau$} \right\}
    $$
    where $\boldsymbol{\beta} = \left( \beta_1,\cdots,\beta_p \right)'$. $\tau$ is some positive threshold on parameter magnitude:
\end{definition}
Here, $\tau$ is very important:
\begin{itemize}
    \item $\tau$ is key to distinguishing between important covariates and noise covariates for the purpose of variable selection 
    \item $\tau$ typically needs to satisfy $\tau\sqrt{n/\log p}\xrightarrow{n\rightarrow\infty}\infty$
\end{itemize}

It turns out that the solution to the regularizaiton problem has the (very natural) hard-thresholding property:
\begin{proposition}{Hard-thresholding property}
    For the $L_0$-penalty $p_{\lambda}(t) = \lambda\mathbf{1}_{t\neq 0}$, the global minimizer $\hat{\boldsymbol{\beta}} = \left(\hat{\beta}_1,\cdots,\hat{\beta}_p\right)'$ of the regularization problem over $\mathbb{R}^p$ satisfies that each component $\hat{\beta}_j$ is either 0 or has magnitude larger than some positive threshold
\end{proposition}
This hard-thresholding property is shared by many other penalties such as SICA penalties. This property guarantees sparcity of the model: weak signals are generally difficult to stand out comparing to noise variables due to impact of high dimensionality

\section{Asymptotic Equivalence of Regularization Methods}
For a universal $\lambda=c_0\sqrt{\log p /n}$ with $c_0>0$ and $p$ implicitly as $n \vee p$, consider 2 key events:
\begin{align*}
    \mathcal{E}&=\left\{ \lVert n^{-1} \mathbf{X}'\boldsymbol{\epsilon} \rVert _{\infty} \leq \lambda/2 \right\} & \mathcal{E}_0 &= \left\{ \lVert n^{-1}\mathbf{X}'_{\alpha_0}\boldsymbol{\epsilon} \rVert _{\infty} \leq c_0 \sqrt{\log n/n}  \right\} 
\end{align*}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}