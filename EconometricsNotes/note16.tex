\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{16}{Graphical Network Inference}{}{Sai Zhang}{}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Consider a classic question: For $n$ observations of dimension $p$, how can we capture the statistical relationships between the variables of interest? Consider the example of the multivariate Gaussian distribution:
\begin{example}{Multivariate Gaussian Distribution}{}
    Suppose we have $n$ observations of dimension $p$, $\mathbf{x}\sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$. let $\mathbf{S}$ be the empirical covariance matrix. Then the probability density 
    $$
    f(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}\det (\boldsymbol{\Sigma})^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \right\}
    $$
    define the \textbf{inverse covariance matrix} or \textbf{precision matrix} as $ \boldsymbol{\Omega}=\boldsymbol{\Sigma}^{-1} $, then we have 
    $$
    f_{\mathbf{\mu},\boldsymbol{\Omega}} = \exp \left\{ \boldsymbol{\mu'\Omega x} - \left< \boldsymbol{\Omega},\frac{1}{2}\mathbf{xx}' \right> -\frac{p}{2}\log(2\pi) + \frac{1}{2} \log\det(\boldsymbol{\Omega}) - \frac{1}{2}\boldsymbol{\mu'\Omega\mu} \right\}
    $$
    where $\left< \mathbf{A},\mathbf{B} \right> = \mathrm{tr}(\mathbf{AB})$.
\end{example}
In this example, we know that \textbf{every} multivariate Gaussian distribution can be represented by a pairwise \myhl[myblue]{\textbf{Gaussian Markov Random Field (GMRF)}}, which an \textbf{\underline{undirected graph}} $G=(V,E)$
\begin{itemize}
    \item representing the collection of variables $\mathbf{x}$ by a vertex set $\mathcal{V}=\left\{1,\cdots,p\right\}$
    \item encoding correlations between variables by a set of edges $\mathcal{E}=\left\{ (i,j)\in \mathcal{V}\mid i=\neq j,\Omega_{ij}\neq 0 \right\}$  
\end{itemize}

For simplicity, we normalize $\boldsymbol{\mu} = \mathbf{0}$. If we draw $n$ i.i.d. samples $\mathbf{x}_1,\cdots,\mathbf{x}_n \sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$, then the log-likelihood is
\begin{align*}
    \mathcal{L}(\boldsymbol{\Omega}) &= \frac{1}{n}\sum^n_{i=1}\log f(\mathbf{x}_i) = \frac{1}{2}\log \det (\boldsymbol{\Omega}) - \frac{1}{2n}\sum^n_{i=1}\mathbf{x}'_1\boldsymbol{\Theta}\mathbf{x}_i \\
    &= \frac{1}{2}\log\det (\boldsymbol{\Omega}) - \frac{1}{2}\left< \Omega,\frac{1}{n}\sum^n_{i=1}\mathbf{x}'_i\mathbf{x}'_i \right>
\end{align*}

\paragraph*{What's the goal?} We want to estimate a \myhl[myblue]{\textbf{sparse}} graph structure given $n\ll p$ i.i.d. observations. But what does sparsity means in this context? A sparse graph is \textbf{\underline{equivalent}} to a sparse precision matrix: the precision matrix should have many 0s.

\paragraph*{Sparse precision matrix} for the Gaussian vector mentioned above $\mathbf{x}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$, we have $\forall u,v$ $$ x_u \perp x_v \mid \mathbf{x}_{\mathcal{V}\setminus \left\{ u,v \right\} } \Leftrightarrow \Omega_{u,v}=0 $$
that is, sparsity of the precision matrix is equivalent to \myhl[myblue]{\textbf{conditional independence}}\footnote{Meanwhile, for independence: $\Sigma_{u,v}=0\Leftrightarrow x_u \perp x_v$}. Consider \hyperref[fig:sparse_precision_graph]{a graph}, where $x_1$ and $x_4$ are only connected through other nodes, that is $x_1$ and $x_4$ are conditional independent, then we can have the precision matrix be something like:
$$
\boldsymbol{\Theta} = \begin{bmatrix}
    * & * & 0 & 0 & * & 0 & 0 & 0 \\
    * & * & 0 & 0 & 0 & * & * & 0 \\
    0 & 0 & * & 0 & * & 0 & 0 & * \\
    0 & 0 & 0 & * & 0 & 0 & * & 0 \\
    * & 0 & * & 0 & * & 0 & 0 & * \\
    0 & * & 0 & 0 & 0 & * & 0 & 0 \\
    0 & * & 0 & * & 0 & 0 & * & 0 \\
    0 & 0 & * & 0 & * & 0 & 0 & *
\end{bmatrix}
$$
where 0 captures precisely the conditional independence.

\begin{figure}[ht]\label{fig:sparse_precision_graph}
    \begin{minipage}[b]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[scale=1]
        % Step
        \draw[color=myblue] (-2,4) node[draw,circle] (x1) {$x_1$};
        \draw[color=myblue] (2,4) node[draw,circle] (x5) {$x_5$};
        \draw[color=myblue] (0,3) node[draw,circle] (x4) {$x_4$};
        \draw[color=myblue] (-3,2) node[draw,circle] (x2) {$x_2$};
        \draw[color=myblue] (0.3,1.5) node[draw,circle] (x6) {$x_6$};
        \draw[color=myblue] (3,2) node[draw,circle] (x8) {$x_8$};
        \draw[color=myblue] (-2,0) node[draw,circle] (x3) {$x_3$};
        \draw[color=myblue] (2.5,0) node[draw,circle] (x7) {$x_7$};
        % egdes
        \draw[color=myred, very thick] (x1) -- (x5);
        \draw[color=myred, very thick] (x5) -- (x8);
        \draw[color=myred, very thick] (x8) -- (x3);
        \draw[color=myred, very thick] (x3) -- (x5);
        \draw[color=myred, very thick] (x1) -- (x2);
        \draw[color=myred, very thick] (x2) -- (x6);
        \draw[color=myred, very thick] (x2) -- (x7);
        \draw[color=myred, very thick] (x4) -- (x7);
    \end{tikzpicture}
    
    \textbf{$x_1$ and $x_4$ are \textit{connected}}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[scale=1]
            % Step
            \draw[color=myblue] (-2,4) node[draw,circle] (x1) {$x_1$};
            \draw[color=myblue!35!white] (2,4) node[draw,circle] (x5) {$x_5$};
            \draw[color=myblue] (0,3) node[draw,circle] (x4) {$x_4$};
            \draw[color=myblue!35!white] (-3,2) node[draw,circle] (x2) {$x_2$};
            \draw[color=myblue!35!white] (0.3,1.5) node[draw,circle] (x6) {$x_6$};
            \draw[color=myblue!35!white] (3,2) node[draw,circle] (x8) {$x_8$};
            \draw[color=myblue!35!white] (-2,0) node[draw,circle] (x3) {$x_3$};
            \draw[color=myblue!35!white] (2.5,0) node[draw,circle] (x7) {$x_7$};
            % egdes
            \draw[color=myred!35!white, very thick, densely dashed] (x1) -- (x5);
            \draw[color=myred!35!white, very thick, densely dashed] (x5) -- (x8);
            \draw[color=myred!35!white, very thick, densely dashed] (x8) -- (x3);
            \draw[color=myred!35!white, very thick, densely dashed] (x3) -- (x5);
            \draw[color=myred!35!white, very thick, densely dashed] (x1) -- (x2);
            \draw[color=myred!35!white, very thick, densely dashed] (x2) -- (x6);
            \draw[color=myred!35!white, very thick, densely dashed] (x2) -- (x7);
            \draw[color=myred!35!white, very thick, densely dashed] (x4) -- (x7);
        \end{tikzpicture}
        
        \textbf{$x_1$ and $x_4$ are \textit{NOT connected}, conditionally}
        
    \end{minipage}
\end{figure}

Intuitively, a sparse graph is much simpler, which is why conditional independence is desired. So how to achieve sparsity? We can again use a L-1 regularization when maximizing the log-likelihood $\mathcal{L}(\boldsymbol{\Omega})$. Let the sample covariance matrix $\mathbf{S} = \frac{1}{n}\sum^n_{i=1}\mathbf{x}_i\mathbf{x}_i'$
$$
\log \det (\boldsymbol{\Omega}) - \mathrm{tr}(\mathbf{S}\boldsymbol{\Omega}) - \rho\left\Vert \boldsymbol{\Omega} \right\Vert _1
$$

%\newpage
%\bibliographystyle{plainnat}
%\bibliography{ref.bib}

\end{document}