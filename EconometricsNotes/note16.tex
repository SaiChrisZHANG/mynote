\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{16}{Graphical Network Inference}{}{Sai Zhang}{}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Consider a classic question: For $n$ observations of dimension $p$, how can we capture the statistical relationships between the variables of interest? Consider the example of the multivariate Gaussian distribution:
\begin{example}{Multivariate Gaussian Distribution}{}
    Suppose we have $n$ observations of dimension $p$, $\mathbf{x}\sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$. let $\mathbf{S}$ be the empirical covariance matrix. Then the probability density 
    $$
    f(\mathbf{x}) = \frac{1}{(2\pi)^{p/2}\det (\boldsymbol{\Sigma})^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \right\}
    $$
    define the \textbf{inverse covariance matrix} or \textbf{precision matrix} as $ \boldsymbol{\Omega}=\boldsymbol{\Sigma}^{-1} $, then we have 
    $$
    f_{\mathbf{\mu},\boldsymbol{\Omega}} = \exp \left\{ \boldsymbol{\mu'\Omega x} - \left< \boldsymbol{\Omega},\frac{1}{2}\mathbf{xx}' \right> -\frac{p}{2}\log(2\pi) + \frac{1}{2} \log\det(\boldsymbol{\Omega}) - \frac{1}{2}\boldsymbol{\mu'\Omega\mu} \right\}
    $$
    where $\left< \mathbf{A},\mathbf{B} \right> = \mathrm{tr}(\mathbf{AB})$.
\end{example}
In this example, we know that \textbf{every} multivariate Gaussian distribution can be represented by a pairwise \myhl[myblue]{\textbf{Gaussian Markov Random Field (GMRF)}}, which an \textbf{\underline{undirected graph}} $G=(V,E)$
\begin{itemize}
    \item representing the collection of variables $\mathbf{x}$ by a vertex set $\mathcal{V}=\left\{1,\cdots,p\right\}$
    \item encoding correlations between variables by a set of edges $\mathcal{E}=\left\{ (i,j)\in \mathcal{V}\mid i=\neq j,\Omega_{ij}\neq 0 \right\}$  
\end{itemize}

For simplicity, we normalize $\boldsymbol{\mu} = \mathbf{0}$. If we draw $n$ i.i.d. samples $\mathbf{x}_1,\cdots,\mathbf{x}_n \sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$, then the log-likelihood is
\begin{align*}
    \mathcal{L}(\boldsymbol{\Omega}) &= \frac{1}{n}\sum^n_{i=1}\log f(\mathbf{x}_i) = \frac{1}{2}\log \det (\boldsymbol{\Omega}) - \frac{1}{2n}\sum^n_{i=1}\mathbf{x}'_1\boldsymbol{\Theta}\mathbf{x}_i \\
    &= \frac{1}{2}\log\det (\boldsymbol{\Omega}) - \frac{1}{2}\left< \Omega,\frac{1}{n}\sum^n_{i=1}\mathbf{x}'_i\mathbf{x}'_i \right>
\end{align*}

\paragraph*{What's the goal?} We want to estimate a \myhl[myblue]{\textbf{sparse}} graph structure given $n\ll p$ i.i.d. observations. But what does sparsity means in this context? A sparse graph is \textbf{\underline{equivalent}} to a sparse precision matrix: the precision matrix should have many 0s.

\paragraph*{Sparse precision matrix} for the Gaussian vector mentioned above $\mathbf{x}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})$, we have $\forall u,v$ $$ x_u \perp x_v \mid \mathbf{x}_{\mathcal{V}\setminus \left\{ u,v \right\} } \Leftrightarrow \Omega_{u,v}=0 $$
that is, sparsity of the precision matrix is equivalent to \myhl[myblue]{\textbf{conditional independence}}\footnote{Meanwhile, for independence: $\Sigma_{u,v}=0\Leftrightarrow x_u \perp x_v$}. Consider \hyperref[fig:sparse_precision_graph]{a graph}, where $x_1$ and $x_4$ are only connected through other nodes, that is $x_1$ and $x_4$ are conditional independent, then we can have the precision matrix be something like:
$$
\boldsymbol{\Theta} = \begin{bmatrix}
    * & * & 0 & 0 & * & 0 & 0 & 0 \\
    * & * & 0 & 0 & 0 & * & * & 0 \\
    0 & 0 & * & 0 & * & 0 & 0 & * \\
    0 & 0 & 0 & * & 0 & 0 & * & 0 \\
    * & 0 & * & 0 & * & 0 & 0 & * \\
    0 & * & 0 & 0 & 0 & * & 0 & 0 \\
    0 & * & 0 & * & 0 & 0 & * & 0 \\
    0 & 0 & * & 0 & * & 0 & 0 & *
\end{bmatrix}
$$
where 0 captures precisely the conditional independence.

\begin{figure}[ht]\label{fig:sparse_precision_graph}
    \begin{minipage}[b]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[scale=1]
        % Step
        \draw[color=myblue] (-2,4) node[draw,circle] (x1) {$x_1$};
        \draw[color=myblue] (2,4) node[draw,circle] (x5) {$x_5$};
        \draw[color=myblue] (0,3) node[draw,circle] (x4) {$x_4$};
        \draw[color=myblue] (-3,2) node[draw,circle] (x2) {$x_2$};
        \draw[color=myblue] (0.3,1.5) node[draw,circle] (x6) {$x_6$};
        \draw[color=myblue] (3,2) node[draw,circle] (x8) {$x_8$};
        \draw[color=myblue] (-2,0) node[draw,circle] (x3) {$x_3$};
        \draw[color=myblue] (2.5,0) node[draw,circle] (x7) {$x_7$};
        % egdes
        \draw[color=myred, very thick] (x1) -- (x5);
        \draw[color=myred, very thick] (x5) -- (x8);
        \draw[color=myred, very thick] (x8) -- (x3);
        \draw[color=myred, very thick] (x3) -- (x5);
        \draw[color=myred, very thick] (x1) -- (x2);
        \draw[color=myred, very thick] (x2) -- (x6);
        \draw[color=myred, very thick] (x2) -- (x7);
        \draw[color=myred, very thick] (x4) -- (x7);
    \end{tikzpicture}
    
    \textbf{$x_1$ and $x_4$ are \textit{connected}}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[scale=1]
            % Step
            \draw[color=myblue] (-2,4) node[draw,circle] (x1) {$x_1$};
            \draw[color=myblue!35!white] (2,4) node[draw,circle] (x5) {$x_5$};
            \draw[color=myblue] (0,3) node[draw,circle] (x4) {$x_4$};
            \draw[color=myblue!35!white] (-3,2) node[draw,circle] (x2) {$x_2$};
            \draw[color=myblue!35!white] (0.3,1.5) node[draw,circle] (x6) {$x_6$};
            \draw[color=myblue!35!white] (3,2) node[draw,circle] (x8) {$x_8$};
            \draw[color=myblue!35!white] (-2,0) node[draw,circle] (x3) {$x_3$};
            \draw[color=myblue!35!white] (2.5,0) node[draw,circle] (x7) {$x_7$};
            % egdes
            \draw[color=myred!35!white, very thick, densely dashed] (x1) -- (x5);
            \draw[color=myred!35!white, very thick, densely dashed] (x5) -- (x8);
            \draw[color=myred!35!white, very thick, densely dashed] (x8) -- (x3);
            \draw[color=myred!35!white, very thick, densely dashed] (x3) -- (x5);
            \draw[color=myred!35!white, very thick, densely dashed] (x1) -- (x2);
            \draw[color=myred!35!white, very thick, densely dashed] (x2) -- (x6);
            \draw[color=myred!35!white, very thick, densely dashed] (x2) -- (x7);
            \draw[color=myred!35!white, very thick, densely dashed] (x4) -- (x7);
        \end{tikzpicture}
        
        \textbf{$x_1$ and $x_4$ are \textit{NOT connected}, conditionally}
        
    \end{minipage}
\end{figure}

Intuitively, a sparse graph is much simpler, which is why conditional independence is desired. So how to achieve sparsity? We can again use a L-1 regularization when maximizing the log-likelihood $\mathcal{L}(\boldsymbol{\Omega})$. Denote the sample covariance matrix as $\mathbf{S} = \frac{1}{n}\sum^n_{i=1}\mathbf{x}_i\mathbf{x}_i'$, then the problem becomes the so-called \myhl[myblue]{\textbf{Graphical Lasso}}
$$
\max_{\boldsymbol{\Omega}\geq \mathbf{0}}\log \det (\boldsymbol{\Omega}) - \mathrm{tr}(\mathbf{S}\boldsymbol{\Omega}) - \rho\left\Vert \boldsymbol{\Omega} \right\Vert _1
$$
which is equivalent to 
$$
\min_{\boldsymbol{\Omega}\geq \mathbf{0}} -\log \det (\boldsymbol{\Omega}) + \mathrm{tr}(\mathbf{S}\boldsymbol{\Omega}) +\rho\left\Vert \boldsymbol{\Omega} \right\Vert _1
$$

\section{Graphical Lasso}
The graphical lasso method is developed by \citep{friedman2008sparse}. For the optimization problem 
\begin{equation}
    \min_{\boldsymbol{\Omega}\geq \mathbf{0}} -\log \det (\boldsymbol{\Omega}) + \mathrm{tr}(\mathbf{S}\boldsymbol{\Omega}) +\rho\left\Vert \boldsymbol{\Omega} \right\Vert _1 
\end{equation}
The first-order optimality condition gives
\begin{align*}
    &\mathbf{0} \in \boldsymbol{\Omega}^{-1} - \mathbf{S} -\lambda \boldsymbol{\Gamma}
    %\Rightarrow & \boldsymbol{\Omega}^{-1}_{i,i} = \mathbf{S}_{i,i} + \lambda, \ 1\leq i \leq p & \text{in diagonal entries (self-loop), }1\in\partial\left\vert \boldsymbol{\Omega}_{i,i} \right\vert
\end{align*}
where $\boldsymbol{\Gamma}$ is a matrix of component-wise signs of $\boldsymbol{\Omega}$
\begin{align*}
    \boldsymbol{\Gamma} = \partial \left\Vert \boldsymbol{\Omega} \right\Vert _1 & \Rightarrow \gamma_{jk} \begin{cases}
        = \text{sign}(\omega_{jk}), &\omega_{jk}\neq 0\\
        \in  [-1,1], &\omega_{jk}=0
    \end{cases}
\end{align*}
since in a graph, we always have that, following the global stationarity conditions, $\omega_{jj}>0$, which implies that 
\begin{align}\label{eq:ghlasso_stationarity_condition}
    w_{ii} &= s_{ii} + \lambda & i=1,\cdots,p
\end{align}
where we denote a working version of $\boldsymbol{\Omega}^{-1}$ as $\mathbf{W}$.

The idea is to repeatedly cycle through all columns-rows and in each step optimize only a single column-row. Consider the following partition where all matrices are partitioned into one column/row versus the rest
\begin{align*}
    \boldsymbol{\Omega} &= \begin{pmatrix}
        \boldsymbol{\Omega}_{11} & \boldsymbol{\omega}_{12}\\
        \boldsymbol{\omega}_{12}' & \omega_{22}
     \end{pmatrix} & \mathbf{S} &= \begin{pmatrix}
        \mathbf{S}_{11} & \mathbf{s}_{12}\\
        \mathbf{s}_{12}' & s_{22}
     \end{pmatrix} & \mathbf{W} &= \begin{pmatrix}
        \mathbf{W}_{11} & \mathbf{w}_{12}\\
        \mathbf{w}_{12}' & w_{22}
     \end{pmatrix} & \boldsymbol{\Gamma} &= \begin{pmatrix}
        \boldsymbol{\Gamma}_{11} & \boldsymbol{\gamma}_{12}\\
        \boldsymbol{\gamma}_{12}' & \gamma_{22}
     \end{pmatrix}
\end{align*}
apply this partition to the optimality condition, get
\begin{align*}
    \boldsymbol{\Omega}^{-1} &= \mathbf{S} - \lambda\boldsymbol{\Gamma} \\
    \begin{pmatrix}
        \mathbf{W}_{11} & \mathbf{w}_{12}\\
        \mathbf{w}_{12}' & w_{22}
    \end{pmatrix} &= \begin{pmatrix}
        \mathbf{S}_{11} & \mathbf{s}_{12}\\
        \mathbf{s}_{12}' & s_{22}
     \end{pmatrix} + \lambda \begin{pmatrix}
        \boldsymbol{\Gamma}_{11} & \boldsymbol{\gamma}_{12}\\
        \boldsymbol{\gamma}_{12}' & \gamma_{22}
     \end{pmatrix}
\end{align*}
where $\boldsymbol{\Omega}_{11}$ is $(p-1)\times(p-1)$, $\boldsymbol{\omega}_{12}$ is $(p-1)\times 1$, $\omega_{22}$ is a scalar.

Consider a \myhl[myblue]{\textbf{blockwise}} step: suppose we fix all but the last row/column, then using properties of inverses of block-partitioned matrices, we have 
\begin{align*}
    \begin{pmatrix}
        \mathbf{W}_{11} & \mathbf{w}_{12}\\
        \mathbf{w}_{12}' & w_{22}
     \end{pmatrix} &= \begin{pmatrix}
        \left( \boldsymbol{\Omega}_{11}-\frac{\boldsymbol{\omega}_{12}\boldsymbol{\omega}_{12}'}{\omega_{22}} \right)^{-1} & -\mathbf{W}_{11}\frac{\boldsymbol{\omega}_{12}}{\omega_{22}}\\
         & \frac{1}{\omega_{22}}-\frac{\boldsymbol{\omega}_{12}'\mathbf{W}_{11}\boldsymbol{\omega}_{12}}{\omega^2_{22}}
     \end{pmatrix}\\
     &= \begin{pmatrix}
        \boldsymbol{\Omega}^{-1}_{11}+\frac{\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}^{-1}_{11}}{\omega_{22}-\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}} & -\frac{\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}}{\omega_{22}-\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}} \\
        & \frac{1}{\omega_{22}-\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}}
     \end{pmatrix}
\end{align*}

then, by the partitioned optimality condition, we have\footnote{For Eq.\ref{eq:modified_glasso_conditions}, by Eq.\ref{eq:ghlasso_stationarity_condition}, we know that $w_{22}=s_{22}+\lambda$, which is fixed.}:
\begin{align}\label{eq:glasso_conditions}
    \mathbf{0} &= -\mathbf{w}_{12} + \mathbf{s}_{12}+\lambda \boldsymbol{\gamma}_{12} = \mathbf{W}_{11}\frac{\boldsymbol{\omega}_{12}}{\omega_{22}}+ \mathbf{s}_{12}+\lambda \boldsymbol{\gamma}_{12}
\end{align}
\begin{align}\label{eq:modified_glasso_conditions}
    \mathbf{0} &= \frac{\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}}{\omega_{22}-\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}} + \mathbf{s}_{12} + \lambda\boldsymbol{\gamma}_{12} = w_{22}\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12} + \mathbf{s}_{12} + \lambda\boldsymbol{\gamma}_{12}
\end{align}

The graphic Lasso algorithm them solves Eq.\ref{eq:glasso_conditions} for $\boldsymbol{\beta}=\boldsymbol{\omega}_{12}/\omega_{12}$, that is
$$
\mathbf{W}_{11}\boldsymbol{\beta} + \mathbf{s}_{12} + \lambda\boldsymbol{\gamma}_{12} = \mathbf{0}
$$
where $\boldsymbol{\gamma}_{12}\in\mathrm{sign}(\boldsymbol{\beta})$ since $\omega_{22}>0$, which is essentially solving:
$$
\min_{\boldsymbol{\beta}\in\mathbb{R}^{p-1}}\left\{ \frac{1}{2}\boldsymbol{\beta}'\mathbf{W}_{11}\boldsymbol{\beta} + \boldsymbol{\beta}'\mathbf{s}_{12} + \lambda\left\Vert \boldsymbol{\beta} \right\Vert _1 \right\}
$$
and $\mathbf{W}_{11}>0$ is assumed to be fixed. 

This problem is analogous to a lasso regression problem of \myhl[myblue]{the \textbf{last variable} on \textbf{the rest}}, but the cross-product matrix $\mathbf{S}_{11}$ is replaced by its \textbf{\underline{current estimation}} $\mathbf{W}_{11}$. It is relatively easier to solve using elementwise coordinate descent, then
\begin{align*}
    \mathbf{w}_{12} &= -\mathbf{W}_{11}\frac{\boldsymbol{\omega}_{12}}{\omega_{22}} &\Rightarrow \hat{\mathbf{w}}_{12} &= -\mathbf{W}_{11}\hat{\boldsymbol{\beta}} & \text{Step 1}\\
    w_{22} &= \frac{1}{\omega_{22}}-\frac{\boldsymbol{\omega}_{12}'\mathbf{W}_{11}\boldsymbol{\omega}_{12}}{\omega^2_{22}} &\Rightarrow \frac{1}{\hat{\omega}_{22}} &= w_{22} - \hat{\boldsymbol{\beta}}'\hat{\mathbf{w}}_{12} & \text{Step 2} \\
    \boldsymbol{\omega}_{12} &= -\mathbf{W}_{11}^{-1}\mathbf{w}_{12}\omega_{22} &\Rightarrow \hat{\boldsymbol{\omega}}_{12} &= -\mathbf{W}^{-1}_{11}\hat{\mathbf{w}}_{12}\hat{\omega}_{22} & \text{Step 3}
\end{align*}
notice that after solving for $\boldsymbol{\beta}$ and updating $\mathbf{w}_{12}$ in Step 1, the graphic Lasso procedure can move onto the next block, that is, only Step 1 is used in the loop, Step 2 and 3 can be done at the end. The algorithm can be summarized as:

\begin{algorithm}{Graphical Lasso algorithm}{glasso_algorithm}
    \begin{itemize}
        \item[1] Initialize $\mathbf{W}= \mathbf{S} + \lambda\mathbf{I}$ 
        \item Cycle around the columns repeatedly, performing the following steps till convergence: 
        \begin{itemize}
            \item[a] rearrange the rows/columns so that the target column is \textbf{\underline{the last}} (implicitly)
            \item[b] solve the lasso problem, starting the solution from the previous round for this column
            \item[c] update the row/column (\textbf{\textit{off-diagonal}}) of the covariance using $\hat{\mathbf{w}}_{12}$
            \item[d] save $\hat{\boldsymbol{\beta}}$ for this column in the matrix $\mathbf{B}$
        \end{itemize}
        \item[3] after convergence, for every row/column, compute the diagonal entries $\hat{\omega}_{jj}$, and covert the $\mathbf{B}$ matrix to $\boldsymbol{\Omega}$
    \end{itemize}
\end{algorithm}

\section{What Is GLasso Solving?}
Again, consider the optimization problem 
\begin{equation*}
    \min_{\boldsymbol{\Omega}\geq \mathbf{0}} -\log \det (\boldsymbol{\Omega}) + \mathrm{tr}(\mathbf{S}\boldsymbol{\Omega}) +\rho\left\Vert \boldsymbol{\Omega} \right\Vert _1 
\end{equation*}
and its stationarity condition
\begin{align*}
    &\mathbf{0} = \boldsymbol{\Omega}^{-1} - \mathbf{S} -\lambda \boldsymbol{\Gamma}
    %\Rightarrow & \boldsymbol{\Omega}^{-1}_{i,i} = \mathbf{S}_{i,i} + \lambda, \ 1\leq i \leq p & \text{in diagonal entries (self-loop), }1\in\partial\left\vert \boldsymbol{\Omega}_{i,i} \right\vert
\end{align*}
rewrite the stationarity condition
\begin{align*}
    \mathbf{0} &= \boldsymbol{\Omega}^{-1} - \mathbf{S} -\lambda \boldsymbol{\Gamma} = \boldsymbol{\Omega} - \left(\mathbf{S}+ \lambda\boldsymbol{\Gamma}\right)^{-1}
\end{align*}
since $\boldsymbol{\Gamma}=\mathrm{sign}(\boldsymbol{\Omega})$, write $\tilde{\boldsymbol{\Gamma}}=\lambda \boldsymbol{\Gamma}$, we have $\left\Vert \tilde{\boldsymbol{\Gamma}} \right\Vert _{\infty}\leq \lambda$. Denote the element-wise absolute value matrix of $\boldsymbol{\Omega}$ as $\mathrm{abs}(\boldsymbol{\Omega})$, then let $\tilde{\boldsymbol{\Gamma}} = \lambda \boldsymbol{\Gamma}$, $\mathbf{P}=\mathrm{abs}(\boldsymbol{\Omega})$, we have 
\begin{align*}
    \mathbf{0} &= \boldsymbol{\Omega} - \left(\mathbf{S}+ \lambda\boldsymbol{\Gamma}\right)^{-1}\\
    &= \mathbf{P} \circ \mathrm{sign}(\tilde{\boldsymbol{\Gamma}}) - (\mathbf{S}+\tilde{\boldsymbol{\Gamma}})^{-1}
\end{align*}
and mechanically, we also have
\begin{align*}
    \mathbf{P} \circ \left(\mathrm{abs}(\tilde{\boldsymbol{\Gamma}}) -\lambda\mathbf{1}_p\mathbf{1}'_p \right) &= \mathbf{0} \\
    \left\Vert \tilde{\boldsymbol{\Gamma}} \right\Vert _{\infty} &\leq \lambda
\end{align*}
together, these are just hte KKT optimality condition for the following box-constrained SDP
\begin{equation}
    \max_{\tilde{\boldsymbol{\Gamma}}:\left\Vert \tilde{\boldsymbol{\Gamma}} \right\Vert _{\infty}\leq \lambda} g(\tilde{\boldsymbol{\Gamma}}) \coloneq \log\det (\mathbf{S}+\tilde{\boldsymbol{\Gamma}}) + p
\end{equation}
with the transformation $\mathbf{S}+\tilde{\boldsymbol{\Gamma}} = \boldsymbol{\Omega}^{-1}$. Essentially, this is the dual problem of the initial optimization problem, both of them are solved by the GLasso algorithm.

\paragraph*{Issues of GLasso method}:
\begin{itemize}
    \item the non-monotonic behavior of Glasso in minimizing $f(\boldsymbol{\Omega})$
    \begin{itemize}
        \item[-] $\boldsymbol{\theta}_{12}$ is entangled in $\mathbf{W}_{11}$, which is \myhl[myblue]{\textbf{\textit{incorrectly}}} treated as a constant
        \item[-] after updating $\boldsymbol{\theta}_{12}$, the entire (working) covariance matrix $\mathbf{W}$ changes, but Glasso algorithm only updates $\mathbf{w}_{12}$ and $\mathbf{w}_{12}'$
    \end{itemize}
    \item high dimesnionality problems
    \begin{itemize}
        \item[-] not computationally efficient when $p$ is ultra-large
        \item[-] $\boldsymbol{\Sigma}^{-1}$ doesn't exist when $p>n$
        \item[-] method is not scalable 
    \end{itemize}
\end{itemize}
Next, we address these issues by introducing some modifications. 

\section{Graphical Lasso: Modifications}
\subsection{Primal GLasso}
Consider the optimality condition in Eq.\ref{eq:modified_glasso_conditions}:
\begin{align*}
    \mathbf{0} &= \frac{\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}}{\omega_{22}-\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12}} + \mathbf{s}_{12} + \lambda\boldsymbol{\gamma}_{12} = w_{22}\boldsymbol{\Omega}^{-1}_{11}\boldsymbol{\omega}_{12} + \mathbf{s}_{12} + \lambda\boldsymbol{\gamma}_{12}
\end{align*}
Here, the dependence of the covariance submatrix $\mathbf{W}_{11}$ on $\boldsymbol{\Omega}_{12}$ is \myhl[myblue]{\textbf{explicit}}. Let $\boldsymbol{\alpha} = \boldsymbol{\omega}_{12}w_{22}$ with fixed $w_{22}\geq 0$\footnote{$w_{22}= 1/(\omega_22-\boldsymbol{\omega}_{12}'\boldsymbol{\Omega}_{11}^{-1}\boldsymbol{\omega}_{12})$}, then this optimality condition is essentially solving
$$
\min_{\boldsymbol{\alpha}\in\mathbb{R}^{p-1}}\left\{ \frac{1}{2}\boldsymbol{\alpha'\Omega}_{11}^{-1}\boldsymbol{\alpha} + \boldsymbol{\alpha}'\mathbf{s}_{12} + \lambda\left\Vert \boldsymbol{\alpha}\right\Vert _1 \right\}
$$
the minimizer of this problem $\hat{\boldsymbol{\alpha}}$ can then be used to derive the estimation for $\boldsymbol{\omega}_{12}$: $$\hat{\boldsymbol{\omega}}_{12} = \frac{\hat{\boldsymbol{\alpha}}}{w_{22}}$$, then we can update $\omega_{22}$ as before via $$ \hat{\omega}_{22} = \frac{1}{w_{22}} + \hat{\boldsymbol{\omega}}_{12}'\boldsymbol{\Theta}^{-1}_{11} \hat{\boldsymbol{\omega}}_{12}$$ with $w_{22}=s_{22}+\lambda$. Another problem is how to obtain $\boldsymbol{\Omega}^{-1}_{11}$: as the iterations proceed, maintain $\mathbf{W}=\boldsymbol{\Omega}^{-1}$, and $\boldsymbol{\Omega}^{-1}_{11}$ can be derived from $$ \boldsymbol{\Omega}^{-1}_{11}=\mathbf{W}_{11}-\frac{\mathbf{w}_{12}\mathbf{w}_{12}'}{w_{22}} $$
once $\boldsymbol{\omega}_{12}$ is updated, the \myhl[myblue]{\textbf{\textit{entire}}} working covariance matrix $\mathbf{W}$ is updated using $\boldsymbol{\Omega}^{-1}_{11}$. This procedure, the so-called primal graphical lasso, can be represented in the following algorithm:

\begin{algorithm}{P-GLasso Algorithm}{pglasso_algorithm}
    \begin{itemize}
        \item[1] Initialize $\mathbf{W} = \mathrm{diag}(\mathbf{S}) + \lambda\mathbf{I}$ and $\boldsymbol{\Omega} = \mathbf{W}^{-1}$
        \item[2] Cycle around the columns repeatedly, performing the following steps till convergence:
        \begin{itemize}
            \item[a] rearrange the rows/columns so that the target column is the last (implicitly)
            \item[b] compute $\mathbf{\Omega}^{-1}_{11}$ using $\boldsymbol{\Omega}^{-1}_{11}=\mathbf{W}_{11}-\frac{\mathbf{w}_{12}\mathbf{w}_{12}'}{w_{22}}$
            \item[c] solve $\min_{\boldsymbol{\alpha}\in\mathbb{R}^{p-1}}\left\{ \frac{1}{2}\boldsymbol{\alpha'\Omega}_{11}^{-1}\boldsymbol{\alpha} + \boldsymbol{\alpha}'\mathbf{s}_{12} + \lambda\left\Vert \boldsymbol{\alpha}\right\Vert _1 \right\} $ for $\boldsymbol{\alpha}$, using as warm starts the solution from the previous round of row/column updates. Then update $\hat{\boldsymbol{\omega}_{12}} = \hat{\boldsymbol{\alpha}}/w_{22}$ and $\hat{\omega_{22}}$
            \item[d] update $\boldsymbol{\Omega}$ and $\mathbf{W}$, ensuring that $\boldsymbol{\Omega}\mathbf{W} = \mathbf{I}_p$
        \end{itemize}
        \item[3] output the solution: precision matrix $\boldsymbol{\Omega}$ and its exact inverse, covariance matrix $\mathbf{W}$
    \end{itemize}
\end{algorithm}

\subsection{Innovated Scalable Efficient Estimation}
Now, we try to tackle the high-dimesnionality issues: $\boldsymbol{\Sigma}^{-1}$ does \textbf{not} exist when $p>n$. Again, 
$$
\mathbf{x} \sim \mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})
$$
consider a linear transformation where $\tilde{\mathbf{x}} = \boldsymbol{\Omega}\mathbf{x}$ ($\boldsymbol{\Omega}$ is still the precision matrix $\boldsymbol{\Sigma}^{-1}$), and 
$$
\mathrm{cov}(\tilde{\mathbf{x}}) = \boldsymbol{\Omega}\mathrm{cov}(\mathbf{x})\boldsymbol{\Omega} = \boldsymbol{\Omega}\boldsymbol{\Sigma}\boldsymbol{\Omega}=\boldsymbol{\Omega}
$$
But $\boldsymbol{\Omega}$ is unknown and to be estimated. To get around this, we \textbf{break} the long vector $\tilde{\mathbf{x}}$ into small sub-vectors and then estimate each one.

\paragraph*{Notation} for subsets $A,B\subset \left\{ 1,\cdots,p \right\}$, let $\mathbf{x}_A$ denote a sub-vector of $\mathbf{x}$ formed by its components with indices in $A$, and the sub-(precision)-matrix is $\boldsymbol{\Omega}_{A,B}=\left(\omega_{jk}\right)_{j\in A,k\in B}$, $\boldsymbol{\Omega}_A\coloneq \boldsymbol{\Omega}_{A,A}$ for simplicity.
Then define $$\tilde{\mathbf{x}}_A = \boldsymbol{\Omega}_A\boldsymbol{\eta}_A$$
where $\boldsymbol{\eta}_A = \mathbf{x}_A+ \boldsymbol{\Omega}_A^{-1}\boldsymbol{\Omega}_{A,A^C}\mathbf{x}_{A^C}$, and $A^C\coloneq \left\{1,\cdots,p\right\}\setminus A$. With this definition, we have the following proposition:

\begin{proposition}{Conditional Distribution of Sub-vectors}{cond_disc_subvec}
    Conditional distribution $\mathbf{x}_A\mid \mathbf{x}_B \sim \mathcal{N}\left(\boldsymbol{\mu}_{A\mid B},\boldsymbol{\Sigma}_{A\mid B}\right)$, where 
    \begin{align*}
        \boldsymbol{\mu}_{A\mid B} &= \boldsymbol{\mu}_A + \boldsymbol{\Sigma}_{A,B}\boldsymbol{\Sigma}_B^{-1}\left(\mathbf{x}_B-\boldsymbol{\mu}_{B}\right) \\
        \boldsymbol{\Sigma}_{A\mid B} &= \boldsymbol{\Sigma}_A-\boldsymbol{\Sigma}_{A,B}\boldsymbol{\Sigma}_B^{-1}\boldsymbol{\Sigma}_{B,A}
    \end{align*}
    and when $\mathbf{x}_B=\mathbf{x}_{A^C}$, we have
    $$
    \mathbf{x}_A\mid \mathbf{x}_{A^C} \sim \mathcal{N}\left( -\boldsymbol{\Omega}_A^{-1}\boldsymbol{\Omega}_{A,A^C}\mathbf{x}_{A^C},\boldsymbol{\Omega}_A^{-1} \right)
    $$
\end{proposition}
Prop.\ref{prop:cond_disc_subvec} gives a multivariate linear regression model:
$$
\mathbf{x}_A = \mathbf{C}'_A\mathbf{x}_{A^C}+\boldsymbol{\eta}_A
$$
where $\mathbf{C}_A=-\boldsymbol{\Omega}_{A^C,A}\boldsymbol{\Omega}^{-1}_A$ is the coefficient matrix, and $\boldsymbol{\eta}_A$ is model errors with Gaussian distribution $\mathcal{N}(\mathbf{0},\boldsymbol{\Omega}_A^{-1})$. Then we can have the following algorithm to solve this problem:

\begin{algorithm}{ISEE Algorithm}{isee}
    \begin{itemize}
        \item[1] Let $(A_l)^L_{l=1}$ be a partition of index set $\left\{ 1,\cdots,p \right\}$, s.t. $\bigcup ^L_{l=1}A_l=\left\{1,\cdots,p \right\}$
        \item[2] estimate $\boldsymbol{\eta}_{A_l}$ and then obtain estimated $\tilde{\mathbf{x}}_{A_l}$
        \item[3] stack all estimated sub-vectors $\tilde{\mathbf{x}}_{A_l}$ together to obtain $\tilde{\mathbf{x}}$
    \end{itemize}
\end{algorithm}
ISEE algorithm breaks large-scale precision estimation into \textbf{\myhl[myblue]{small-scale linear regression problems}}, each of which is computationally efficient and effective.

\paragraph*{Estimation} for the $n\times p$ data matrix $\mathbf{X}=\left(\mathbf{x}_1,\cdots,\mathbf{x}_n\right)'$, we construct the linear transformation $$ \tilde{\mathbf{X}} = \left( \tilde{\mathbf{x}}_1,\cdots,\tilde{\mathbf{x} }_n \right) = \mathbf{X}\boldsymbol{\Omega} $$
then the multivariate linear regression model by matrix notation is 
$$
\mathbf{X}_A = \mathbf{X}_{A^C}\mathbf{C}_A+\mathbf{E}_A
$$
and the corresponding sub-matrix $\tilde{\mathbf{X}}_A$ can be written as
$$
\tilde{\mathbf{X}}_A = (\mathbf{X}\boldsymbol{\Omega})_A = \mathbf{X}_A\boldsymbol{\Omega}_A + \mathbf{X}_{A^C}\boldsymbol{\Omega}_{A^C,A} = \left( \mathbf{X}_A+\mathbf{X}_{A^C}\boldsymbol{\Omega}_{A^C,A}\boldsymbol{\Omega}^{-1}_A \right)\boldsymbol{\Omega}_A = \mathbf{E}_A\boldsymbol{\Omega}_A
$$
Sparsity is achieved bia scaled Lasso: for each node $j$ in index set $A$, $$ \mathbf{X}_j = \mathbf{A^C}\boldsymbol{\beta}_j + \mathbf{E}_j $$ where $\boldsymbol{\beta}_j$ is the column of $\mathbf{C}_A$ corresponds to node $j$. The estimation is then done in the following steps:
\begin{itemize}
    \item run the PLS 
    \begin{align*}
        \left(\hat{\boldsymbol{\beta}}_j,\hat{\theta}_j^{1/2}\right) &= \arg\min_{\boldsymbol{\beta}\in\mathbf{R}^{p-\left\vert A \right\vert },\sigma\geq 0} \left\{ \frac{\left\Vert \mathbf{X}_j-\mathbf{X}_{A^C} \boldsymbol{\beta} \right\Vert^2_2}{2n\sigma} + \frac{\sigma}{2} + \lambda\left\Vert \boldsymbol{\beta}_*\right\Vert _1 \right\}
    \end{align*}
    where $\boldsymbol{\beta}_*$ is component-wise product of $\boldsymbol{\beta}$ and $\left( \frac{1}{\sqrt{n}}\left\Vert \mathbf{X}_k \right\Vert _2 \right)_{k\in A^C}$, and the penalizing factor is $\lambda = C\left( \frac{2\log p}{n} \right)$.
    \item after obtaining $\hat{\boldsymbol{\beta}}_j$, get the estimation of the (partitioned) precision matrix $\hat{\boldsymbol{\Omega}}_A$ $$ \hat{\Omega}_A = \left(\frac{1}{n}\hat{\mathbf{E}}'_A\hat{\mathbf{E}}_A\right)^{-1} $$ where $\hat{\mathbf{E}}_j = \mathbf{X}_j -\mathbf{X}_{A^C}\hat{\boldsymbol{\beta}}_j$, $\hat{\mathbf{E}}_A=\left( \hat{\mathbf{E}}_j \right)_{j\in A}$
    \item for the whole partition $(A_l)^L_{l=1}$, we have $\hat{\mathbf{X}} = (\hat{\mathbf{X}}_{A_l})_{1\leq l\leq L}$ where $\hat{\mathbf{X}}_{A_l}=\hat{\mathbf{E}}_{A_l}\hat{\boldsymbol{\Omega}}_{A_l}$, then the initital estiation of the whole precision matrix is $\hat{\Omega}_{ISEE,ini} = \frac{1}{n}\hat{\mathbf{X}}'\hat{\mathbf{X}}$
    \item next, introduce a threshold $\tau \geq 0$, define $$ \hat{\boldsymbol{\Omega}}_{ISEE,g} = T_{\tau}\left( \hat{\boldsymbol{\Omega}}_{ISEE,ini} \right) $$ where $T_{\tau}(\mathbf{B}) = \left( b_{jk}\mathbf{1}_{\left\vert b_{jk}\right\vert \geq \tau} \right)$ for matrix $\mathbf{B}=\left(b_{jk}\right)$, then estimate the structure $\mathbf{E}$ as $\hat{\mathbf{E}}_{ISEE}=\mathrm{supp}\left( \hat{\boldsymbol{\Omega}}_{ISEE,g}\right)$. One can then use the cross validation method to choose the optimal threshold:
    \begin{itemize}
        \item[-] randomly split sample of $n$ rows into 2 samples of $n_1$ and $n_2$, repeat $N_1$ times. Denote $\hat{\Omega}^{1,\nu}_{ISEE,ini},\hat{\Omega}^{2,\nu}_{ISEE,ini}$ the corresponding covariance matrices
        \item[-] choose $\tau$ by minimizing $$ \mathcal{R}(\tau) = \frac{1}{N_1}\sum^{N_1}_{\nu=1}\left\Vert T_{\tau}\left( \hat{\Omega}^{1,\nu}_{ISEE,ini} \right) - \hat{\Omega}^{1,\nu}_{ISEE,ini} \right\Vert^2$$
    \end{itemize}
\end{itemize} 

\section{Heterogeneous Graphical Networks}
Next, we introduce heterogeneity into this problem: the samples are now drawn from $k$ different subpopulations
\begin{align*}
    \mathbf{X}^{(t)} = \left( \mathbf{X}^{(t)}_1,\cdots,\mathbf{X}^{(t)}_p \right) &\sim \mathcal{N}\left(\mathbf{0},\left(\boldsymbol{\Omega}^{(t)}\right)^{-1}\right) & t&=1,\cdots, k
\end{align*}
Some simple observations on this problem are
\begin{itemize}
    \item For each node $1\leq j \leq p$ in graph $1\leq t\leq k$ $$ \mathbf{X}^{(t)}_j\mid \mathbf{X}^{(t)}_{-j} \sim \mathcal{N}\left( {\mathbf{X}^{(t)}_{-j} }'\mathbf{C}^{(t)}_j,\frac{1}{\omega_{j,j}^{(t)}} \right) $$ with $\mathbf{C}_j^{(t)} = -\frac{\boldsymbol{\Omega}^{(t)}_{-j,j}}{\omega_{j,j}^{(t)}}$ and $\boldsymbol{\Omega}^{(t)} = \left( \omega_{a,b}^{(t)} \right)_{p\times p}$
    \item For each pair of nodes $(a,b)$, $$ \mathrm{Cov}\left( \boldsymbol{\epsilon}_a^{(t)},\boldsymbol{\epsilon}_b^{(t)} \right) = \frac{\omega^{(t)}_{a,b}}{\omega^{(t)}_{a,a}\omega^{(t)}_{b,b}} $$ with $\boldsymbol{\epsilon}^{(t)} = \mathbf{X}^{(t)}_j- {\mathbf{X}^{(t)}_{-j}}' \mathbf{C}^{(t)}_j \sim \mathcal{N}\left( \mathbf{0},\frac{1}{\omega_{j,j}^{(t)}} \right)$ \myhl[myblue]{\textbf{independent}} across $t$
    \item Under null hypothesis on joint link strength vector $$H_{0,ab}: \omega^0_{a,b} = \left( \omega^{(1)}_{a,b},\cdots,\omega_{a,b}^{(k)} \right)' = \mathbf{0} $$ distributions of $\mathbf{X}^{(t)}_j\mid \mathbf{X}^{(t)}_{-j}$ across $t$ share \textbf{similar sparsity structure} on coefficient vectors $\mathbf{C}_j^{(t)}$
\end{itemize}
Based on these observations, we can turn the problem of multiple-network estimation into a problem of \textbf{high-dimensional multi-response linear regression with heterogeneity}.

\subsection*{Nodewise Heterogeneous Multi-Response Regression}
For each node $1\leq j\leq p$, 
\begin{align*}
    \begin{pmatrix}
        \mathbf{X}_j^{(1)}\\
        \mathbf{X}_j^{(2)}\\
        \vdots\\
        \mathbf{X}_j^{(k)}
    \end{pmatrix} = \begin{pmatrix}
        \mathbf{X}_{-j}^{(1)} \\
        & \mathbf{X}_{-j}^{(2)} \\
        & & \ddots \\
        & & & \mathbf{X}_{-j}^{(k)}
    \end{pmatrix} \begin{pmatrix}
        \mathbf{C}_j^{(1)}\\
        \mathbf{C}_j^{(2)}\\
        \vdots\\
        \mathbf{C}_j^{(k)}
    \end{pmatrix}
    + \begin{pmatrix}
        \boldsymbol{\epsilon}_j^{(1)}\\
        \boldsymbol{\epsilon}_j^{(2)}\\
        \vdots\\
        \boldsymbol{\epsilon}_j^{(k)}
    \end{pmatrix}
\end{align*}
The noises are \myhl[myblue]{\textbf{Heterogeneous}} since $\omega_{j,j}^{(t)}$ generally varies over $1\leq t\leq k$. To tackle this problem with a tuning-free estimation procedure, follow this roadmap:
\begin{itemize}
    \item construct estimators based on residuals from the heterogeneous multi-response regressions
    \begin{itemize}
        \item Initial estimation $\hat{\mathbf{C}}_j^0 = \left( {\hat{\mathbf{C}}_j^{(1)'}},\cdots,{\hat{\mathbf{C}}_j^{(k)'}} \right)'$ for the $(p-1)k$ dimensional vector ${\mathbf{C}}_j^0 = \left( {{\mathbf{C}}_j^{(1)'}},\cdots,{{\mathbf{C}}_j^{(k)'}} \right)'$ with the tuning-free heterogeneous group square-root Lasso (HGSL) 
        $$ \hat{\mathbf{C}}^0_1 = \arg\min_{\boldsymbol{\beta}^0 \in \mathbb{R}^{(p-1)k}}\left\{ \sum^k_{t=1}\sqrt{\textcolor{myblue}{Q_t}\left(\boldsymbol{\beta}^{(t)}\right)} + \textcolor{myred}{\lambda}\sum^p_{l=2}\left\Vert \sqrt{\bar{D}_{1(l)}}\boldsymbol{\beta}^0_{(l)} \right\Vert  \right\} $$ where $\textcolor{myblue}{Q_t}\left(\boldsymbol{\beta}^{(t)}\right)$ is the quadratic loss over class $t$.
        \item Get the residuals $\hat{\mathbf{E}}^{(t)}_{i,j} = \mathbf{X}^{(t)}_{i,j} - \mathbf{X}^{(t)'}_{i,-j}\hat{\mathbf{C}}_{j}^{(t)}$ with $1\leq i\leq n^{(t)},1\leq j\leq p$, and $n^{(t)}$ the sample size for graph $t$.
        \item then get 
        \begin{itemize}
            \item an estimator for $\hat{\omega}_{j,j}^{(t)} = \frac{n^{(t)}}{\sum^{n^{(t)}}_{i=1}\hat{\mathbf{E}}_{i,j}^{(t)'}\hat{\mathbf{E}}_{i,j}^{(t)'}}$
            \item a bias-corrected statistic for estimating $-\mathrm{Cov}\left(\boldsymbol{\epsilon}^{(t)}_a,\boldsymbol{\epsilon}^{(t)}_b\right)$ $$ T^{(t)}_{n,k,a,b} = \frac{1}{n^{(t)}}\left[ \sum^{n^{(t)}}_{i=1} \hat{\mathbf{E}}_{i,a}^{(t)} \hat{\mathbf{E}}_{i,b}^{(t)} + \sum^{n^{(t)}}_{i=1} \left(\hat{\mathbf{E}}_{i,a}^{(t)}\right)^2 \hat{\mathbf{C}}_{b,a}^{(t)} + \sum^{n^{(t)}}_{i=1}\left(\hat{\mathbf{E}}_{i,b}^{(t)}\right)^2 \hat{\mathbf{C}}_{a,b}^{(t)} \right] $$ where bias can occur if $a\neq b$, and the statistic $T^{(t)}_{n,k,a,b}$ asymptotically close to statistic $$ J^{(t)}_{n,k,a,b} = \left[ 1-\omega_{a,a}^{(t)}\left(\hat{\omega}_{a,a}^{(t)}\right)^{-1} - \omega_{b,b}^{(t)}\left( \hat{\omega}_{b,b}^{(t)} \right)^{-1} \right] \frac{\omega_{a,b}^{(t)}}{\omega_{a,a}^{(t)}\omega_{b,b}^{(t)}} $$ which in turn asymptotically close to negative covariance $-\mathrm{Cov}\left(\boldsymbol{\epsilon}^{(t)}_a,\boldsymbol{\epsilon}^{(t)}_b\right)$.
        \end{itemize}
    \end{itemize}
    \item rely on 2 tests (chi-based and linear-functional-based) for common sparsity pattern with optimality 
    \begin{itemize}
        \item Chi-based test: test null hypothesis $H_{0,ab}:\omega_{a,b}^0 = \left(\omega_{a,b}^{(1)},\cdots,\omega_{a,b}^{(k)}\right)' =\mathbf{0}$ for joint link strength vector, then the Chi-based test statistic is defined as $$ U_{n,k,a,b}^2 = \sum^k_{t=1}n^{(t)}\hat{\omega}_{b,b}^{(t)}\hat{\omega}_{a,a}^{(t)}\left( T^{(t)}_{n,k,a,b} \right)^2 $$
    \end{itemize}
    \item develop the scaled iterative thresholding HGSL algorithm with provable convergence for the scalability of Tuning-free heterogeneous inference (THI)
\end{itemize}


\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}