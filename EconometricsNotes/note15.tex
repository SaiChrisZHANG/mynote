\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{15}{Sparse Orthogonal Factor Regression}{}{Sai Zhang}{Sparcity and dimensionality reduction for Multivariate Linear Regression models.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Consider a Mutlivariate Linear Regression (MLR) model
\begin{align*}
    \underset{n\times q}{\mathbf{Y}} = \underset{n\times p}{\mathbf{X}} \cdot \underset{p\times q}{\mathbf{C}}+ \underset{n\times q}{\mathbf{E}}
\end{align*}
How to apply regularization methods to this model? There are several approaches to consider 
\begin{itemize}
    \item \myhl[myblue]{\textbf{Shrinkage}}: ridge regression to overcome multicollinearity
    \item \myhl[myblue]{\textbf{sparsity}}: variable selection in multivariate setting 
    \item \myhl[myblue]{\textbf{Reduced-rank}}
    \begin{itemize}
        \item[-] \textbf{\underline{Dimension reduction}} via reducing rank of $\mathbf{C}$
        \item[-] $\min \lVert \mathbf{Y}-\mathbf{XC} \rVert^2_F$ s.t. $\mathrm{rank}(\mathbf{C})\leq r$  
    \end{itemize}
    \item \myhl[myblue]{\textbf{Combinations}}
    \item \myhl[myblue]{\textbf{Low-rank}} plus \myhl[myblue]{\textbf{sparse decomposition}}: robust PCA, latent variable graphical models, covariance estimation 
    \item \myhl[myblue]{\textbf{Regularized matrix}} or \myhl[myblue]{\textbf{tensor regression}}
\end{itemize}
Or, we can introduce a very attractive sparsity structure to achieve simultaneous dimension reduction and variable selection. This structure should be characterized by
\begin{itemize}
    \item Having a few \textbf{distinct} channels/pathways relating responses and predictors
    \item Each of such associations may involve only \textbf{a smaller subset}, but not all of the responses and predictors  
\end{itemize}
that is 
\begin{align*}
    \mathbf{Y} &= \mathbf{X}\mathbf{C} +\mathbf{E}\\
    &= \mathbf{X}\cdot \begin{pmatrix}
        c_{11} & c_{12} & \cdots & c_{1q} \\
        c_{21} & c_{22} & \cdots & c_{2q} \\
        \vdots & \vdots & \ddots & \vdots \\
        c_{p1} & c_{p2} & \cdots & c_{pq} 
    \end{pmatrix} + \mathbf{E} \\
    &= \mathbf{X}\cdot \begin{pmatrix}
        \textcolor{myblue}{0} & \textcolor{myred}{u_{12}} & \cdots & \textcolor{myorange}{u_{1r}} \\
        \textcolor{myblue}{u_{21}} & \textcolor{myred}{0} & \cdots & \textcolor{myorange}{c_{2r}} \\
        \textcolor{myblue}{\vdots} & \textcolor{myred}{\vdots} & \ddots & \textcolor{myorange}{\vdots} \\
        \textcolor{myblue}{u_{p1}} & \textcolor{myred}{u_{p2}} & \cdots & \textcolor{myorange}{u_{pr}} 
    \end{pmatrix}\cdot \begin{pmatrix}
        \textcolor{myblue}{d_1} & & & \\
         & \textcolor{myred}{d_2} & & \\
         & & \ddots & \\
         & & & \textcolor{myorange}{0}
    \end{pmatrix} \cdot \begin{pmatrix}
        \textcolor{myblue}{0} & \textcolor{myblue}{0} & \cdots & \textcolor{myblue}{v_{q1}} \\
        \textcolor{myred}{v_{12}} & \textcolor{myred}{v_{22}} & \cdots & \textcolor{myred}{0} \\
        \vdots & \vdots & \ddots & \vdots \\
        \textcolor{myorange}{v_{1r}} & \textcolor{myorange}{v_{2r}} & \cdots & \textcolor{myorange}{v_{qr}} 
    \end{pmatrix}
      + \mathbf{E}
\end{align*}
This way, we can have 
\begin{itemize}
    \item \myhl[myblue]{\textbf{Sparsity}}: selection of both \textbf{\underline{latent}} and \textbf{\underline{original}} variables
    \item \myhl[myblue]{\textbf{Low-rank SVD}}: different subsets of responses allowed to be associated with different subsets of predictors
\end{itemize}
Consider an example:
\begin{example}{Dimension Reduction and Variable Selection via Sparse SVD}{sparse_svd}
    Consider the case where $p=1000,q=100$, then $C$, as a $p\times q$ matrix, contains 100000 coefficients. Meanwhile, for a rank-3 SVD model:
    $$
     \mathbf{C} = d_1\mathbf{u}_1\mathbf{v}_1' + d_2\mathbf{u}_2\mathbf{v}_2' + d_3\mathbf{u}_3\mathbf{v}_3'
    $$
    where $\mathbf{u}_1,\mathbf{u}_2,\mathbf{u}_3$ are all $p\times 1$, $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ are all $q\times 1$, $d_1,d_2,d_3$ are all scalars. Hence, there are only $3\times (1000+100+1) = 3303$ paramaters to estimate. If futher assume sparcity, the dimension would be even lower.
\end{example}
Now let's develop a scalable procedure for this idea.

\section{Sparse Orthogonal Factor Regression}
Consider the sigular value decomposition of $\mathbf{C}$
$$
\mathbf{C} = \mathbf{UDV}'=\sum^r_{k=1}d_k\mathbf{u}_k\mathbf{v}_k'
$$
where $\mathbf{U}$ and $\mathbf{V}$ are both \myhl[myblue]{\textbf{orthonormal}}: $\mathbf{UU}' = \mathbf{VV}'=\mathbf{I}$. Then we can achieve dimension reduction via \textbf{low-dimensional latent model} $$ \tilde{\mathbf{Y}} = \tilde{\mathbf{X}}\mathbf{D}+\tilde{\mathbf{E}} $$ where 
\begin{itemize}
    \item $\tilde{\mathbf{Y}} = \mathbf{YV}$: $\mathbf{V}$ sparsity leads to \textbf{\underline{response}} variable selection
    \item $\tilde{\mathbf{X}} = \mathbf{XU}$: $\mathbf{U}$ sparsity leads to \textbf{\underline{predictor}} variable selection
\end{itemize}

How consider 
\begin{align}
    \left( \hat{\mathbf{D}},\hat{\mathbf{U}},\hat{\mathbf{V}} \right) &= \arg\min_{\mathbf{D,U,V}}\left\{ \frac{1}{2}\left\Vert \mathbf{Y-XUDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{UD}) + \lambda_b \rho_b (\mathbf{VD})\right\} & \text{s.t. } \mathbf{U'U}=\mathbf{V'V}=\mathbf{I}_m 
\end{align}
where 
\begin{itemize}
    \item $\rho_a(\cdot),\rho_b(\cdot)$ are penalty functions with regularization parameters $\lambda_d,\lambda_a,\lambda_b\geq 0$. These sparsity penalizations on $\mathbf{UD}$ and $\mathbf{VD}$ can be thought as \textbf{importance weighting}
    \item $\lVert \cdot \rVert _F$ is the nuclear norm, defined as the \textbf{sum} of its singular values $\lVert \mathbf{A}\rVert _F =\sum_i\sigma_i(\mathbf{A})$. It encourages sparsity among singular values and achieve \textbf{\underline{rank reduction}}
    \item The orthgonality on $\mathbf{U,V}$ allow a flexible form of sparsity-inducing penalties
\end{itemize}



Two applications are
\begin{itemize}
    \item Biclustering with sparse SVD 
    \begin{align}
        \left( \hat{\mathbf{D}},\hat{\mathbf{U}},\hat{\mathbf{V}} \right) &= \arg\min_{\mathbf{D,U,V}}\left\{ \frac{1}{2}\left\Vert \mathbf{X-UDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{UD}) + \lambda_b \rho_b (\mathbf{VD})\right\} & \text{s.t. } \mathbf{U'U}=\mathbf{V'V}=\mathbf{I}_m 
    \end{align}
    \item Sparse PCA (sparsity in loadings of principla components)
    \begin{align}
        \left( \hat{\mathbf{A}},\hat{\mathbf{V}} \right) &= \arg\min_{\mathbf{A,V}}\left\{ \frac{1}{2}\left\Vert \mathbf{X-XAV}' \right\Vert _F^2  + \lambda_a \rho_a (\mathbf{A}) \right\} & \text{s.t. } \mathbf{V'V}=\mathbf{I}_m 
    \end{align}
\end{itemize}

%\newpage
%\bibliographystyle{plainnat}
%\bibliography{ref.bib}

\end{document}