\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{15}{Sparse Orthogonal Factor Regression}{}{Sai Zhang}{Sparcity and dimensionality reduction for Multivariate Linear Regression models.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Consider a Mutlivariate Linear Regression (MLR) model
\begin{align*}
    \underset{n\times q}{\mathbf{Y}} = \underset{n\times p}{\mathbf{X}} \cdot \underset{p\times q}{\mathbf{C}}+ \underset{n\times q}{\mathbf{E}}
\end{align*}
How to apply regularization methods to this model? There are several approaches to consider 
\begin{itemize}
    \item \myhl[myblue]{\textbf{Shrinkage}}: ridge regression to overcome multicollinearity
    \item \myhl[myblue]{\textbf{sparsity}}: variable selection in multivariate setting 
    \item \myhl[myblue]{\textbf{Reduced-rank}}
    \begin{itemize}
        \item[-] \textbf{\underline{Dimension reduction}} via reducing rank of $\mathbf{C}$
        \item[-] $\min \lVert \mathbf{Y}-\mathbf{XC} \rVert^2_F$ s.t. $\mathrm{rank}(\mathbf{C})\leq r$  
    \end{itemize}
    \item \myhl[myblue]{\textbf{Combinations}}
    \item \myhl[myblue]{\textbf{Low-rank}} plus \myhl[myblue]{\textbf{sparse decomposition}}: robust PCA, latent variable graphical models, covariance estimation 
    \item \myhl[myblue]{\textbf{Regularized matrix}} or \myhl[myblue]{\textbf{tensor regression}}
\end{itemize}
Or, we can introduce a very attractive sparsity structure to achieve simultaneous dimension reduction and variable selection. This structure should be characterized by
\begin{itemize}
    \item Having a few \textbf{distinct} channels/pathways relating responses and predictors
    \item Each of such associations may involve only \textbf{a smaller subset}, but not all of the responses and predictors  
\end{itemize}
that is 
\begin{align*}
    \mathbf{Y} &= \mathbf{X}\mathbf{C} +\mathbf{E}\\
    &= \mathbf{X}\cdot \begin{pmatrix}
        c_{11} & c_{12} & \cdots & c_{1q} \\
        c_{21} & c_{22} & \cdots & c_{2q} \\
        \vdots & \vdots & \ddots & \vdots \\
        c_{p1} & c_{p2} & \cdots & c_{pq} 
    \end{pmatrix} + \mathbf{E} \\
    &= \mathbf{X}\cdot \begin{pmatrix}
        \textcolor{myblue}{0} & \textcolor{myred}{u_{12}} & \cdots & \textcolor{myorange}{u_{1r}} \\
        \textcolor{myblue}{u_{21}} & \textcolor{myred}{0} & \cdots & \textcolor{myorange}{c_{2r}} \\
        \textcolor{myblue}{\vdots} & \textcolor{myred}{\vdots} & \ddots & \textcolor{myorange}{\vdots} \\
        \textcolor{myblue}{u_{p1}} & \textcolor{myred}{u_{p2}} & \cdots & \textcolor{myorange}{u_{pr}} 
    \end{pmatrix}\cdot \begin{pmatrix}
        \textcolor{myblue}{d_1} & & & \\
         & \textcolor{myred}{d_2} & & \\
         & & \ddots & \\
         & & & \textcolor{myorange}{0}
    \end{pmatrix} \cdot \begin{pmatrix}
        \textcolor{myblue}{0} & \textcolor{myblue}{0} & \cdots & \textcolor{myblue}{v_{q1}} \\
        \textcolor{myred}{v_{12}} & \textcolor{myred}{v_{22}} & \cdots & \textcolor{myred}{0} \\
        \vdots & \vdots & \ddots & \vdots \\
        \textcolor{myorange}{v_{1r}} & \textcolor{myorange}{v_{2r}} & \cdots & \textcolor{myorange}{v_{qr}} 
    \end{pmatrix}
      + \mathbf{E}
\end{align*}
This way, we can have 
\begin{itemize}
    \item \myhl[myblue]{\textbf{Sparsity}}: selection of both \textbf{\underline{latent}} and \textbf{\underline{original}} variables
    \item \myhl[myblue]{\textbf{Low-rank SVD}}: different subsets of responses allowed to be associated with different subsets of predictors
\end{itemize}
Consider an example:
\begin{example}{Dimension Reduction and Variable Selection via Sparse SVD}{sparse_svd}
    Consider the case where $p=1000,q=100$, then $C$, as a $p\times q$ matrix, contains 100000 coefficients. Meanwhile, for a rank-3 SVD model:
    $$
     \mathbf{C} = d_1\mathbf{u}_1\mathbf{v}_1' + d_2\mathbf{u}_2\mathbf{v}_2' + d_3\mathbf{u}_3\mathbf{v}_3'
    $$
    where $\mathbf{u}_1,\mathbf{u}_2,\mathbf{u}_3$ are all $p\times 1$, $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ are all $q\times 1$, $d_1,d_2,d_3$ are all scalars. Hence, there are only $3\times (1000+100+1) = 3303$ paramaters to estimate. If futher assume sparcity, the dimension would be even lower.
\end{example}
Now let's develop a scalable procedure for this idea.

\section{Sparse Orthogonal Factor Regression}
Consider the sigular value decomposition of $\mathbf{C}$
$$
\mathbf{C} = \mathbf{UDV}'=\sum^r_{k=1}d_k\mathbf{u}_k\mathbf{v}_k'
$$
where $\mathbf{U}$ and $\mathbf{V}$ are both \myhl[myblue]{\textbf{orthonormal}}: $\mathbf{UU}' = \mathbf{VV}'=\mathbf{I}$. Then we can achieve dimension reduction via \textbf{low-dimensional latent model} $$ \tilde{\mathbf{Y}} = \tilde{\mathbf{X}}\mathbf{D}+\tilde{\mathbf{E}} $$ where 
\begin{itemize}
    \item $\tilde{\mathbf{Y}} = \mathbf{YV}$: $\mathbf{V}$ sparsity leads to \textbf{\underline{response}} variable selection
    \item $\tilde{\mathbf{X}} = \mathbf{XU}$: $\mathbf{U}$ sparsity leads to \textbf{\underline{predictor}} variable selection
\end{itemize}

How consider 
\begin{align}
    \left( \hat{\mathbf{D}},\hat{\mathbf{U}},\hat{\mathbf{V}} \right) &= \arg\min_{\mathbf{D,U,V}}\left\{ \frac{1}{2}\left\Vert \mathbf{Y-XUDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{UD}) + \lambda_b \rho_b (\mathbf{VD})\right\} & \text{s.t. } \mathbf{U'U}=\mathbf{V'V}=\mathbf{I}_m 
\end{align}
where 
\begin{itemize}
    \item $\rho_a(\cdot),\rho_b(\cdot)$ are penalty functions with regularization parameters $\lambda_d,\lambda_a,\lambda_b\geq 0$. These sparsity penalizations on $\mathbf{UD}$ and $\mathbf{VD}$ can be thought as \textbf{importance weighting}
    \item $\lVert \cdot \rVert _F$ is the nuclear norm, defined as the \textbf{sum} of its singular values $\lVert \mathbf{A}\rVert _F =\sum_i\sigma_i(\mathbf{A})$. It encourages sparsity among singular values and achieve \textbf{\underline{rank reduction}}
    \item The orthgonality on $\mathbf{U,V}$ allow a flexible form of sparsity-inducing penalties
\end{itemize}

If we further enrich this model by introducting an \myhl[myblue]{{adaptive weighting $\mathbf{W}$ matrices}}
\begin{align*}
    \left( \hat{\boldsymbol{\Theta}},\hat{\boldsymbol{\Omega}} \right) &= \arg\min_{\boldsymbol{\Theta,\Omega}}\left\{ \frac{1}{2}\left\Vert \mathbf{Y-XUDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{W}_d\circ \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{W}_a\circ\mathbf{A}) + \lambda_b \rho_b (\mathbf{W}_b\circ\mathbf{B})\right\}
\end{align*}
s.t. $\mathbf{U'U}=\mathbf{V'V}=\mathbf{I}_m$, $\mathbf{UD}=\mathbf{A}$, $\mathbf{VD}=\mathbf{B}$. But why? Singular values and singular vectors of \myhl[myblue]{\textbf{larger magnitude}} should be \myhl[myblue]{\textbf{less penalized}} to reduce bias and improve efficiency.


Two applications are
\begin{itemize}
    \item Biclustering with sparse SVD 
    \begin{align*}
        \left( \hat{\mathbf{D}},\hat{\mathbf{U}},\hat{\mathbf{V}} \right) &= \arg\min_{\mathbf{D,U,V}}\left\{ \frac{1}{2}\left\Vert \mathbf{X-UDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{UD}) + \lambda_b \rho_b (\mathbf{VD})\right\} & \text{s.t. } \mathbf{U'U}=\mathbf{V'V}=\mathbf{I}_m 
    \end{align*}
    \item Sparse PCA (sparsity in loadings of principla components)
    \begin{align*}
        \left( \hat{\mathbf{A}},\hat{\mathbf{V}} \right) &= \arg\min_{\mathbf{A,V}}\left\{ \frac{1}{2}\left\Vert \mathbf{X-XAV}' \right\Vert _F^2  + \lambda_a \rho_a (\mathbf{A}) \right\} & \text{s.t. } \mathbf{V'V}=\mathbf{I}_m 
    \end{align*}
\end{itemize}

\section{Nonasymptotic Properties of SOFAR}
First, define the robust spark for the regularity conditions
\begin{definition}{The robust spark $\kappa_c$}{robust_spark}
    The robust spark $\kappa_c$ of the $n\times p$ design matrix $\mathbf{X}$ is defined as the smallest possible positive integer such that there exists an $n\times \kappa_c$ submatrix of $\frac{1}{\sqrt{n}}\mathbf{X}$ having a \myhl[myred]{\textbf{singular value}} \textbf{less than} a given positive constant $c$
\end{definition}
The robust spark $\kappa_c$ here can be at least of order $O\left( \frac{n}{\log p} \right)$ with large probability for Gaussian design with dependency. With this definition, we characterize the following 5 conditions
\begin{itemize}
    \item \myhl[myblue]{\textbf{Parameter space}}: True parameters $(\mathbf{C}^*,\mathbf{D}^*,\mathbf{A}^*,\mathbf{B}^*)$ lie in $\mathcal{C}\times \mathcal{D}\times \mathcal{A}\times \mathcal{B}$, where 
    \begin{itemize}
        \item $\mathcal{C} = \left\{ \mathbf{C}\in \mathbb{R}^{p\times q}: \lVert \mathbf{C} \rVert _0<\kappa_{c_2}/2 \right\}$, with $\kappa_{c_2}$ being the robust spark of $\mathbf{X}$
        \item $\mathcal{D} = \left\{ \mathbf{D} = \mathrm{diag}\left\{d_j\right\}\in \mathbb{R}^{q\times q}: d_j=0 \text{ or }\lvert d_j \rvert \geq \tau \right\}$
        \item $\mathcal{A} = \left\{ \mathbf{A}=\left\{a_{ij}\right\}\in\mathbb{R}^{p\times q}:a_{ij}=0\text{ or }\lvert a_{ij} \rvert \geq \tau  \right\}$
        \item $\mathcal{B} = \left\{ \mathbf{B}=\left\{b_{ij}\right\}\in\mathbb{R}^{p\times q}:b_{ij}=0\text{ or }\lvert b_{ij} \rvert \geq \tau  \right\}$
    \end{itemize}
    $\tau >0$ asymptotically vanishing.
    \item \myhl[myblue]{\textbf{Constrained eigenvalue}}: It holds that for some constant $c_3 >0$
    \begin{align*}
        \max_{\lVert \mathbf{u} \rVert _0 <\frac{\kappa_{c_2}}{2}, \lVert \mathbf{u}\rVert _2=1}\lVert \mathbf{Xu}\rVert ^2_2 &\leq c_3 n, & \max_{1\leq j\leq r}\lVert \mathbf{Xu}^*_j\rVert ^2_2 &\leq c_3 n
    \end{align*}
    where $\mathbf{u}^*_j$ is the \textbf{\underline{left singular vector}} of $\mathbf{C}^*$ corresponding to singular value $d^*_j$
    \item \myhl[myblue]{\textbf{Error term}}: The error term $\mathbf{E}\in\mathbb{R}^{n\times q}\sim \mathcal{N}(\mathbf{0},\mathbf{I}\otimes \boldsymbol{\Sigma} )$ with the maximum eigenvalue $\alpha_{\max}$ of $\boldsymbol{\Sigma}$ bounded from above and diagonal entries of $\boldsymbol{\Sigma}$ being $\sigma^2_j$ 
    \item \myhl[myblue]{\textbf{Penalty functions}}: For matrices $\mathbf{M}$ and $\mathbf{M}^*$ of the same size, the penalty functions $\rho_h$ with $h\in \left\{a,b\right\}$ satisfies $$ \lvert \rho_h(\mathbf{M})-\rho_h(\mathbf{M}^*)\rvert \leq \lVert\mathbf{M}-\mathbf{M}^* \rVert _1 $$
    \item \myhl[myblue]{\textbf{Relative spectral gap}}: The nonzero singular values of $\mathbf{C}^*$ satisfy that $$ {d^*_{j-1}}^2-{d^*_{j}}^2\geq \sqrt{\delta}{d^*_{j-1}}^2,\ \ 2\leq j\leq r $$ with a constant $\delta >0$, both $r$ and $\sum^r_{j=1}\left( \frac{d^*_1}{d^*_j}\right)^2$ can diverge as $n\rightarrow \infty$
\end{itemize}
\paragraph{How to understand the 5 conditions?}
\begin{itemize}
    \item \myhl[myblue]{\textbf{Parameter space}} and \myhl[myblue]{\textbf{constrained eigenvalue}} are essential for investigating computable solution to non-convex SOFAR optimization problem 
    \item Gaussianity of \myhl[myblue]{error term} can be relaxed 
    \item \myhl[myblue]{\textbf{Penalty functions}} can be many kinds of \underline{sparsity-inducing} penalties, including entrywise $L_1$-norm\footnote{Entrywise $L_1$-norm encourages sparsity among predictor/response effects specific to each rank-1 SVD layer} and row-wise $(2,1)$-norm\footnote{$(2,1)$-norm is defined as the summation of absolute values of all components of a matrix. It promotes predictor/response-wise sparsity \textbf{regardless} of specific layer}
    \item \myhl[myblue]{\textbf{Relative spectral gap}} rules out non-identifiable case where some non-zero singular values are tied with each other and associated singular vectors in matrices $\mathbf{U}^*,\mathbf{V}^*$ are identifiable only up to some \textbf{\underline{orthogonal transformation}}
\end{itemize}

\section{Estimation: Convexity-Assisted Nonconvex Optimization}
Non-convexity of SOFAR objective function poses important algorithmic and theoretical challenges, hence consider a \myhl[myblue]{\textbf{two-step}} approach exploiting the framework of \underline{\textbf{convexity-assisted nonconvex optimization}} (CANO) to obtain SOFAR estimator:

\paragraph*{Step 1} minimize \myhl[myblue]{\textbf{$L_1$-penalized squared loss}} for multivariate regression to obtain an initial estimator
\begin{theorem}{Error Bounds for the Initial Estimator}{error_bound_step1}
    Under some regularity conditions, with large probability the initial estimator satisfies the following error bounds simultaneously:
    \begin{align*}
        \left\Vert \tilde{\mathbf{C}}-\mathbf{C}^* \right\Vert _F\leq R_n &\equiv c\sqrt{\frac{s \log (pq)}{n}} & \text{(A)}\\
        \left\Vert \tilde{\mathbf{D}}-\mathbf{D}^* \right\Vert _F &\leq c\sqrt{\frac{s \log (pq)}{n}} &\text{(B)}\\
        \left\Vert \tilde{\mathbf{A}}-\mathbf{A}^* \right\Vert _F + \left\Vert \tilde{\mathbf{B}}-\mathbf{B}^* \right\Vert _F &\leq c\eta_n \sqrt{\frac{s \log (pq)}{n}} & \text{(C)}
    \end{align*}
    where $c= \left\Vert \mathbf{C}^* \right\Vert _0$ and $\eta_n = 1+ \sqrt{\frac{\sum^r_{j=1}(d_1^*/d_j^*)^2}{\delta}} $
\end{theorem}
When $q=1$, bound (A) is consistent with the oracle inequality for Lasso. In this step, finer sparse SVD structure of coefficient matrix $\mathbf{C}^*$ is completely ignored, so intuitively, the second step should be able to improve error bounds.

\paragraph*{Step 2}  minimize SOFAR objective function in an \myhl[myblue]{\textbf{asymptotically shrinking neighborhood}} of initial estimator
\begin{theorem}{Nonasymptotic Error Bounds for SOFAR Estimator}{error_bound_step2}
    Under some regularity conditions, with large probability the SOFAR estimator satisfies the following error bounds simultaneously:
    \begin{align*}
        \left\Vert \tilde{\mathbf{C}}-\mathbf{C}^* \right\Vert _F &\leq c\sqrt{\min\left\{s,(r+s_a+s_b)\eta^2_n\right\}\cdot \frac{ \log (pq)}{n}} & \text{(a)}\\
        \left\Vert \tilde{\mathbf{D}}-\mathbf{D}^* \right\Vert _F + \left\Vert \tilde{\mathbf{A}}-\mathbf{A}^* \right\Vert _F + \left\Vert \tilde{\mathbf{B}}-\mathbf{B}^* \right\Vert _F &\leq c\eta_n\sqrt{ \min\left\{s,(r+s_a+s_b)\eta^2_n\right\} \cdot \frac{\log (pq)}{n}} &\text{(b)}
    \end{align*}
    and 
    \begin{align*}
        \left\Vert \tilde{\mathbf{D}}-\mathbf{D}^* \right\Vert _0 + \left\Vert \tilde{\mathbf{A}}-\mathbf{A}^* \right\Vert _0 + \left\Vert \tilde{\mathbf{B}}-\mathbf{B}^* \right\Vert _0 &\leq c (r+s_a+s_b) & \text{(c)}\\
        \left\Vert \tilde{\mathbf{D}}-\mathbf{D}^* \right\Vert _1 + \left\Vert \tilde{\mathbf{A}}-\mathbf{A}^* \right\Vert _1 + \left\Vert \tilde{\mathbf{B}}-\mathbf{B}^* \right\Vert _1 &\leq c(r+s_a+s_b) \eta_n^2\lambda_{\max} & \text{(d)}\\
        \frac{1}{n} \left\Vert \mathbf{X}\left(\hat{\mathbf{C}}-\mathbf{C}^*\right) \right\Vert ^2_F & \leq c(r+s_a+s_b)\eta_n^2\lambda_{\max}^2 & \text{(e)}
    \end{align*}
    where $r=\left\Vert \mathbf{D}^* \right\Vert _0$, $s_a = \left\Vert \mathbf{A}^* \right\Vert _0$, $s_b=\left\Vert \mathbf{B}^* \right\Vert _0$ and still $c= \left\Vert \mathbf{C}^* \right\Vert _0$ and $\eta_n = 1+ \sqrt{\frac{\sum^r_{j=1}(d_1^*/d_j^*)^2}{\delta}} $
\end{theorem}
here 
\begin{itemize}
    \item Bound (d) and (e) are the minimum of 2 rates
    \item $s$ (the sparsity of matrix $\mathbf{C}^*$) comes from the first step of Lasso estimation, $r+s_a+s_b$ (total sparsity of $\mathbf{D}^*,\mathbf{A}^*,\mathbf{B}^*$) comes from the second step of SOFAR refinement
    \item Under Frobenius norm, $s>(r+s_a +s_b)\eta_n^2$ gives that the two-step procedure enhances error rates
\end{itemize}
also,
\begin{itemize}
    \item In the case of univariate response with $q=1$, $\eta_n=1+\delta,r=1,s_a=s,s_b=1$, the upper bounds are then reduced to those for high-dimensional univariate response regressions
    \item In the case of rank-one $r=1$, $\eta_n = 1+\frac{1}{\sqrt{\delta}}$ and $s=s_as_b$, which leads to
    \begin{itemize}
        \item[-] SOFAR bounds: $c\sqrt{\frac{(s_a+s_b)\log(pq)}{n}}$, $c\sqrt{\frac{(s_a+s_b)\log(pq)}{n}}$, $c(s_a+s_b)$, $c(s_a+s_b)\sqrt{\frac{\log(pq)}{n}}$ and $\frac{c(s_a+s_b)\log(pq)}{n}$
        \item[-] Lasso bound (step 1): $c\sqrt{\frac{s_as_b\log(pq)}{n}}$
    \end{itemize}
    SOFAR estimator have much improved rates of covergence even in this case.
\end{itemize}

\section{Implementation with ALM-BCD}
Now consider estimation implementation. The idea is to use the \myhl[myblue]{\textbf{augmented Lagrangian method (ALM)}} coupled with \myhl[myblue]{\textbf{block coordinate descent (BCD)}}. The implementation procedure uses variable splitting to separate \underline{\textbf{orthogonality constraints}} and \underline{\textbf{sparsity-inducing penalties}} into different subproblems, enabling efficient optimization in a BCD fashion.

Consider $\boldsymbol{\Theta} = (\mathbf{D,U,V}),\boldsymbol{\Omega}=(\mathbf{A,B})$ in the optimization problem
$$
\min_{\boldsymbol{\Theta,\Omega}}\left\{ \frac{1}{2}\left\Vert \mathbf{Y-XUDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{A}) + \lambda_b \rho_b (\mathbf{B})\right\}
$$
s.t. $\mathbf{U'U}=\mathbf{V'V}=\mathbf{I}_m$, $\mathbf{UD}=\mathbf{A}$, $\mathbf{VD}=\mathbf{B}$. Then the \myhl[myblue]{\textbf{\underline{augmented Lagrangian}}} is
\begin{align*}
    \mathcal{L}_{\mu}\left( \boldsymbol{\Theta,\Omega,\Gamma} \right) =& \frac{1}{2} \left\Vert \mathbf{Y-XUDV}' \right\Vert _F^2 + \lambda_d \lVert \mathbf{D} \rVert _1 + \lambda_a \rho_a (\mathbf{A}) + \lambda_b \rho_b (\mathbf{B}) \\
    & + \left< \Gamma_a \mathbf{UD-A} \right> + \left< \Gamma_b \mathbf{VD-B} \right> + \frac{\mu}{2}\left\Vert \mathbf{UD-A} \right\Vert^2_F + \frac{\mu}{2}\left\Vert \mathbf{VD-B} \right\Vert^2_F 
\end{align*}
where $\boldsymbol{\Gamma} = \left( \boldsymbol{\Gamma}_a,\boldsymbol{\Gamma_b} \right)$. The procedure has 2 components 
\begin{itemize}
    \item $(\boldsymbol{\Theta,\Omega})$-step $$ \left(\boldsymbol{\Theta}^{k+1},\boldsymbol{\Omega}^{k+1}\right) \leftarrow \arg \min_{\boldsymbol{\Theta,\Omega}} \mathcal{L}_\mu(\boldsymbol{\Theta,\Omega},\boldsymbol{\Gamma}^k) $$ s.t. $\mathbf{U'U=V'V=I}_m$. This step embodies 
    \begin{itemize}
        \item block coordinate descent 
        \item orthogonal Procrustes problem 
        \item entrywise or block shoft thresholding
    \end{itemize}
    \item $\Gamma$-step 
    \begin{align*}
        \boldsymbol{\Gamma}^{k+1}_a &\leftarrow \boldsymbol{\Gamma}^k_a + \mu \left( \mathbf{U}^{k+1}\mathbf{D}^{k+1}-\mathbf{A}^{k+1} \right)\\
        \boldsymbol{\Gamma}^{k+1}_b &\leftarrow \boldsymbol{\Gamma}^k_b + \mu \left( \mathbf{V}^{k+1}\mathbf{D}^{k+1}-\mathbf{B}^{k+1} \right)
    \end{align*}
    where
    \begin{itemize}
        \item The penalty parameter $\mu$
    \end{itemize}
\end{itemize}


%\newpage
%\bibliographystyle{plainnat}
%\bibliography{ref.bib}

\end{document}