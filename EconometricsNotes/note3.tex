\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{url}
\usepackage{titlesec}
\setcounter{secnumdepth}{3}
\usepackage{palatino}
\usepackage{marginnote}
\usepackage{multirow}
\usepackage{easybmat,bigdelim,arydshln}
\usepackage[authoryear,round]{natbib}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable, theorems}
\usepackage{newpxtext,newpxmath}
\usepackage{longtable}
\usepackage{enumitem}
\makeatletter

\let\bar\overline

\setlist[itemize]{topsep=0pt,leftmargin=10pt,itemsep=-0.2em}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat = newest}
\usetikzlibrary{patterns,decorations.pathreplacing,decorations.markings}
\usepgfplotslibrary{fillbetween}

\hypersetup{
    colorlinks,
    citecolor=red,
    filecolor=black,
    linkcolor=violet,
    urlcolor=blue
}

\makeatletter
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\newtheoremstyle{indented}
  {3pt}% space before
  {3pt}% space after
  {\addtolength{\@totalleftmargin}{3.5em}
   \addtolength{\linewidth}{-3.5em}
   \parshape 1 3.5em \linewidth}% body font
  {}% indent
  {\bfseries}% header font
  {.}% punctuation
  {.5em}% after theorem header
  {}% header specification (empty for default)
\makeatother

\theoremstyle{definition}
\newtheorem{defin}{Definition}[section] % Creates a new counter, number within section
\newtheorem{prt}[defin]{Remark} 
\newtheorem{prts}[defin]{Remarks} % Again share defin's counter
\newtheorem{exmp}[defin]{Example} % etc.
\newtheorem{exmps}[defin]{Examples}
\newtheorem*{note}{Note}
\tcbuselibrary{theorems}

% use counter*=defin to make each tcbtheorem share defin's counter

\newtcbtheorem[use counter*=defin, number within=section]{definition}{Key takeaways}{enhanced, breakable,
    colback = white, colframe = red!55!black, colbacktitle = red!55!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{takeaway}

\newtcbtheorem[use counter*=defin, number within=section]{theorem}{Theorem}{enhanced, breakable,
    colback = white, colframe = blue!45!black, colbacktitle = blue!45!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{thm}
    
\newtcbtheorem[use counter*=defin, number within=section]{proposition}{Proposition}{enhanced, breakable,
    colback = white, colframe = teal, colbacktitle = teal, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{prop}

\newtcolorbox{example}[1]{enhanced, breakable, colback = white, colframe = orange!85!black, colbacktitle = orange!85!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries, title={Example: #1}}

\newtcbox{\myhl}[1][white]
  {on line, arc = 0pt, outer arc = 0pt,
    colback = #1!20!white, colframe = #1!50!black,
    boxsep = 0pt, left = 1pt, right = 1pt, top = 1pt, bottom = 1pt, boxrule = 0pt, bottomrule =0pt, toprule =0pt}
    
\newtcbox{\myhlrule}[1][white]
  {on line, arc = 0pt, outer arc = 0pt,
    colback = #1!20!white, colframe = #1!50!black,
    boxsep = 0pt, left = 1pt, right = 1pt, top = 1pt, bottom = 1pt, boxrule = 0pt, bottomrule =0.5pt, toprule =0.5pt}
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

\newcommand{\sidenotes}[1]{\marginnote{\raggedright\scriptsize#1}}
%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Econometrics
	\hfill \today} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Topic #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it #3 \hfill by #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Week #1: #2}{Week #1: #2}

   {\bf Key points}: {\begin{itemize}
    \item 
   \end{itemize}}

   {\bf Disclaimer}: {\it These notes are written by Sai Zhang (\href{mailto:saizhang.econ@gmail.com}{email me} or check my \href{https://github.com/SaiChrisZHANG}{Github page}). The main reference for this topic is \citet*{armstrong2020bias}, I thank Prof. Armstrong for his valuable advice.}
   \vspace*{4mm}
}
%

\tikzset{-stealth-/.style={decoration={
  markings,
  mark=at position #1 with {\arrow{stealth}}},postaction={decorate}}}

  \tikzset{tangent/.style={
    decoration={
        markings,% switch on markings
        mark=
            at position #1
            with
            {
                \coordinate (tangent point-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt,0pt);
                \coordinate (tangent unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (1,0pt);
                \coordinate (tangent orthogonal unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt,1);
            }
    },
    postaction=decorate
},
use tangent/.style={
    shift=(tangent point-#1),
    x=(tangent unit vector-#1),
    y=(tangent orthogonal unit vector-#1)
},
use tangent/.default=1}

\begin{document}
\lecture{3}{\textit{Moving the Goalposts} Approach}{}{Sai Zhang}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Finite Sample Bias-Variance Tradeoffs}
\subsection{Setup}
Consider the fixed design regression model
\begin{equation}
  y_i = w_i\beta\left(z_i\right) + h\left(z_i\right) + \epsilon_i
\end{equation}
where
\begin{itemize}
  \item $w_i,z_i$ are treated as \textbf{fixed}
  \item $\epsilon_i$ is {\textbf{independent}}, with $\mathbb{E}\left[\epsilon_i\right]=0,\mathbb{E}\left[\epsilon^2_i\right]=\sigma^2_i$
  \item observation: $\left\{ \left(y_i,w_i,z_i^{\prime}\right)^{\prime} \right\}^n_{i=1}$
\end{itemize}
one example is the case where $w_i$ is \myhl[blue!45!black]{\textbf{binary}}, then 
$$
\beta(z) = f(1,z)-f(0,z)
$$
which is just the ATE conditional on $z$ under the unconfoundedness assumption. This includes the RD design, where $z_i$ is the running variable and $w_i$ is the treatment assignment.

Now, consider for the weighted average treatment effect
$$
L_{\mu}\left[\beta(\cdot)\right] = \int \beta(z)\mathrm{d}\mu(z)
$$
where $\int\mu(z)=1$ is a \myhl[blue!45!black]{\textbf{signed}} measure (weight, allowing \textbf{negative} weights), construct a linear estimator
$$
\hat{L}_a = \sum^n_{i=1}a_iy_i
$$
where the estimation weights $a_i$ can depend on $\left\{ z_i,w_i,\sigma^2_i \right\}^n_{i=1}$, but \textbf{not} on $y_i$. Together, the bias of $\hat{L}_a$ for $L_{\mu}\left[\beta(\cdot)\right]$, \underline{given the regression function $\beta(\cdot),h(\cdot)$}, is 
$$
\mathbb{E}_{\beta(\cdot),h(\cdot)}\left[\hat{L}_a\right] - L_{\mu}\left[\beta(\cdot)\right] = \sum^n_{i=1}a_i\left[w_i\beta(z_i)+h(z_i)\right] - \int\beta(z)\mathrm{d}\mu(z)
$$
and its variance, \underline{given the regression function $\beta(\cdot),h(\cdot)$}, is just
$$
\mathrm{Var}_{\beta(\cdot),h(\cdot)}\left[\hat{L}_a\right] = \sum^n_{i=1}a_i^2\sigma^2_i
$$

To bound the bias, assume $h(\cdot)$ is known to belong in a class of functions $\mathcal{H}$, then two approaches can be adopted, for the regularity of $\beta(\cdot)$ and the choice of $\mu(\cdot)$:
\begin{itemize}
  \item[1] arbitrary $\beta(\cdot)$, optimizing weights $\mu$ by \textit{moving the goalposts}, s.t. $L_{\mu}\left[\beta(\cdot)\right]$ is easy to estimate \citep{crump2006moving,imbens2019optimized}
  which gives the worst-case bias
  \begin{align}\label{eq:worstbias1}
    \inf_{\mu} &\sup_{\beta(\cdot),h(\cdot)}\left\vert \sum^n_{i=1}a_i\left[w_i\beta(z_i)+h(z_i)\right] - \int\beta(z)\mathrm{d}\mu(z) \right\vert &\text{s.t. } & h(\cdot)\in\mathcal{H},\int \mathrm{d}\mu(z)=1
  \end{align}
  \item[2] assume constant treatment effects, i.e., $\beta(z)=\beta,\forall z$, which means that $L_{\mu}\left[\beta(\cdot)\right]=\beta$ regardless of $\mu$ \citep{armstrong2020bias}, and the worst-case bias is 
  \begin{align}\label{eq:worstbias2}
    \sup_{\beta,h(\cdot)}&\left\vert \sum^n_{i=1}a_i\left[w_i\beta + h(z_i)\right] -\beta \right\vert & \text{s.t. } & h(\cdot)\in\mathcal{H}
  \end{align}
\end{itemize}

And, the two approaches can be linked as such:
\begin{itemize}
  \item \underline{If $\sum^n_{i=1}a_iw_i=1$}, \ref{eq:worstbias1} and \ref{eq:worstbias2} are both equal to
  \begin{equation}\label{eq:equalboth}
    \sup_{h(\cdot)}\left\vert \sum^n_{i=1}a_ih(z_i) \right\vert\ \text{s.t. } h(\cdot)\in\mathcal{H}
  \end{equation}
  \begin{itemize}
    \item[-] \ref{eq:worstbias1} automatically equals \ref{eq:equalboth}
    \item[-] \ref{eq:worstbias2} is optimized (w.r.t. $\mu$) by setting $\mu$ to place weight $a_iw_i$ on observation $i$, i.e., $\mu(\mathcal{Z})=\sum_{i:z_i\in\mathcal{Z}}a_iw_i$, which implies $\sum^n_{i=1}a_iw_i\beta(z_i)-\int\beta(z)\mathrm{d}\mu(z)=0$, hence the equality.
  \end{itemize}

  \item Otherwise, \ref{eq:worstbias1} and \ref{eq:worstbias2} are both infinite:
  \begin{itemize}
    \item[-] \ref{eq:worstbias2} can be made arbitrarily large by choosing large enough $\beta$ 
    \item[-] \ref{eq:worstbias1} can be made arbitrarily large by making $\beta(\cdot)$ constant (as in \ref{eq:worstbias2}) and large enough
  \end{itemize}
\end{itemize}

\subsection{Moving-the-goalpost Approach}

\subsection{Constant-treatment-effect Approach}
\citet{armstrong2020bias} adopt this approach, focusing on the case where $h(\cdot)$ is a high dimensional linear function, and the penalty function is an $l_p$ norm of the coefficients.

\subsubsection*{Basic setting: Homoskedastic Gaussian errors}
First, consider
\begin{equation}
  Y= w \beta + Z\gamma +\epsilon
\end{equation}
where
\begin{itemize}
  \item[-] $\beta\in\mathbb{R}$ is the constant treatment effect to be estimated 
  \item[-] $\gamma \in \Gamma$ is the control coefficients, subject to the restriction (i.e., the function class $\mathcal{H}$)
  \begin{equation}
    \Gamma = \Gamma(C) = \left\{\gamma \in \mathcal{G}:\mathrm{Pen}(\gamma)\leq C \right\}
  \end{equation}
  where $\mathrm{Pen}(\cdot)$ is a seminorm\footnote{Seminorm satisfies
  \myhl[blue!45!black]{\textbf{triangle inequality} $\mathrm{Pen}\left(\gamma+\tilde{\gamma}\right)\leq \mathrm{Pen}(\gamma)$} and \myhl[blue!45!black]{\textbf{homogeneity} $\mathrm{Pen}\left(c\gamma\right)=\lvert c\rvert \mathrm{Pen}(\gamma),\forall c$}, but \textbf{NOT} necessarily positive definite ($\mathrm{Pen}(\gamma)=0$ does not imply $\gamma=0$). Essentially, any convex set $\Gamma$ that is symmetric satisfies this definition.
  } on some linear subspace $\mathcal{G}$ of $\mathbb{R}^k$.
  \item[-] $w= \left(w_1,\cdots,w_n\right)^{\prime}\in\mathbb{R}^n$ and $Z=\left(z_1^{\prime},\cdots,z_n^{\prime}\right)^{\prime}\in\mathbb{R}^{n\times k}$ are defined as before
  \item[-] $\epsilon \sim \mathcal{N}\left(0,\sigma^2 I_n\right)$ is assumed \myhl[blue!45!black]{\textbf{normal and homoskedastic}}, with $\sigma^2$ known
\end{itemize}

For estimation, the goal is to construct estimators and CIs for $\beta$:
\begin{itemize}
  \item estimator $\hat{\beta}$: consider the worst-case performance over the parameter space $\mathbb{R}\times\Gamma$ under the \textbf{MSE} criterion
  $$
  R_{MSE}\left(\hat{\beta};\Gamma\right)=\sup_{\beta\in\mathbb{R},\gamma\in\Gamma}\mathbb{E}_{\beta,\gamma}\left[\left(\hat{\beta}-\beta\right)^2\right]
  $$
  \item for CIs, we have 2 requirements:
  \begin{itemize}
    \item[A] \underline{\textbf{coverage}}: A $100\cdot(1-\alpha)\%$ CI with half-length $\hat{\chi}=\hat{\chi}(Y,X)$ is an interval $\left\{\hat{\beta}\pm \hat{\chi}\right\}$ s.t.$$ \inf_{\beta\in\mathbb{R},\gamma\in\Gamma} \mathrm{P}_{\beta,\gamma}\left(\beta\in\left\{\hat{\beta}\pm\hat{\chi}\right\}\right) \geq 1-\alpha $$
    \item[B] \underline{\textbf{length}}: the exepcted length of a CI $\mathbb{E}_{\beta,\gamma}\left[2\hat{\chi}\right]$ should be as short as possible
  \end{itemize}
  notice that length-optimized CIs are \textbf{not} necessarily centered at an MSE-centered $\hat{\beta}$.
\end{itemize}

\subsubsection*{Linear estimators and CIs}


\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}