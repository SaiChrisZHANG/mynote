\documentclass{article}
\setlength{\oddsidemargin}{0 in}
\setlength{\evensidemargin}{0 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\usepackage{url}
\usepackage{titlesec}
\setcounter{secnumdepth}{3}
\usepackage{palatino}
\usepackage{marginnote}
\usepackage{multirow}
\usepackage{easybmat,bigdelim,arydshln}
\usepackage[authoryear,round]{natbib}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{tcolorbox}
\tcbuselibrary{skins, breakable, theorems}
\usepackage{newpxtext,newpxmath}
\usepackage{longtable}
\usepackage{enumitem}
\makeatletter

\let\bar\overline

\setlist[itemize]{topsep=0pt,leftmargin=10pt,itemsep=-0.2em}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat = newest}
\usetikzlibrary{patterns,decorations.pathreplacing,decorations.markings}
\usepgfplotslibrary{fillbetween}

\hypersetup{
    colorlinks,
    citecolor=red,
    filecolor=black,
    linkcolor=violet,
    urlcolor=blue
}

\makeatletter
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\newtheoremstyle{indented}
  {3pt}% space before
  {3pt}% space after
  {\addtolength{\@totalleftmargin}{3.5em}
   \addtolength{\linewidth}{-3.5em}
   \parshape 1 3.5em \linewidth}% body font
  {}% indent
  {\bfseries}% header font
  {.}% punctuation
  {.5em}% after theorem header
  {}% header specification (empty for default)
\makeatother

\theoremstyle{definition}
\newtheorem{defin}{Definition}[section] % Creates a new counter, number within section
\newtheorem{prt}[defin]{Remark} 
\newtheorem{prts}[defin]{Remarks} % Again share defin's counter
\newtheorem{exmp}[defin]{Example} % etc.
\newtheorem{exmps}[defin]{Examples}
\newtheorem*{note}{Note}
\tcbuselibrary{theorems}

% use counter*=defin to make each tcbtheorem share defin's counter

\newtcbtheorem[use counter*=defin, number within=section]{definition}{Key takeaways}{enhanced, breakable,
    colback = white, colframe = red!55!black, colbacktitle = red!55!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{takeaway}

\newtcbtheorem[use counter*=defin, number within=section]{theorem}{Theorem}{enhanced, breakable,
    colback = white, colframe = blue!45!black, colbacktitle = blue!45!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{thm}
    
\newtcbtheorem[use counter*=defin, number within=section]{proposition}{Proposition}{enhanced, breakable,
    colback = white, colframe = teal, colbacktitle = teal, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{prop}

\newtcbtheorem[use counter*=defin, number within=section]{lemma}{Lemma}{enhanced, breakable,
    colback = white, colframe = orange!80!black, colbacktitle = orange!80!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries}{lemma}

\newtcolorbox{example}[1]{enhanced, breakable, colback = white, colframe = orange!85!black, colbacktitle = orange!85!black, attach boxed title to top left = {yshift = -2.5mm, xshift = 3mm}, boxed title style = {sharp corners},fonttitle=\bfseries, title={Example: #1}}

\newtcbox{\myhl}[1][white]
  {on line, arc = 0pt, outer arc = 0pt,
    colback = #1!20!white, colframe = #1!50!black,
    boxsep = 0pt, left = 1pt, right = 1pt, top = 1pt, bottom = 1pt, boxrule = 0pt, bottomrule =0pt, toprule =0pt}
    
\newtcbox{\myhlrule}[1][white]
  {on line, arc = 0pt, outer arc = 0pt,
    colback = #1!20!white, colframe = #1!50!black,
    boxsep = 0pt, left = 1pt, right = 1pt, top = 1pt, bottom = 1pt, boxrule = 0pt, bottomrule =0.5pt, toprule =0.5pt}
%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

\newcommand{\sidenotes}[1]{\marginnote{\raggedright\scriptsize#1}}
%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf Econometrics
	\hfill \today} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Topic #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it #3 \hfill by #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Week #1: #2}{Week #1: #2}

   {\bf Key points}: {\begin{itemize}
    \item 
   \end{itemize}}

   {\bf Disclaimer}: {\it These notes are written by Sai Zhang (\href{mailto:saizhang.econ@gmail.com}{email me} or check my \href{https://github.com/SaiChrisZHANG}{Github page}). The main references for this topic are \citet{armstrong2020bias,armstrong2018optimal}, I thank Prof. Armstrong for his valuable advice.}
   \vspace*{4mm}
}
%

\tikzset{-stealth-/.style={decoration={
  markings,
  mark=at position #1 with {\arrow{stealth}}},postaction={decorate}}}

  \tikzset{tangent/.style={
    decoration={
        markings,% switch on markings
        mark=
            at position #1
            with
            {
                \coordinate (tangent point-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt,0pt);
                \coordinate (tangent unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (1,0pt);
                \coordinate (tangent orthogonal unit vector-\pgfkeysvalueof{/pgf/decoration/mark info/sequence number}) at (0pt,1);
            }
    },
    postaction=decorate
},
use tangent/.style={
    shift=(tangent point-#1),
    x=(tangent unit vector-#1),
    y=(tangent orthogonal unit vector-#1)
},
use tangent/.default=1}

\begin{document}
\lecture{3}{\textit{Moving the Goalposts} Approach}{}{Sai Zhang}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Finite Sample Bias-Variance Tradeoffs}
\subsection{Setup}
Consider the fixed design regression model
\begin{equation}
  y_i = w_i\beta\left(z_i\right) + h\left(z_i\right) + \epsilon_i
\end{equation}
where
\begin{itemize}
  \item $w_i,z_i$ are treated as \textbf{fixed}
  \item $\epsilon_i$ is {\textbf{independent}}, with $\mathbb{E}\left[\epsilon_i\right]=0,\mathbb{E}\left[\epsilon^2_i\right]=\sigma^2_i$
  \item observation: $\left\{ \left(y_i,w_i,z_i^{\prime}\right)^{\prime} \right\}^n_{i=1}$
\end{itemize}
one example is the case where $w_i$ is \myhl[blue!45!black]{\textbf{binary}}, then 
$$
\beta(z) = f(1,z)-f(0,z)
$$
which is just the ATE conditional on $z$ under the unconfoundedness assumption. This includes the RD design, where $z_i$ is the running variable and $w_i$ is the treatment assignment.

Now, consider for the weighted average treatment effect
$$
L_{\mu}\left[\beta(\cdot)\right] = \int \beta(z)\mathrm{d}\mu(z)
$$
where $\int\mu(z)=1$ is a \myhl[blue!45!black]{\textbf{signed}} measure (weight, allowing \textbf{negative} weights), construct a linear estimator
$$
\hat{L}_a = \sum^n_{i=1}a_iy_i
$$
where the estimation weights $a_i$ can depend on $\left\{ z_i,w_i,\sigma^2_i \right\}^n_{i=1}$, but \textbf{not} on $y_i$. Together, the bias of $\hat{L}_a$ for $L_{\mu}\left[\beta(\cdot)\right]$, \underline{given the regression function $\beta(\cdot),h(\cdot)$}, is 
$$
\mathbb{E}_{\beta(\cdot),h(\cdot)}\left[\hat{L}_a\right] - L_{\mu}\left[\beta(\cdot)\right] = \sum^n_{i=1}a_i\left[w_i\beta(z_i)+h(z_i)\right] - \int\beta(z)\mathrm{d}\mu(z)
$$
and its variance, \underline{given the regression function $\beta(\cdot),h(\cdot)$}, is just
$$
\mathrm{Var}_{\beta(\cdot),h(\cdot)}\left[\hat{L}_a\right] = \sum^n_{i=1}a_i^2\sigma^2_i
$$

To bound the bias, assume $h(\cdot)$ is known to belong in a class of functions $\mathcal{H}$, then two approaches can be adopted, for the regularity of $\beta(\cdot)$ and the choice of $\mu(\cdot)$:
\begin{itemize}
  \item[1] arbitrary $\beta(\cdot)$, optimizing weights $\mu$ by \textit{moving the goalposts}, s.t. $L_{\mu}\left[\beta(\cdot)\right]$ is easy to estimate \citep{crump2006moving,imbens2019optimized}
  which gives the worst-case bias
  \begin{align}\label{eq:worstbias1}
    \inf_{\mu} &\sup_{\beta(\cdot),h(\cdot)}\left\vert \sum^n_{i=1}a_i\left[w_i\beta(z_i)+h(z_i)\right] - \int\beta(z)\mathrm{d}\mu(z) \right\vert &\text{s.t. } & h(\cdot)\in\mathcal{H},\int \mathrm{d}\mu(z)=1
  \end{align}
  \item[2] assume constant treatment effects, i.e., $\beta(z)=\beta,\forall z$, which means that $L_{\mu}\left[\beta(\cdot)\right]=\beta$ regardless of $\mu$ \citep{armstrong2020bias}, and the worst-case bias is 
  \begin{align}\label{eq:worstbias2}
    \sup_{\beta,h(\cdot)}&\left\vert \sum^n_{i=1}a_i\left[w_i\beta + h(z_i)\right] -\beta \right\vert & \text{s.t. } & h(\cdot)\in\mathcal{H}
  \end{align}
\end{itemize}

And, the two approaches can be linked as such:
\begin{itemize}
  \item \underline{If $\sum^n_{i=1}a_iw_i=1$}, \ref{eq:worstbias1} and \ref{eq:worstbias2} are both equal to
  \begin{equation}\label{eq:equalboth}
    \sup_{h(\cdot)}\left\vert \sum^n_{i=1}a_ih(z_i) \right\vert\ \text{s.t. } h(\cdot)\in\mathcal{H}
  \end{equation}
  \begin{itemize}
    \item[-] \ref{eq:worstbias1} automatically equals \ref{eq:equalboth}
    \item[-] \ref{eq:worstbias2} is optimized (w.r.t. $\mu$) by setting $\mu$ to place weight $a_iw_i$ on observation $i$, i.e., $\mu(\mathcal{Z})=\sum_{i:z_i\in\mathcal{Z}}a_iw_i$, which implies $\sum^n_{i=1}a_iw_i\beta(z_i)-\int\beta(z)\mathrm{d}\mu(z)=0$, hence the equality.
  \end{itemize}

  \item Otherwise, \ref{eq:worstbias1} and \ref{eq:worstbias2} are both infinite:
  \begin{itemize}
    \item[-] \ref{eq:worstbias2} can be made arbitrarily large by choosing large enough $\beta$ 
    \item[-] \ref{eq:worstbias1} can be made arbitrarily large by making $\beta(\cdot)$ constant (as in \ref{eq:worstbias2}) and large enough
  \end{itemize}
\end{itemize}

\subsection{Moving-the-goalpost Approach}

\subsection{Constant-treatment-effect Approach}
\citet{armstrong2020bias} adopt this approach, focusing on the case where $h(\cdot)$ is a high dimensional linear function, and the penalty function is an $l_p$ norm of the coefficients.

\subsubsection*{Basic setting: Homoskedastic Gaussian errors}
First, consider
\begin{equation}
  Y= w \beta + Z\gamma +\epsilon
\end{equation}
where
\begin{itemize}
  \item[-] $\beta\in\mathbb{R}$ is the constant treatment effect to be estimated 
  \item[-] $\gamma \in \Gamma$ is the control coefficients, subject to the restriction (i.e., the function class $\mathcal{H}$)
  \begin{equation}
    \Gamma = \Gamma(C) = \left\{\gamma \in \mathcal{G}:\mathrm{Pen}(\gamma)\leq C \right\}
  \end{equation}
  where $\mathrm{Pen}(\cdot)$ is a seminorm\footnote{Seminorm satisfies
  \myhl[blue!45!black]{\textbf{triangle inequality} $\mathrm{Pen}\left(\gamma+\tilde{\gamma}\right)\leq \mathrm{Pen}(\gamma)$} and \myhl[blue!45!black]{\textbf{homogeneity} $\mathrm{Pen}\left(c\gamma\right)=\lvert c\rvert \mathrm{Pen}(\gamma),\forall c$}, but \textbf{NOT} necessarily positive definite ($\mathrm{Pen}(\gamma)=0$ does not imply $\gamma=0$). Essentially, any convex set $\Gamma$ that is symmetric satisfies this definition.
  } on some linear subspace $\mathcal{G}$ of $\mathbb{R}^k$.
  \item[-] $w= \left(w_1,\cdots,w_n\right)^{\prime}\in\mathbb{R}^n$ and $Z=\left(z_1^{\prime},\cdots,z_n^{\prime}\right)^{\prime}\in\mathbb{R}^{n\times k}$ are defined as before
  \item[-] $\epsilon \sim \mathcal{N}\left(0,\sigma^2 I_n\right)$ is assumed \myhl[blue!45!black]{\textbf{normal and homoskedastic}}, with $\sigma^2$ known
\end{itemize}

For estimation, the goal is to construct estimators and CIs for $\beta$:
\begin{itemize}
  \item estimator $\hat{\beta}$: consider the worst-case performance over the parameter space $\mathbb{R}\times\Gamma$ under the \textbf{MSE} criterion
  $$
  R_{MSE}\left(\hat{\beta};\Gamma\right)=\sup_{\beta\in\mathbb{R},\gamma\in\Gamma}\mathbb{E}_{\beta,\gamma}\left[\left(\hat{\beta}-\beta\right)^2\right]
  $$
  \item for CIs, we have 2 requirements:
  \begin{itemize}
    \item[A] \underline{\textbf{coverage}}: A $100\cdot(1-\alpha)\%$ CI with half-length $\hat{\chi}=\hat{\chi}(Y,X)$ is an interval $\left\{\hat{\beta}\pm \hat{\chi}\right\}$ s.t.$$ \inf_{\beta\in\mathbb{R},\gamma\in\Gamma} \mathrm{P}_{\beta,\gamma}\left(\beta\in\left\{\hat{\beta}\pm\hat{\chi}\right\}\right) \geq 1-\alpha $$
    \item[B] \underline{\textbf{length}}: the exepcted length of a CI $\mathbb{E}_{\beta,\gamma}\left[2\hat{\chi}\right]$ should be as short as possible
  \end{itemize}
  notice that length-optimized CIs are \textbf{not} necessarily centered at an MSE-centered $\hat{\beta}$.
\end{itemize}

\subsubsection*{Linear estimators and CIs}
Again, consider estimators that are \textbf{linear} in the outcomes $Y$, $\hat{\beta}=a'Y$, where $a$ is the $n-$vector weights. In the vector form, the worst-case bias (as \ref{eq:worstbias2}) is 
\begin{equation}
  \bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right) = \sup_{\beta\in\mathbb{R},\gamma\in\Gamma} a'\left( w\beta + Z\gamma \right)-\beta
\end{equation}
and the variance, under the assumption of homoskedasticity, is $$ \mathrm{Var} \left(\hat{\beta}\right) = \sigma^2a'a $$
Then the MSE is 
$$
  R_{MSE}\left(\hat{\beta};\Gamma\right)=\sup_{\beta\in\mathbb{R},\gamma\in\Gamma}\mathbb{E}_{\beta,\gamma}\left[\left(\hat{\beta}-\beta\right)^2\right]=\bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right)^2 +\mathrm{Var}\left(\hat{\beta}\right)
$$
The $t-$statistic is $$ \frac{\hat{\beta}-\beta}{\sqrt{\mathrm{Var}\left(\hat{\beta}\right)}} \sim \mathcal{N}(b,1),\ \lvert b\rvert\leq \frac{\bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right)}{\sqrt{\mathrm{Var}\left(\hat{\beta}\right)}}$$
and a two-sided CI can then be formed as
\begin{align}
  \hat{\beta} &\pm \chi, &\text{where }\chi=\sqrt{\mathrm{Var}\left(\hat{\beta}\right)}\cdot \mathrm{cv}_{\alpha}\left( \frac{\bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right)}{\sqrt{\mathrm{Var}\left(\hat{\beta}\right)}} \right)
\end{align}
and the $\mathrm{cv}_{\alpha}(B)$ denotes the $1-\alpha$ quantile of a $\lvert \mathcal{N}(B,1) \rvert$. This is a \textbf{fixed-length confidence interval} (FLCI), with a fixed length of $2\chi$. It depends on $X$ and $\sigma^2$, but not on $Y$ or $(\beta,\gamma)^{\prime}$.

\subsubsection*{Optimal weights}
We have two optimization goals
\begin{itemize}
  \item minimizing MSE: $ R_{MSE}\left(\hat{\beta};\Gamma\right)=\bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right)^2 +\mathrm{Var}\left(\hat{\beta}\right)$
  \item minimizing CI length: $\chi=\sqrt{\mathrm{Var}\left(\hat{\beta}\right)}\cdot \mathrm{cv}_{\alpha}\left( \bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right)/\sqrt{\mathrm{Var}\left(\hat{\beta}\right)} \right)$
\end{itemize}
They both increasing in $\mathrm{Var}\left(\hat{\beta}\right)$ and $\bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}\right)$, hence to find the optimal weights, it suffices to minimize variance subject to a bound $B$ on worst-case bias, which can be written as:
\begin{equation}\label{eq:solve_optimalweight}
  \min_{a\in\mathbb{R}}a'a\ \text{s.t.}\ \sup_{\beta\in\mathbb{R},\gamma\in\Gamma}a'\left(w\beta+Z\gamma \right)-\beta \leq B
\end{equation}

The optimal weight is then given by:
\begin{theorem}{Optimal Weight}{optimal_weight}
  Let $\pi^*_{\lambda}$ be a solution to\footnote{This regression can be refered to as a regularized propensity score regression (but $w_i$ need not be binary) with penalty $\mathrm{Pen}(\pi)$}
  \begin{equation}\label{eq:penalizedreg}
    \min_{\pi}\lVert w-Z\pi \rVert^2_2 \ \text{ s.t. }\ \mathrm{Pen}(\pi)\leq t_{\lambda}
  \end{equation}
  and suppose that $\lVert w-Z\pi \rVert_2>0$, $\mathrm{Pen}(\cdot)$ is continuous, then the optimal weight solving \ref{eq:solve_optimalweight} is $$ a^*_{\lambda}=\frac{w-Z\pi_{\lambda}^*}{\left(w-Z\pi_{\lambda}^*\right)'w} $$
  with the bound $$ B=\frac{C}{t_{\lambda}}\cdot \frac{\left(w-Z\pi_{\lambda}^*\right)' Z\pi^*_{\lambda}}{\left(w-Z\pi_{\lambda}^*\right)'w} $$
  Consequently, we have 
  \begin{itemize}
    \item estimator $$ \hat{\beta}_{\lambda} = {a^*_{\lambda}}' Y= \frac{\left( w-Z\pi_{\lambda}^* \right)'Y}{\left(w-Z\pi_{\lambda}^*\right)'w} $$
    \item worst-case bias $$ \bar{\mathrm{bias}}_{\Gamma}\left(\hat{\beta}_{\lambda}\right) = C\bar{B}_{\lambda} = \frac{C}{\mathrm{Pen}\left(\pi^*_{\lambda}\right)}  \frac{\left( w-Z\pi_{\lambda}^* \right)'Z\pi^*_{\lambda}}{\left(w-Z\pi_{\lambda}^*\right)'w}$$
    \item variance of estimator $$ V_{\lambda} = \frac{\sigma^2 \lVert w-Z\pi_{\lambda}^* \rVert^2_2}{\left[ \left(w-Z\pi_{\lambda}^*\right)'w \right]^2} $$ 
  \end{itemize}
\end{theorem}

This result follows by applying \citet{donoho1994statistical}, \citet{low1995bias} and \citet{armstrong2018optimal}, rewriting \ref{eq:solve_optimalweight} as a convex optimization problem.

\newpage
\appendix
\section{Proofs}
\subsection{Proof of Theorem \ref{thm:optimal_weight}}
Following \citet[Equation (25)]{armstrong2018optimal}, the modulus of continuity is given by 
\begin{align*}
  \omega(\delta) &=\sup_{\beta,\gamma}2\beta &\text{s.t.\ }&\lVert w\beta+Z\gamma \rVert_2 \leq \frac{\delta}{2}, &\mathrm{Pen}(\gamma) &\leq C
\end{align*}
Introducing a substitution (rescaling $\gamma$ by $\beta$) $\pi = -\frac{\gamma}{\beta}$, get 
\begin{align}\label{eq:continuity_modulus}
  \omega(\delta) &=\sup_{\beta,\pi}2\beta &\text{s.t.\ }&\beta\lVert w-Z\pi \rVert_2 \leq \frac{\delta}{2}, &\beta\mathrm{Pen}(\pi) &\leq C
\end{align}
recall the optimization problem in Theorem \ref{thm:optimal_weight}:
$$ \min_{\pi}\lVert w-Z\pi \rVert^2_2 \ \text{ s.t. }\ \mathrm{Pen}(\pi)\leq t_{\lambda} $$

We can relate the two problems via the following logic: we want to make $\lVert w-Z\pi \rVert_2$ and $\mathrm{Pen}(\pi)$ small so that large values of $\beta$ satisfy the constraint of \ref{eq:continuity_modulus}. Formally:
\begin{lemma}{}{A1}
  \begin{itemize}
    \item If $\exists \pi \in\mathcal{G}$ s.t. $w=Z\pi$ and $\mathrm{Pen}(\pi)=0$, then $w(\delta)=\infty,\forall \delta\geq 0$ \hfill (automatic)
    \item \textbf{\underline{Otherwise}}:
    \begin{itemize}
      \item[(i)] $\forall \delta>0$, the problem \ref{eq:continuity_modulus} has a solution $\beta^{mod}_{\delta},\pi^{mod}_{\delta}$ with $\beta^{mod}_{\delta}>0$. For $t_{\lambda}=\frac{C}{\beta^{mod}_{\delta}}=\frac{2C}{w(\delta)}$, $\pi^{mod}_{\delta}$ is also a solution to the penalized regression (\ref{eq:penalizedreg})
      $$ \min_{\pi}\lVert w-Z\pi \rVert^2_2 \ \text{ s.t. }\ \mathrm{Pen}(\pi)\leq t_{\lambda} $$
      with optimized objective 
      $$ \lVert w-Z\pi \rVert_2 = \frac{\delta}{2\beta_{\delta}^{mod}}=\frac{\delta}{w(\delta)} >0$$

      \item[(ii)] $\forall t_{\lambda}>0$, the penalized regression above has a solution $\pi^{\star}_{\lambda}$. Setting
      \begin{align*}
        \beta^*_{\lambda} &=\frac{C}{t_{\lambda}} \\
        \delta_{\lambda} &=2\beta^*_{\lambda}\lVert w-Z\pi \rVert_2 = \frac{2C}{t_{\lambda}}\lVert w-Z\pi \rVert_2
      \end{align*}
      the pair $\left(\beta^*_{\lambda},\pi^*_{\lambda}\right)$ solves the modulus problem \ref{eq:continuity_modulus} at $\delta=\delta_{\lambda}$, with optimized objective $w(\delta_{\lambda})=\frac{2C}{t_{\lambda}}$, as long as $\lVert w-Z\pi \rVert_2 >0$
    \end{itemize}
  \end{itemize}
\end{lemma}

\underline{\textbf{Proof of Lemma \ref{lemma:A1}}}: we prove the lemma with the following steps:
\begin{itemize}
  \item[A] \underline{the penalized problem \ref{eq:penalizedreg} has a solution}: 
  
  Let $\mathcal{G}^{(0)}$ denote the linear subspace of vectors $\pi \in \mathcal{G}$ s.t. $Z\pi=0$, $\mathrm{Pen}(\pi)=0$; let $\mathcal{G}^{(1)}$ be a subspace s.t. $\mathcal{G}=\mathcal{G}^{(0)} \oplus \mathcal{G}^{(1)}$. Then, we can write $\pi \in \mathcal{G}$ uniquely as $\pi = \pi^{(0)}+\pi^{(1)}$ where $\pi^{(0)} \in\mathcal{G}^{(0)},\pi^{(1)}\in\mathcal{G}^{(1)}$. Therefore, we have $Z\pi = Z\pi^{(1)}$, and 
  $$
  \begin{aligned}
    \mathrm{Pen}(\pi^{(1)}) &= \mathrm{Pen}(\pi^{(1)}) - \mathrm{Pen}(-\pi^{(0)}) \leq \mathrm{Pen}(\pi)\\
      \mathrm{Pen}(\pi^{(1)}) &= \mathrm{Pen}(\pi^{(1)}) + \mathrm{Pen}(\pi^{(0)}) \geq \mathrm{Pen}(\pi)
  \end{aligned} \Rightarrow \mathrm{Pen}(\pi^{(1)}) = \mathrm{Pen}(\pi)
  $$
  THen, \ref{eq:penalizedreg} can be wriiten in terms of $\pi^{(1)}\in\mathcal{G}^{(1)}$ only. The level sets of this optimization problem are bounded and closed (by continuity of the seminorm $\mathrm{Pen}(\cdot)$), so it has a solution, which is also the solution to the original problem.
  \item[B] \underline{the modulus problem \ref{eq:continuity_modulus} has a solution}: for the problem \ref{eq:continuity_modulus}, feasible values of $\beta$ are bounded as:
  \begin{align*}
    \beta &\leq \frac{\delta}{2}\cdot \frac{1}{\lVert w-Z\pi \rVert_2} & \beta \leq C \cdot\frac{1}{\mathrm{Pen}(\pi)}
  \end{align*}
  i.e., $\beta$ is bounded by the inverse of the minimum of $\max \{ \lVert w-Z\pi \rVert_2,\mathrm{Pen}(\pi) \}$ over $\pi$, and it is strictly positive. Hence, $\beta,\tilde{\pi}^{(1)}$ can be restricted to a compact set without changing the optimization problem.
  \item[C] \underline{proof of statement (i)}: Proof by contradiction, if it's not true, then $\exists \tilde{\pi}$ s.t.
  \begin{align*}
    \mathrm{Pen}(\tilde{\pi})&\leq \frac{C}{\beta_{\delta}^{mod}}\equiv t_{\lambda}, & \lVert w-Z\tilde{\pi} \rVert_2 \leq \lVert w-Z \pi_{\delta}^{mod} \rVert_2 - \nu
  \end{align*}
  then for some $\eta$, let $\tilde{\pi}_{\eta}=(1-\eta)\tilde{\pi}$, we have
  \begin{align*}
    \lVert w-Z\tilde{\pi}_{\eta} \rVert_2 & = \lVert w- Z(1-\eta)\tilde{\pi} \rVert_2\\
    & \leq \lVert w-X\tilde{\pi} \rVert_2 + \eta\lVert Z\tilde{\pi} \rVert_2 \\
    &\leq \lVert w -X\pi_{\delta}^{mod} \rVert_2 -v +\eta \lVert Z\tilde{\pi}\rVert_2\\
    &\leq \frac{\delta}{2\beta_{\delta}^{mod}}-v+\eta \lVert Z\tilde{\pi} \rVert_2
  \end{align*}
  Hence, $\exists \eta$ small enough, s.t.
  \begin{align*}
    \lVert w-Z\tilde{\pi}_{\eta} \rVert_2 & < \frac{\delta}{2\beta^{mod}_{\delta}}, &\mathrm{Pen}(\tilde{\pi}_{\eta})\leq (1-\eta)\frac{C}{\beta_{\delta}^{mod}}< \frac{C}{\beta_{\delta}^{mod}}
  \end{align*}
   therefore, by setting $\pi=\tilde{\pi}_{\eta}$, we can allow a strictly bigger $\beta$, which is a contradiction.
  \item[D] \underline{proof of statement (ii)}: This result follows immediately.
\end{itemize}

Next, use Lemma \ref{lemma:A1} to prove Theorem \ref{thm:optimal_weight}. Following \citet{armstrong2018optimal}, the class of bias-variance optimizing estimators is 
$$
\frac{\left(w \beta^{mod}_{\delta}+Z\gamma^{mod}_{\delta}\right)'Y}{\left(w \beta^{mod}_{\delta}+Z\gamma^{mod}_{\delta}\right)'w}
$$
This is given by the \textbf{centrosymmetry} of the smooth function.

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}