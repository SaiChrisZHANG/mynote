\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{5}{Two-Way Cluster-Robust (TWCR) Standard Errors}{}{Sai Zhang}{The validity of Two-Way Cluster-Robust (TWCR) standard errors}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{One-Way Clustering}
First, consider the case of one-way clustering. The linear model with one-way clustering $$ y_{ig} = \mathbf{x}_{ig}\boldsymbol{\beta} + u_{ig} $$
where $i$ denotes the $i$th of the $N$ individuals in the sample, $j$ denotes the $g$th of the $G$ clusters, assume that
\begin{itemize}
    \item $\mathbb{E}\left[u_{ig}\mid \mathbf{x}_{ig}\right] =0$
    \item error independence across clusters: for $i\neq j$
    \begin{equation}\label{eq:error_independence}
        \mathbb{E}\left[ u_{ig} u_{jg'}\mid \mathbf{x}_{ig},\mathbf{x}_{jg'} \right] = 0
    \end{equation}
    unless $g=g'$, that is, errors for individuals within the same cluster may be correlated.
\end{itemize}
Grouping observations by cluster, get
$$
\mathbf{y}_g = \mathbf{X}_g \boldsymbol{\beta} + \mathbf{u}
$$
where $\mathbf{X}_g$ has dimension $N_g\times K$ and $\mathbf{y}_g$ has dimension $N_g \times 1$, with $N_g$ observations in cluster $g$. 
Stacking over cluster, get the matrix form of the model
$$
\mathbf{y=X}\boldsymbol{\beta}+\mathbf{u}
$$
with $\mathbf{y,u}$ being $N\times 1$ vectors, $\mathbf{X}$ being an $N\times K$ matrix. OLS estimator gives 
\begin{equation}\label{eq:OLSest}
    \hat{\boldsymbol{\beta}} = \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}=\left( \sum^G_{g=1}\mathbf{X}_g'\mathbf{X}_g \right)^{-1} \sum^G_{g=1}\mathbf{X}'_g\mathbf{y}_g
\end{equation}
then, by CLT, we have that $\sqrt{G} \left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right) \xrightarrow{d} \mathcal{N}(0,\boldsymbol{\Sigma})$ where the variance matrix of the limit normal distribution $\boldsymbol{\Sigma}$ is 
\begin{equation}\label{eq:limit_varcovmat}
    \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right] \right)^{-1} \left(\lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{u}'_g\mathbf{u}_g\mathbf{X}_g\right] \right) \times \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right]  \right)^{-1}
\end{equation}
If the primary source of clustering is due to group-level common shocks, a useful approximation is that for the $j$th regressor, the default OLS variance estimate based on $s^2 \left(\mathbf{X'X}\right)^{-1}$ should be inflated by $\tau_j \simeq 1+\rho_{x_j}\rho_u\left(\bar{N}_g -1\right)$, where 
\begin{itemize}
    \item $s$ is the estimated standard deviation of the error
    \item $\rho_{x_j}$ is a measure of within-cluster correlation of $x_j$
    \item $\rho_u$ is the within-cluster error correlation 
    \item $\bar{N}_g$ is the average cluster size
\end{itemize}
It's easy to see the $\tau_j$ can be large even with small $\rho_u$ \citep{kloek1981ols,scott1982effect,moulton1990illustration}. If assume the model for the cluster error variance matrices $\boldsymbol{\Omega}_g = \mathbb{V}\left[\mathbf{u}_g \mid \mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{u}_g\mathbf{u}_g'\mid \mathbf{X}_g\right]$, 
and there is a consistent estimate $\hat{\boldsymbol{\Omega}}_g$ of $\boldsymbol{\Omega}_g$, we can estimate $\mathbb{E}\left[\mathbf{X}_g'\mathbf{u}_g\mathbf{u}_g'\mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{X}_g'\boldsymbol{\Omega}_g \mathbf{X}_g\right]$ via GLS.

\paragraph*{Cluster-robust variance matrix estimate} consider 
\begin{equation}\label{eq:oneway_clurob}
    \hat{\mathbb{V}} \left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\left(\sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g \right) \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where $\hat{\mathbf{u}}_g = \mathbf{y}_g - \mathbf{X}_g\hat{\boldsymbol{\beta}}$. This estimate is consistent if $$ G^{-1}\sum^G_{g=1}\mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g - G^{-1}\sum^G_{g=1}\mathbb{E}\left[ \mathbf{X}_g' \mathbf{u}_g \mathbf{u}_g' \mathbf{X}_g \right] \xrightarrow{\mathrm{p}} \mathbf{0} $$ as $G\rightarrow \infty$. 
An informal presentation of Eq.(\ref{eq:oneway_clurob}) is to rewrite the central matrix as 
\begin{equation}\label{eq:onewayclu_centralmat}
    \hat{\mathbf{B}} = \sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g = \mathbf{X}'\begin{bmatrix}
        \hat{\mathbf{u}}_1\hat{\mathbf{u}}_1' & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \hat{\mathbf{u}}_2\hat{\mathbf{u}}_2' & & \vdots \\
        \vdots & & \ddots & \mathbf{0} \\
        \mathbf{0} & \cdots & & \hat{\mathbf{u}}_G\hat{\mathbf{u}}_G'
    \end{bmatrix}\mathbf{X} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right) \mathbf{X} 
\end{equation}
where $\otimes$ denotes element-wise multiplication. The $(p,q)$th element of this matrix is 
\begin{equation*}
    \sum^N_{i=1}\sum^N_{j=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1}\left(i,j\text{ in the same cluster}\right)
\end{equation*}
with $\hat{u}_i = y_i - \mathbf{x}'_i \hat{\boldsymbol{\beta}}$.

$\mathbf{S}^G$ is an $N\times N$ indicator matrix with $\mathbf{S}_{ij}^G=1$ only if the $i$th and $j$th observation belong to the same cluster: it zeros out a large amount of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ (asymptotically equivalently, ${\mathbf{u}}{\mathbf{u}}'$), specifically, only $\sum^G_{g=1}N_g^2$ out of $N^2 = \left(\sum^G_{g=1}N_g\right)^2$ terms are not zero (sub-matrices on the diagonal). Asymptotically
\begin{itemize}
    \item for fixed $N_g$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g}\xrightarrow{G\rightarrow\infty}0$
    \item for balanced clusters $N_g = N/G$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g} = \frac{1}{G} \xrightarrow{G\rightarrow\infty}0$
\end{itemize}


A strand of literature popularizes 
this method:
\begin{itemize}
    \item \citet{liang1986longitudinal}: in a generalized estimatin equations setting
    \item \citet{arellano1987computing}: fixed effects estimator in linear panel models
    \item \citet{hansen2007asymptotic}: asymptotic theory for panel data where $T\rightarrow\infty$ in addition to $N\rightarrow\infty$ (or $N_g\rightarrow\infty$ in addition to $G\rightarrow\infty$ in the notation above).
\end{itemize}

\section{Two-Way Clustering}
Now, consider the case of two-way clustering, 
$$
y_{i,gh} = \mathbf{x}'_{i,gh}\boldsymbol{\beta} + u
$$
where each observation may belong to \textbf{two} dimension of groups: group $g\in \left\{1,\cdots,G\right\}$ and $h\in \left\{ 1,\cdots,H \right\}$, and for $i\neq j$
\begin{equation}\label{eq:twoway_errors}
    \mathbb{E} \left[ u_{i,gh} u_{j,g'h'} \mid \mathbf{x}_{i,gh},\mathbf{j,g'h'} \right] = 0
\end{equation}
unless $g=g'$ or $h=h'$, that is, errors for individuals within the same group (along either $g$ or $h$) may be correlated.

\paragraph*{Cluster-robust variance matrix estimate} extending the one-way clustering case, keep elements of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ where the $i$th and $j$th observations share a cluster in \myhl[myblue]{\textbf{any}} dimension, then similar to Eq.(\ref{eq:onewayclu_centralmat})
\begin{equation}\label{eq:twowayclu_centermat}
    \hat{\mathbf{B}} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH}\right)\mathbf{X}
\end{equation}
here $\mathbf{S}^{GH}$ is an $N\times N$ indicator matrix with $\mathbf{S}^{GH}_{ij}=1$ only if the $i$th and $j$th observation share any cluster, the $(p,q)$th element of this matrix is 
$$
\sum^N_{i=1}\sum^N_{i=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1} \left(i,j\text{ share any cluster}\right)
$$
$\hat{\mathbf{B}}$ can also be presented in one-way cluster-robust fashion:
\begin{align}\label{eq:twowayclu_centmat}
    \hat{\mathbf{B}} &= \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH} \right)\mathbf{X} = \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right)\mathbf{X} + \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^H \right)\mathbf{X} - \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{G\cap H} \right)\mathbf{X}
\end{align}
where $\mathbf{G}^{GH} =\mathbf{G}^G+ \mathbf{G}^H - \mathbf{G}^{G\cap H} $, with 
\begin{itemize}
    \item $\mathbf{G}^G$: $\mathbf{G}^G_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $g\in \left\{1,2,\cdots,G\right\}$
    \item $\mathbf{G}^H$: $\mathbf{G}^H_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $h\in \left\{1,2,\cdots,H\right\}$
    \item $\mathbf{G}^{G\cap H}$: $\mathbf{G}^{G\cap H}_{ij}=1$ only if the $i$th and $j$th observation belong to \textbf{both} the same cluster $g\in \left\{1,2,\cdots,G\right\}$ and the same cluster $h\in \left\{1,2,\cdots,H\right\}$
\end{itemize}
then, similar to one-way clustering case,
\begin{align}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] =& \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^G\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &+ \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^H\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &- \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^{G\cap H}\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1}
\end{align}
that is,
\begin{equation}\label{eq:twowayclu_decomp}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \hat{\mathbb{V}}^G\left[\hat{\boldsymbol{\beta}}\right] + \hat{\mathbb{V}}^H\left[\hat{\boldsymbol{\beta}}\right] - \hat{\mathbb{V}}^{G\cap H}\left[\hat{\boldsymbol{\beta}}\right]
\end{equation}
each of Eq.(\ref{eq:twowayclu_decomp}) can be separately computed by OLS of $\mathbf{y}$ on $\mathbf{X}$, with variance matrix estimates $\hat{\mathbb{V}}$ based on 
\begin{itemize}
    \item[i] clustering on $g\in \left\{1,2,\cdots, G\right\}$
    \item[ii] clustering on $h \in \left\{1,2,\cdots, H\right\}$
    \item[iii] clustering on $(g,h)\in \left\{(1,1),\cdots,(G,H)\right\}$
\end{itemize}

\paragraph*{Practical considerations} It is required to know what \textit{ways} will be potentially important for clustering, which can be tested via checking the dimension of correlations in the errors. There are several ways to test 
\begin{itemize}
    \item estimate sample covariances of $\mathbf{X}'\hat{\mathbf{u}}$ within dimensions, test the null that the \myhl[myblue]{\textbf{average}} of such covariances is 0: rejecting this null is sufficient (not necessary) to reject the null of no clustering \citep{white1980heteroskedasticity}
    \item for \myhl[myblue]{\textbf{small samples}}, Eq. (\ref{eq:oneway_clurob}) is baised downwards. This is corrected (in Stata) by replacing $\hat{\mathbf{u}}_g$ with $\sqrt{c}\hat{\mathbf{u}}_g$, where $c = \frac{G}{G-1}\frac{N-1}{N-K}\simeq \frac{G}{G-1}$. For two-way clustering (Eq. \ref{eq:twowayclu_centmat}), there are 2 ways of correction:
    \begin{itemize}
        \item choose correction terms for each of the 3 components: $$c_1= \frac{G}{G-1}\frac{N-1}{N-K}, c_2= \frac{H}{H-1}\frac{N-1}{N-K},c_3=\frac{I}{I-1}\frac{N-1}{N-K}$$ with $I$ being the number of unique clusters determined by $G\cap H$
        \item choose a constant terms for all components: $$c=\frac{J}{J-1}\frac{N-1}{N-K}$$ with $J=\min(G,H)$
    \end{itemize}
    \item \myhl[myblue]{\textbf{Var-cov matrix not positive-semidefinite}}: $\hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right]$ might have negative elements on the diagonal (Eq. \ref{eq:twowayclu_decomp}), informly, this is more likely to  arise when clustering is done over the same groups as the fixed effects. One way to address this issue is using \textit{eigendecomposition} technique:
    $$
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}'
    $$
    where 
    \begin{itemize}
        \item $\mathbf{U}$ containing the eigenvectors of $\hat{\mathbf{V}}$
        \item $\boldsymbol{\Lambda} = \mathrm{diag}\left[\lambda_1,\cdots,\lambda_d\right]$ contains the eigenvalues of $\hat{\mathbf{V}}$ 
    \end{itemize}
    then create $\boldsymbol{\Lambda}^+ = \mathrm{diag} \left[\lambda_1^+,\cdots,\lambda_d^+\right]$ with $\lambda^+_j=\max\left(0,\lambda_j\right)$ and use $\hat{\mathbf{V}}^+\left[\hat{\boldsymbol{\beta}}\right]=\mathbf{U}\boldsymbol{\Lambda}^+\mathbf{U}'$ as the estimate
\end{itemize}

\section{Multiway Clustering}
\citet{cameron2011robust} extended the framework\footnote{Also proposed by \citet{thompson2011simple}.} to allow clustering in $D$ dimensions, then we can do the following reframing
\begin{itemize}
    \item $G_d$: the number of clusters in dimension $d\in \left\{ 1,2,\cdots,D \right\}$
    \item $D-$vector $\boldsymbol{\delta}_i = \boldsymbol{\delta}(i)$, with funciton $\boldsymbol{\delta}:\left\{1,2,\cdots,N\right\}\rightarrow \bigtimes ^D_{d=1}\left\{1,2,\cdots, G_d\right\}$ lists the cluster membership in each dimension of each observation
\end{itemize}
then we have 
$$
\mathbf{1}\left[i,j\text{ shares a clustser}\right] = 1 \Leftrightarrow \delta_{id}=\delta_{jd}
$$
for some $d\in \left\{1,2, \cdots, D\right\}$, where $\delta_{id}$ denotes the $d$th element of $\boldsymbol{\delta}_i$. Also 
\begin{itemize}
    \item $D-$vector $\mathbf{r}$: define the set $$ R\equiv \left\{ \mathbf{r}:r_d\in\left\{0,1\right\},d=1,2,\cdots,D,\mathbf{r\neq 0} \right\} $$
    elements of the set $R$ can be used to index all cases where 2 observations share a cluster in at least one dimension. Define the function 
    $$
    \mathbf{I_r} (i,j) \equiv \mathbf{1} \left[r_d\delta_{id} = r_d \delta_{jd},\forall d\right]
    $$
    which indicates whether observations $i$ and $j$ have identical cluster menbership for \myhl[myblue]{\textbf{all}} dimensions $d$ s.t. $r_d=1$.
    Then we have a \textit{aggregate} identifier
    $$
    \mathbf{I}(i,j) = 1 \Leftrightarrow \mathbf{I_r}(i,j)=1\text{ for some }\mathbf{r}\in R
    $$
    i.e., 2 observations share \myhl[myblue]{\textbf{at least}} one dimension.
\end{itemize}
The define the $2^D-1$ matrices
\begin{equation}\label{eq:centermat_multiclu}
    \tilde{\mathbf{B}}_{\mathbf{r}}\equiv \sum^N_{i=1}\sum^N_{j=1}\mathbf{x}_i\mathbf{x}_j' \hat{u}_i\hat{u}_j\mathbf{I_r}(i,j)
\end{equation}
with $\mathbf{r}\in R$.

\paragraph*{Var-cov matrix estimator} consider, similarly, an estimator 
\begin{equation}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\tilde{\mathbf{B}} \left(\mathbf{X'X}\right)^{-1} \equiv  \left(\mathbf{X'X}\right)^{-1}  \left(\sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r \right)  \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where cases of clustering on an odd number of dimensions are added, those of clustering on an even number of dimensions are subtracted. Consider the case of $D=3$,
\begin{equation*}
    \left(\tilde{\mathbf{B}}_{(1,0,0)} + \tilde{\mathbf{B}}_{(0,1,0)} + \tilde{\mathbf{B}}_{(0,0,1)} \right) - \left( \tilde{\mathbf{B}}_{(1,1,0)} + \tilde{\mathbf{B}}_{(1,0,1)} + \tilde{\mathbf{B}}_{(0,1,1)} \right) + \tilde{\mathbf{B}}_{(1,1,1)}
\end{equation*}
$\tilde{\mathbf{B}}$ is identical to $\hat{\mathbf{B}}$ defined analogically as in Eq.(\ref{eq:twowayclu_centmat}), since 
\begin{itemize}
    \item no observation pair with $\mathbf{I}(i,j)=0$: this is immediate, since $\mathbf{I}(i,j)=0 \Leftrightarrow \mathbf{I_r}(i,j)=0,\forall \mathbf{r}$
    \item the covariance term corresponding to each observation pair with $\mathbf{I}(i,j)=1$ is included \myhl[myblue]{\textbf{exactly once}} in $\tilde{\mathbf{B}} $: by inclusion-exclusion principle for set cardinality
    $$\mathbf{I}(i,j) \Rightarrow \sum_{\left\Vert \mathbf{r} \right\Vert=k,\mathbf{r}\in R}(-1)^{k+1}\mathbf{I_r}(i,j)=1 $$
\end{itemize} 

\paragraph*{Curse of dimensionality} this could arise in a setting with \textbf{many dimensions} of clustering, and in which one or more dimensions have \textbf{few} clusters\footnote{The square design (each dimension has the same number of clusters) with orthogonal dimensions has the \textbf{least} independence of observations.}.
\citet{cameron2011robust} suggested an ad-hoc rule of thumb for approximating sufficient numbers of clusters.

\subsection{Non-linear Estimators}
\paragraph*{$m$-Estimators}
Consider an $m$-estimator that solves
$$
\sum^N_{i=1}\mathbf{h}_i\left(\hat{\boldsymbol{\theta}}\right) = \mathbf{0}
$$
under standard assumptions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal with estimated variance matrix 
\begin{equation}\label{eq:m-est_varmat}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \hat{\mathbf{A}}^{-1}\hat{\mathbf{B}}{\hat{\mathbf{A}^{\prime}}^{-1}}
\end{equation}
where $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}}$ and $\hat{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i\mathbf{h}_i\right]$.

\begin{itemize}
    \item \myhl[myblue]{\textbf{one-way clustering}} $\hat{\mathbf{B}} = \sum^G_{g=1}\hat{\mathbf{h}}_g\hat{\mathbf{h}}_g'$ where $\hat{\mathbf{h}}_g = \sum^{N_g}_{i=1}\hat{\mathbf{h}}_{ig}$, clustering may not lead to parameter inconsistency, depending on whether $\mathbb{E}\left[\mathbf{h}_i(\boldsymbol{\theta})\right]= \mathbf{0}$ with clustering
    \begin{itemize}
        \item \textbf{population-averaged approach}: assum $\mathbf{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi \left( \mathbf{x}_{ig}'\boldsymbol{\beta} \right)$
        \item \textbf{random effects approach}: let $y_{ig}=1$ if $y^*_{ig} > 0$ where $y^*_{ig}=\mathbf{x}_{ig}'\boldsymbol{\beta}+\epsilon_g + \epsilon_{ig}$, where 
        \begin{itemize}
            \item idiosyncratic error $\epsilon_{ig}\sim \mathcal{N}(0,1)$
            \item cluster-specific error $\epsilon_g \sim \mathcal{N}(0,\sigma^2_g)$
        \end{itemize}
        then we have the alternative moment condition $$ \mathbb{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi\left(\frac{\mathbf{x}_{ig}'\boldsymbol{\beta}}{\sqrt{1+\sigma^2_g}}\right) $$ 
    \end{itemize}
    \item \myhl[myblue]{\textbf{multiway clustering}} replacing $\hat{u}_i\mathbf{x}_i$ in Eq.(\ref{eq:centermat_multiclu}) with $\hat{\mathbf{h}}_i$, then we have 
    \begin{align*}
        \hat{\mathbb{V}} \left[\hat{\boldsymbol{\theta}}\right] &= \hat{\mathbf{A}}^{-1} \tilde{\mathbf{B}} \hat{\mathbf{A}'}^{-1}
    \end{align*}
    where 
    \begin{align*}
        \hat{\mathbf{A}} &= \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} & \tilde{\mathbf{B}} &= \sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r & \tilde{\mathbf{B}}_r &\equiv \sum^N_{i=1}\sum^N_{j=1}\hat{\mathbf{h}}_i\hat{\mathbf{h}'}_j\mathbb{I}_{\mathbf{r}}(i,j)
    \end{align*}
    with $\mathbf{r}\in R$\footnote{This multiway clustering can be implemented using several one-way clustered bootstraps. Each of the one-way cluster robust matrices is estimated by a pairs cluster bootstrap that resamples with replacement from the appropriate cluster dimension. They are then combined as if they had been estimated analytically \citep{cameron2011robust}.}.
\end{itemize} 

\paragraph*{GMM estimation} Consider an example of over-identified models: linear two stage least squares with more instruments than endogenous regressors, we have 
$$
\hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}) =  \arg\min_{\boldsymbol{\theta}} \left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)'\mathbf{W}\left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)
$$
where $\mathbf{W}$ is a symmetric positive definite weighting matrix. Under standard regularity conditions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal, with estimated variance matrix 
\begin{equation*}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}\hat{\mathbf{A}}'\mathbf{W}\tilde{\mathbf{B}}\mathbf{W}\hat{\mathbf{A}}\left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}
\end{equation*}
again, $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} $, and $\tilde{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i \mathbf{h}_i\right]$.

\section{Menzel (2021): Asymptotic Gaussianity}
One key of TWCR inference is the asymptotic Gaussianity, \citet{menzel2021bootstrap} pointed out the potential non-Gaussianity of the limit distribution.
Still, consider a random array ($Y_{it}$) indexed by two dimensions by $i=1,\cdots,N$ and $t=1,\cdots,T$. Clusters are sampled independently at random from an infinite population,
but otherwise \textbf{unrestricted} in dependence within each row $\mathbf{Y}_{i\cdot} \coloneq \left(Y_{i1}\cdots,Y_{iT}\right)$ and within each column $\mathbf{Y}_{\cdot t}\coloneq \left(Y_{1t},\cdots,Y_{Nt}\right)$.

\subsection{Distribution of Sample Average}
First, consider 
$$
\bar{Y}_{NT} \coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y_{it}
$$
and approximate the asymptotic distribution regardless of whether, or what type of, cluster-dependence is present.

\paragraph*{3 scenarios} of the array $(Y_{it})$
\begin{itemize}
    \item \myhl[myblue]{\textbf{no cluster-dependence}}: $(Y_{it})$ are mutually independent, CLT at a rate of $(NT)^{-1/2}$ applies (under regularity conditions)
    \item \myhl[myblue]{\textbf{correlation within clusters}}: the convergence rate of $(Y_{it})$ is determined by the number of relevant clusters 
    \item \myhl[myblue]{\textbf{non-separable models of heterogeneity (dependence with clusters, even uncorrelated)}}\footnote{This is specific to clustering in 2 or more dimensions.}: The asymptotic behavior is non-standard
\end{itemize}
Consider 2 examples:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Additive factor model}}
    $$ Y_{it} = \mu + \alpha_i + \gamma_t + \epsilon_{it} $$
    where $\mu$ is a constant, and $\alpha_i,\gamma_i,\epsilon_{it}$ are zero-mean i.i.d. random variables for $i=1,\cdots,N$ and $t=1,\cdots,T$ with bounded second moments, and $N=T$. Based on a standard central limit theory, we have 
    \begin{itemize}
        \item in the \underline{non-degenerate} case with $\mathrm{Var}(\alpha_i) >0$ or $\mathrm{\gamma_t}>0$, the sample distribution $$ \sqrt{N}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\alpha_i) + \mathrm{Var}(\gamma_t)\right) $$
        \item in the \underline{degenerate} case of \underline{no clustering} with $\mathrm{Var}(\alpha_i) = \mathrm{Var}(\gamma_t) =0$, the sample distribution $$ \sqrt{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\epsilon_{it})\right) $$ 
    \end{itemize}
    if marginal distributions of $\alpha_i,\gamma_t,\epsilon_{it}$ are known, we can simulate from the joint distribution of $\left(Y_{it}\right)$ by sampling the individual components at random, a bootstrap procedure would be consistent. If \textbf{unknown}, consider estimators 
    \begin{align*}
        \hat{\alpha}_i & \coloneq \frac{1}{T}\sum^T_{t=1}\left(Y_{it}-\bar{Y}_{NT}\right) = \alpha_i + \frac{1}{T}\sum^T_{t=1} \left(\epsilon_{it}-\bar{\epsilon}_{NT}\right)\\
        \hat{\gamma}_t & \coloneq \frac{1}{N}\sum^N_{i=1}\left(Y_{it}-\bar{Y}_{NT}\right) = \gamma_t + \frac{1}{N}\sum^N_{i=1} \left(\epsilon_{it}-\bar{\epsilon}_{NT}\right)\\
        \hat{\epsilon}_{it} & \coloneq Y_{it} - \bar{Y}_{NT} -\hat{\alpha}_i -\hat{\gamma}_t
    \end{align*}
    then use these empirical distributions for estimation and form a bootstrap sample 
    $$ Y^*_{it} \coloneq \bar{Y}_{NT} + \alpha^*_i + \gamma^*_t + \epsilon^*_{it} $$
    by drawing from these estimators and obtain $\bar{Y}^*_{NT}\coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y^*_{it}$, and verify the conditional variances of the bootstrap distribution given the sample:
    \begin{align*}
        \frac{1}{N}\sum^N_{i=1}\left(\hat{\alpha}_i - \frac{1}{N}\sum^N_{j=1}\hat{\alpha}_j\right)^2 - \left[\mathrm{Var}(\alpha_i) + \frac{\mathrm{Var}(\epsilon_{it})}{T}\right] & \xrightarrow{p} 0\\
        \frac{1}{T}\sum^N_{i=1}\left(\hat{\gamma}_t - \frac{1}{T}\sum^T_{s=1}\hat{\gamma}_t\right)^2 - \left[\mathrm{Var}(\gamma_t) + \frac{\mathrm{Var}(\epsilon_{it})}{N}\right] & \xrightarrow{p} 0
    \end{align*}
    then the bootstrap distribution is
    \begin{itemize}
        \item in the \underline{non-degenerate} case, $$ \sqrt{N}\left( \bar{Y}^*_{NT}-\bar{Y}_{NT} \right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\alpha_i) + \mathrm{Var}(\gamma_t)\right) $$
        the estimation error $\hat{\alpha}_i$ does \textbf{NOT} affect the asymptotic variance.
        \item in the \underline{degenerate} case, $$ \sqrt{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right) \xrightarrow{d} \mathcal{N}\left(0,3\mathrm{Var}(\epsilon_{it})\right) $$
        asymptotically overestimates the variance of the sampling distribution, leading to inconsistency of this naive bootstrapping procedure.
    \end{itemize}
    \item \myhl[myblue]{\textbf{Non-Gaussian limit distribution}} $$ Y_{it} = \alpha_i \gamma_t + \epsilon_{it} $$
    where $\alpha_i,\gamma_t,\epsilon_{it}$ are independently distributed with $\mathbb{E}\left[\epsilon_{it}\right] = 0,\mathrm{Var}(\alpha_i) = \sigma^2_{\alpha},\mathrm{Var}(\gamma_t) = \sigma^2_{\gamma}, \mathrm{Var}(\epsilon)_{it}=\sigma^2_{\epsilon}$. \textbf{If} $\mathbb{E}\left[\alpha_i\right] = \mathbb{E}\left[\gamma_t\right] = 0$, then CLT and Continuous Mapping Theorem (CMT) imply 
    \begin{align*}
        \sqrt{NT} \cdot \bar{Y}_{NT} =& \frac{1}{\sqrt{NT}} \sum^N_{i=1}\sum^T_{t=1} \left(\alpha_i \gamma_t +\epsilon_{it}\right)\\
        =& \left( \frac{1}{\sqrt{N}} \sum^N_{i=1}\alpha_i \right) \left(\frac{1}{\sqrt{T}} \sum^T_{t=1}\gamma_t \right) + \frac{1}{\sqrt{NT}}\sum^N_{i=1}\sum^T_{t=1}\epsilon_{it}\\
        \xrightarrow{d}& \sigma_{\alpha}\sigma_{\gamma}Z_1Z_2 + \sigma_{\epsilon}Z_3
    \end{align*}
\end{itemize}
then even without correlation within clusters, non-separable heterogeneity can still generate dependence in $2^{\text{nd}}$ or higher moments in the limiting distribution\footnote{2 major issues arise:\begin{itemize}
    \item The limiting distribution needs \textbf{not} be Gaussian: plug-in asymptotic inference based on the normal distribution is invalid 
    \item It only comes from two-or-more-dimension cluster dependence, not single-dimension cluster dependence.
\end{itemize}
}.

\subsection{Menzel (2021)'s Bootstrap procedure}
\subsubsection{Notation}
For the array $\left(Y_{it}\right)_{i,t}$, denote 
\begin{itemize}
    \item $\mathbb{P}$: joint distribution of $\left(Y_{it}\right)_{i,t}$
    \item $\mathbb{P}_{NT}$: drifting DGP indexed by $N,T$
    \item $\mathbb{P}^*_{NT}$: bootstrap distribution for $\left(Y^*_{it}\right)$ given the realizations $\left(  Y_{it}:i=1,\cdots,N; t= 1,\cdots,T  \right)$
    \item respective distributions $\mathbb{E},\mathbb{E}_{NT},\mathbb{E}^*_{NT}$
\end{itemize}

\subsubsection{Inference: Sample Mean}
First, consider the assumption of \textit{separate exchangeability}
\begin{assumption}{Separate Exchangeability}{sep_exchange}
    \begin{itemize}
        \item A \textbf{separately exchangeable} array is an infinite array $\left(Y_{it}\right)_{i,t}$ such that for any integers $\tilde{N},\tilde{T}$ and permutations $\pi_1:\left\{1,\cdots,\tilde{N}\right\}\rightarrow \left\{1,\cdots,\tilde{N}\right\}$ and $\pi_2:\left\{1,\cdots,\tilde{T}\right\}\rightarrow \left\{1,\cdots,\tilde{T}\right\}$, we have 
        $$ \left(Y_{\pi_1(i),\pi_2(t)}\right)_{i,t} \overset{d}{=} \left(Y_{it}\right)_{i,t} $$
        such an array is called \textbf{dissociated} if for any $N_0,T_0\geq 1$, $\left(Y_{it}\right)^{i=N_0,t=T_0}_{i=1,t=1}$ is independent of $\left(Y_{it}\right)_{i>N_0,t>T_0}$.
        \item For dyadic data, consider the alternative assumption \textbf{jointly exchangeable} arrays $\left(Y_{ij}\right)_{i,j}$ satisfying 
        $$ \left(Y_{\pi(i),\pi(j)}\right)_{i,j} \overset{d}{=} \left(Y_{ij}\right)_{i,j} $$
        for any permutation $\pi$ on $\left\{1,\cdots,\tilde{N}\right\}$, in addition, $\left(Y_{ij}\right)^{N_0}_{i,j=1}$ is independent of $\left(Y_{ij}\right)_{i,j>N_0}$
    \end{itemize}
\end{assumption}
This assumption can be interpreted as rows (and columns) corresponding to units that are drawn independently from a common population, where we then observe the joint outcome for every row-column pair, consider the requirements in the following applications
\begin{itemize}
    \item \myhl[myblue]{\textbf{DiD/matched data}}: the units corresponding to either dimension of the sample to represent independent draws from a common, infinite population 
    \item \myhl[myblue]{\textbf{non-exhaustively matched data}}: only observe joint outcomes for a posibly self-selected subset of unit pairs, sample selection should be (jointly or separately) exchangeable
    \item \myhl[myblue]{\textbf{U-/V-statistics}}: the kernel $Y_{i_1,\cdots,i_D} \coloneq h\left(X_{i_1},\cdots,X_{i_D}\right) $ evaluated at i.i.d. observations $X_1,\cdots,X_N$ forms a dissociated, jointly exchangeable array
    \item \myhl[myblue]{\textbf{Network}}: unlabeled\footnote{\textit{Unlabeled}: nodel identifiers do not carry any significance for the statistical model.} data implies finite exchangeability, the sampled graph has joint (\textit{infinite}) exchangeability if it is a subgraph of an infinite graph
\end{itemize}
Directly from Assumption \ref{assump:sep_exchange}, any dissociated separately exchangeable array can be represented as 
\begin{equation*}
    Y_{it} = f\left(\alpha_i,\gamma_t,\epsilon_{it}\right)
\end{equation*}
for some function $f(\cdot)$ where $\alpha_1,\cdots,\alpha_N$, $\gamma_1,\cdots,\gamma_T$, $\epsilon_{11},\cdots,\epsilon_{NT}$ are mutually independent, uniformly distributed random variables.

\paragraph*{Projection} now, decompose the array $\left(Y_{it}\right)_{i,t}$ as 
\begin{align*}
    Y_{it} &= b + a_i + g_t + w_{it} & \mathbb{E}\left[w_{it}\mid a_i,g_t\right]&=0
\end{align*}
where $a_i$ and $g_t$ are mean-zero and mutually independent, s.t. the joint distribution of $Y_{it}$ can then be expanded as 
\begin{align*}
    Y_{it} =& \mathbb{E}\left[Y_{it}\right] + \left(\mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\right] \right) + \left( \mathbb{E}\left[Y_{it}\mid \gamma_t\right] - \mathbb{E}\left[Y_{it}\right] \right) \\
    &+ \left( \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]\right) + \left( Y_{it}-\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] \right) \\
    \eqcolon & b+ a_i + g_t + v_{it} + e_{it}
\end{align*}
with
\begin{itemize}
    \item $e_{it} = Y_{it} - \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right]$
    \item $a_i = \mathbb{E}\left[Y_{it}\mid\alpha_i \right]-\mathbb{E}\left[Y_{it}\right]$, $g_t= \mathbb{E}\left[Y_{it}\mid \gamma_t\right]-\mathbb{E}\left[Y_{it}\right]$
    \item $v_{it} = \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]$
    \item $b= \mathbb{E}\left[Y_{it}\right]$
\end{itemize}
here, 
\begin{itemize}
    \item temporal and cross-sectional units were drawn independently: $a_1,\cdots,a_N$ and $g_1,\cdots,g_T$ are independent of each other.
    \item by construction, $\mathbb{E}\left[e_{it}\mid a_i,g_t,v_{it}\right]=0$, $\mathbb{E}\left[v_{it}\mid a_i\right] = \mathbb{E}\left[v_{it}\mid g_t\right]=0$
    \item $e_{it}$, $(a_i,g_t)$ and $v_{it}$ are \textbf{uncorrelated}
\end{itemize}
then, rewrite the sample mean as 
\begin{align*}
    \hat{Y}_{NT} &= b+\bar{a}_N + \bar{g}_T + \bar{v}_{NT} + \bar{e}_{NT}\\
    &\coloneq b + \frac{1}{N}\sum^N_{i=1}a_i + \frac{1}{T}\sum^T_{t=1}g_t + \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}v_{it} + \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}e_{it}
\end{align*}
and the unconditional variances of the projections with 
\begin{align*}
    \sigma^2_a&\coloneq \mathrm{Var}(a_i) & \sigma^2_g&\coloneq \mathrm{Var}(g_t) & \sigma^2_v&\coloneq \mathrm{Var}(v_{it}) & \sigma^2_e&\coloneq \mathrm{Var}(e_{it})   
\end{align*}
let $w_{it}\coloneq v_{it}+ e_{it}$, and denote its variance by $\sigma^2_w = \mathrm{Var}(w_{it})$. Then, assume integrability 
\begin{assumption}{Integrability}{integrability}
    Let $Y_{it} = f(\alpha_i,\gamma_t,\epsilon_{it})$, where $\alpha_i,\gamma_t,\epsilon_{it}$ are random arrays with elements i.i.d. drawn from $[0,1]$ uniform distribution, assume 
    \begin{itemize}
        \item $a_i/\sigma_a$, $g_t/\sigma_g$, $v_{it}/\sigma_v$, $e_{it}/\sigma_e$ are well-defined and have bounded moments up to the order $4+\delta$ for some $\delta>0$, whenever the respective variances $\sigma^2_a,\sigma^2_g,\sigma^2_v,\sigma^2_e$ are non-zero.
        \item $\sigma^2_a + \sigma^2_g >0$, or $\sigma^2_v + \sigma^2_e > 0$
    \end{itemize}
\end{assumption}

\paragraph*{Low-rank approximation}
Consider the row/column projection
\begin{equation*}
    \bar{v}_{NT} \equiv \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}\left(\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] -\mathbb{E}\left[Y_{it}\mid \alpha_i\right] -\mathbb{E}\left[Y_{it}\mid \gamma_t\right] +\mathbb{E}\left[Y_{it}\right] \right) \eqcolon \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}v(\alpha_i,\gamma_t)
\end{equation*}
as a generalized U-statistic with a kerel $v(\alpha,\gamma)$ evaluated at the samples $\alpha_1,\cdots,\alpha_N$ and $\gamma_1,\cdots,\gamma_t$. There are 2 major issues w.r.t. characterizing the distribution of $\bar{Y}_{NT}$
\begin{itemize}
    \item the presence of the projection error $e_{it}$
    \item the factors $\alpha_i,\gamma_t$ are not observable
\end{itemize}
Define,
$$
v(\alpha,\gamma) \coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha,\gamma_t=\gamma\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t=\gamma\right] + \mathbb{E}\left[Y_{it}\right]
$$
under Assumption \ref{assump:integrability}, we have compact integral operators
\begin{align*}
    S(u)(g)&=\int v(a,g)u(a)F_{\alpha}(\mathrm{d}a) & S^*(u)(a)&=\int v(a,g)u(g)F_{\gamma}(\mathrm{d}g)
\end{align*}
where $F_{\alpha},F_{\gamma}$ are the marginal distributions corresponding to the joint $F_{\alpha\gamma}$ of $\alpha_i,\gamma_t$. Then the low-rank approximation is
\begin{equation}\label{eq:low-rank_approximation}
    v(\alpha,\gamma) = \sum^{\infty}_{k=1}c_k\phi_k(\alpha)\psi_k(\gamma)
\end{equation}
under the $L_2(F_{\alpha\gamma})$ norm on the space of smooth functions of $(\alpha,\gamma)\in[0,1]^2$. Here 
\begin{itemize}
    \item $(c_k)_{k\geq 1}$: a sequence of singular values, $\lim\left\vert c_k \right\vert \rightarrow 0$
    \item $\left(\phi_k\left(\cdot\right)\right)_{k\geq 1}$ and $\left(\psi_k\left(\cdot\right)\right)_{k\geq 1}$: orthonormal bases for $L_2\left([0,1],F_{\alpha}\right)$ and $L_2\left([0,1],F_{\gamma}\right)$:
    \begin{itemize}
        \item By construction: $$ \mathbb{E}\left[v(a,\gamma_t)\right] = \mathbb{E}\left[v(\alpha_i,g)\right] =0,\forall a,g\in[0,1] \Rightarrow \mathbb{E}\left[\phi_k(\alpha_i)\right] = \mathbb{E}\left[\psi_k(\gamma_t)\right] = 0,\forall k =1,2,\cdots $$
        \item the basis functions are orthonormal and $\alpha_i$ and $\gamma_t$ are independent, then $\forall K<\infty$ $$ \mathrm{Cov}\left[ \left(\phi_1(\alpha_i),\psi_1(\gamma_t), \cdots, \phi_K(\alpha_i),\psi_K(\gamma_t) \right) \right] $$ is the $2K$-dimensional identity matrix
        \item $\left(\phi_1(\alpha_i),\cdots, \phi_K(\alpha_i)\right)$ can be correlated with $a_i$: $\sigma_{ak}\coloneq \mathrm{Cov}\left(a_i,\phi_k(\alpha_i)\right)$
        \item $\left(\psi_1(\gamma_t),\cdots, \psi_K(\gamma_t)\right)$ can be correlated with $g_t$: $\sigma_{gk}\coloneq \mathrm{Cov}\left(g_t,\psi_k(\gamma_t)\right)$
    \end{itemize}
\end{itemize}
with this representation of Eq.(\ref{eq:low-rank_approximation}), we have\footnote{The limiting distribution of this term is not Gaussian, but can be represented as a linear combination of independent chi-squared random variables. This type of distributions is known as Wiener/Gaussian chaos.} 
\begin{equation*}
    \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}v(\alpha_i,\gamma_t) = \sum^{\infty}_{k=1}c_k \left(\frac{1}{N}\sum^N_{i=1}\phi_k(\alpha_i)\right) \left(\frac{1}{T}\sum^T_{t=1}\psi_k(\gamma_t)\right)
\end{equation*}
and the second-order projection term can also be represented as a function of \textbf{countably many} sample averages of \textbf{i.i.d. mean-zero} random variables.

\begin{assumption}{Eigenfucntions and coefficients in the spectral represention (\ref{eq:low-rank_approximation})}{restrictions_on_lowrankapprox}
    The function $v(\alpha,\gamma)\coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha,\gamma_t=\gamma\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t=\gamma\right] + \mathbb{E}\left[Y_{it}\right]$ admits a spectral representation
    $$
    v(\alpha,\gamma) = \sum^{\infty}_{k=1}c_k\phi_k(\alpha)\psi_k(\gamma)
    $$
    under the $L_2(F_{\alpha\gamma})$ norm. And 
    \begin{itemize}
        \item the singular values are uniformly bounded by a square summable null sequence $\bar{c}_k$: $c_k\leq \bar{c}_k,\forall k=1,2,\cdots$, where $\sum^{\infty}_{k=1}c^2_k< \infty$
        \item $\forall k=1,2,\cdots$, the first 3 moments of the eigenfunctions $\phi_k(\alpha_i)$ and $\psi_k(\gamma_t)$ are bounded by a constant $B>0$
    \end{itemize}
\end{assumption}
To summarize the two assumptions
\begin{itemize}
    \item Assumption \ref{assump:sep_exchange} guarantees the pointwise consistency of the bootstrap
    \item Assumption \ref{assump:restrictions_on_lowrankapprox} gives the uniform consistency of the bootstrap: it imposes common bounds on moments and singular values and restricts the set of joint distribution $F$ to a \myhl[myblue]{\textbf{uniformity}} class\footnote{Here, the sequence $\mathbf{c}\coloneq \left(\bar(c)_k\right)_{k\geq 0}$ controls the magnitude of the error from a finite-dimensional approximation to $v(\alpha,\gamma)$.}.
\end{itemize}

\subsubsection{Bootstrap procedure}
For the sample mean $\bar{Y}_{NT} - \mathbb{E}\left[Y_{it}\right]$, the limiting distribution depends on the scale parameters:
\begin{itemize}
    \item If observations are independent across rows and columns: $\sqrt{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\sigma^2_e\right)$
    \item If $N=T$, within-cluster covariances are bounded from 0 in \textbf{at least one dimension}: $\sqrt{N}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d}\mathcal{N}\left(0,\sigma^2_a+\sigma^2_g\right)$
\end{itemize}
The bootstrap procedure should then be adaptive for both degenerate and non-degenerate cases. For the expansion
\begin{align}\label{eq:sample_mean_decomp}
    Y_{it} =& \mathbb{E}\left[Y_{it}\right] + \left(\mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\right] \right) + \left( \mathbb{E}\left[Y_{it}\mid \gamma_t\right] - \mathbb{E}\left[Y_{it}\right] \right) \\ \nonumber
    &+ \left( \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]\right) + \left( Y_{it}-\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] \right) \\ \nonumber
    \eqcolon & b+ a_i + g_t + v_{it} + e_{it}
\end{align}
the sample analogs are:
\begin{align*}
    \hat{a}_i &\coloneq \frac{1}{T}\sum^T_{t=1}Y_{it}-\bar{Y}_{NT} & \hat{g}_t &\coloneq \frac{1}{N}\sum^N_{i=1}Y_{it} - \bar{Y}_{NT} & \hat{w}_{it}\coloneq & Y_{it} - \hat{a}_i-\hat{g}_t -\bar{Y}_{NT}
\end{align*}

\paragraph*{Evaluating bootstrap performance} it is crucial at what rates these estimators are consistent depending on the extent of clustering in the true DGP.
The variance of the projection terms are: 
\begin{align*}
    \mathrm{Var}\left(\hat{a}_i\right) &= \sigma^2_a + \frac{\sigma^2_w}{T} & \mathrm{Var}\left(\hat{g}_t\right) &= \sigma^2_g + \frac{\sigma^2_w}{N}
\end{align*}
s.t. the \textbf{convolution error} depending on $\sigma^2_w$ dominates in the degenerate case. Therefore, to correct for the contribution of the row/column averages of $w_{it}$, consider the scalar for the distribution of $\hat{a}_i,\hat{g}_t$ by 
\begin{align*}
    \lambda_a &= \frac{T\sigma^2_a}{T\sigma^2_a +\sigma^2_w} & \lambda_g &= \frac{N\sigma^2_g}{N\sigma^2_g +\sigma^2_w} 
\end{align*}

\paragraph*{Component variance estimator} let 
\begin{align*}
    \hat{s}^2_a &\coloneq \frac{1}{N-1}\sum^N_{i=1} \left(\hat{a}_i - \bar{Y}_{NT}\right)^2 \\
    \hat{s}^2_g &\coloneq \frac{1}{T-1}\sum^T_{t=1} \left(\hat{g}_t - \bar{Y}_{NT}\right)^2 \\
    \hat{s}^2_w &\coloneq \frac{1}{NT-N-T} \sum^N_{i=1}\sum^T_{t=1} \left(Y_{it}-\hat{a}_i-\hat{g}_t -\bar{Y}_{NT}\right)^2
\end{align*}
then form the estimators as 
\begin{align}
    \hat{\sigma}^2_a &= \max \left\{0,\hat{s}_a^2 -\frac{1}{T}\hat{s}^2_w\right\} & \hat{\sigma}_g^2 &=\max \left\{0,\hat{s}^2_g -\frac{1}{N}\hat{s}_w^2\right\} & \hat{\sigma}^2_w&\coloneq \hat{s}^2_w
\end{align}
the rates of convergence for these estimators are given in the following lemma:
\begin{lemma}{Stochastic Order of Variance Estimators}{varest_stcha_order}
    Under Assumption \ref{assump:sep_exchange},
    \begin{align*}
        \hat{\sigma}^2_a - \sigma^2_a &= O_p \left(\frac{1}{\sqrt{N}}\left(\sigma_a + \frac{\sigma_e}{\sqrt{T}}\right)^2 + \frac{\sigma^2_v}{T}\right) \\
        \hat{\sigma}^2_g - \sigma^2_g &= O_p \left(\frac{1}{\sqrt{T}}\left(\sigma_g + \frac{\sigma_e}{\sqrt{N}}\right)^2 + \frac{\sigma^2_v}{N}\right) \\
        \hat{\sigma}^2_w - \sigma^2_w &= O_p \left(\frac{\sigma^2_e}{\sqrt{NT}} + \left(\frac{1}{N} + \frac{1}{T}\right)\sigma^2_v \right)
    \end{align*}
    and there exist \textbf{no estimators} for $\sigma^2_a,\sigma^2_g,\sigma^2_w$ that converge at rates faster than these rates. Specifically, $\sigma^2_a$ can \textbf{NOT} be estimated at a rate faster than $T^{-1}$ even when $\sigma^2_a=0$\footnote{See the appendix of \citet{menzel2021bootstrap} for the proof.}.
\end{lemma}
Hence, a bootstrap procedure can use a consistent pre-test for the presence of cluster dependence in the \textbf{first moment}, with the model selectors 
\begin{align*}
    \hat{D}_a(\kappa) &\coloneq \mathbf{1} \left\{T\hat{\sigma}_a^2 \geq \kappa \right\} & \hat{D}_g(\kappa) &\coloneq \mathbf{1}\left\{N\hat{\sigma}_g^2 \geq \kappa \right\}
\end{align*}
$\forall \kappa \geq 0$. And for some $\kappa_a,\kappa_g$, let 
\begin{align*}
    \hat{\lambda}_a & \coloneq \frac{\hat{D}_a (\kappa_a)T\hat{\sigma}^2_a}{\hat{D}_a (\kappa_a)T\hat{\sigma}^2_a+\hat{\sigma}_w^2} & \hat{\lambda}_g & \coloneq \frac{\hat{D}_g (\kappa_g)T\hat{\sigma}^2_g}{\hat{D}_g (\kappa_g)N\hat{\sigma}^2_g + \hat{\sigma}_w}
\end{align*}
and estimate the asymptotic variance of the sample mean as 
\begin{equation}\label{eq:asymp_variance}
    \hat{S}^2_{NT,sel}\coloneq \hat{D}_a(\kappa_a)T\hat{\sigma}^2_a + \hat{D}_g (\kappa_g) N\hat{\sigma}^2_g + \hat{\sigma}^2_w
\end{equation}

\paragraph*{Bootstrap procedures}
\citet{menzel2021bootstrap} proposed the following resampling algorithm to estimate the sampling distribution for exhaustive sampling with cluster dependence in two dimensions 
\begin{algorithm}{Resampling Algorithm}{resampling_menzel}
    \begin{itemize}
        \item[\textbf{(a)}] For the $b$-th bootstrap iteration, draw 
        \begin{align*}
            a^*_{i,b} &\coloneq \hat{a}_{k^*_b(i)} & g^*_{t,b} &\coloneq \hat{g}_{s^*_b(t)}
        \end{align*}
        where $k^*_b(i)$ and $s^*_b(t)$ are i.i.d. draws from the discrete uniform distribution on the index sets $\left\{1,\cdots,N\right\}$ and $\left\{1,\cdots,T\right\}$ respectively
        \item[\textbf{(b)}] Generate $$ w^*_{it,b} \coloneq \omega_{1i,b}\omega_{2t,b}\hat{w}_{k^*_b(i)s^*_b(t)} $$ where $\omega_{1i,b},\omega_{2t,b}$ are i.i.d. random variables with $\mathbb{E}\left[\omega\right]=0$, $\mathbb{E}\left[\omega^2\right] = \mathbb{E}\left[\omega^3\right] = 1$\footnote{Typical choices of $\omega_{1i,b},\omega_{2t,b}$ are the Gamma distribution (with shape $=4$, scale $=1/2$).}
        \item[\textbf{(c)}] Generate a bootstrap sample of draws $$ Y^*_{it,b} = \bar{Y}_{NT} + \sqrt{\hat{\lambda}_a}a^*_{i,b} + \sqrt{\hat{\lambda}_g}g^*_{t,b} + w^*_{it,b} $$ and get the bootstrapped statistic $$ \bar{Y}^*_{NT,b} \coloneq \frac{1}{NT} \sum^N_{i=1}\sum^T_{t=1}Y^*_{it,b} $$ 
        \item[\textbf{(d)}] Repeat this procedure, get a sample of $B$ replications and approximate the conditional distribution of $\bar{Y}^*_{NT}$ given the sample with the empirical distribution over the bootstrap draws $\bar{Y}^*_{NT,1},\cdots,\bar{Y}^*_{NT,B}$ 
    \end{itemize}
\end{algorithm}
For the \textbf{pivotal boostrap}, the last step uses instead the empirical distribution of the studentized bootstrap draws to approximate the distribution of $$ \sqrt{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right)/\hat{S}^*_{NT,sel} $$
where $\hat{S}^*_{NT,sel}$ is the bootstrap analog of the variance estimator $\hat{S}_{NT,sel}$.

\begin{definition}{Bootstrap Procedures}{menzel_bootstrap_procedure}
    Consider 3 versions of the bootstrap procedure based on \ref{algm:resampling_menzel}:
    \begin{itemize}
        \item \myhl[myred]{\textbf{BS-N}} (bootstrap \textit{without} model selection): apply steps (a) - (d), and set $\kappa_a = \kappa_g = 0$
        \item \myhl[myred]{\textbf{BS-S}} (bootstrap \textit{with} model selection): apply steps (a) - (d), and set $\kappa_a,\kappa_g$ according to increasing sequences $\kappa_g,\kappa_a\rightarrow \infty$ s.t. $\kappa_a/T\rightarrow 0$ and $\kappa_g/N\rightarrow 0$
        \item \myhl[myred]{\textbf{BS-C}} (\textit{conservative} bootstrap): addition to the settings of \myhl[myred]{\textbf{BS-S}}, set 
        \begin{align*}
            \hat{\lambda}_a \coloneq& \frac{\hat{q}_a}{\hat{q}_a+\hat{\sigma}^2_w}\frac{\hat{q}_a}{T\hat{\sigma}^2_a} & \hat{\lambda}_g &\coloneq \frac{\hat{q}_g}{\hat{q}_g+\hat{\sigma}^2_w}\frac{\hat{q}_g}{N\hat{\sigma}^2_g}
        \end{align*}
        where 
        \begin{align*}
            \hat{q}_a &\coloneq \max\left\{T\hat{\sigma}^2_a,\kappa_a\right\} & \hat{q}_g &\coloneq \max\left\{N\hat{\sigma}^2_g,\kappa_g\right\}
        \end{align*}
    \end{itemize}
\end{definition}

\paragraph*{Consistency of the bootstrap procedures}
\begin{itemize}
    \item {\textbf{BS-N}} (bootstrap \textit{with} model selection): \myhl[myblue]{\textbf{pointwise} consistent} in $\sigma^2_a,\sigma^2_g,\sigma^2_w$
    \item {\textbf{BS-S}} (bootstrap \textit{without} model selection): \myhl[myblue]{\textbf{uniformly} consistent} if the limiting distribution is Gaussian
    \item {\textbf{BS-C}} (\textit{conservative} bootstrap): \myhl[myblue]{\textbf{consistent}} in the nondegerate case $\sigma^2_a + \sigma^2_g >0$, but asymptotically \myhl[myblue]{\textbf{conservative}} for the degenerate cases
\end{itemize}
To establish the consistency, define the \textbf{adaptive rate} $r_{NT}$ as\footnote{Following Eq. (\ref{eq:sample_mean_decomp}), $\mathrm{Var}(\bar{Y}_{NT}) = \mathrm{Var}(b+\bar{a}_N +\bar{g}_T + \bar{v}_{NT} + \bar{e}_{NT})$.}
\begin{equation*}
    r^{-2}_{NT} \coloneq N^{-1}\sigma^2_a + T^{-1}\sigma^2_g + (NT)^{-1}\sigma^2_w \equiv \mathrm{Var}(\bar{Y}_{NT})
\end{equation*}
then consider the limiting distribution with the respective limits of normalized sequences:
\begin{align}\label{eq:normalized_variances}
    q_{a,NT} &\coloneq r^2_{NT}N^{-1}\sigma^2_a & q_{g,NT} &\coloneq r^2_{NT}T^{-1}\sigma^2_g & q_{e,NT} &\coloneq r^2_{NT}(NT)^{-1}\sigma^2_e & q_{v,NT} &\coloneq r^2_{NT}(NT)^{-1}\sigma^2_v \\
    & & q_{ak,NT} & \coloneq r^2_{NT}N^{-1}\sigma_{ak} & q_{gk,NT} &\coloneq r^2_{NT}T^{-1}\sigma_{gk}\nonumber
\end{align}
for $k=1,2,\cdots$. Let $\varrho_{NT} \coloneq r_{NT}\left(NT\right)^{-1/2}$, then 
$$
q_{a,NT} + q_{g,NT} + q_{e,NT} + q_{v,NT} = 1
$$
stacking the sequences as the vector
$$
\mathbf{q}_{NT} \coloneq \left( q_{e,NT},q_{a,NT},q_{g,NT}, q_{a1,NT},q_{g1,NT}, q_{a2,NT},q_{g2,NT},\cdots \right)
$$
and the singular values for the spectral decomposition (\ref{eq:low-rank_approximation}):
\begin{align*}
    \mathbf{c}_{NT} &\coloneq \left(c_{1,NT},c_{2,NT},\cdots\right) \in l^2 & \text{for }&\mathbb{E}_{NT}\left[Y_{it}\mid \alpha_i,\gamma_t\right]\\
    \mathbf{c} &\coloneq \left(c_1,c_2,\cdots\right) \in l^2 & \text{for }&\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right]
\end{align*}
then for convergent sequences $\mathbf{q}_{NT},\mathbf{c}_{NT},\mathbf{c}$, denote the limits 
\begin{align*}
    q_a &\coloneq \lim_{N,T}q_{a,NT} & q_g &\coloneq \lim_{N,T}q_{g,NT} & q_e & \coloneq \lim_{N,T}q_{e,NT} & q_v & \coloneq \lim_{N,T} q_{v,NT} \\
    \mathbf{q} & \coloneq \lim_{N,T}\mathbf{q}_{NT} & \mathbf{c} & \coloneq \lim_{N,T}\mathbf{c}_{NT} & \varrho &\coloneq \lim_{N,T}\varrho_{NT}
\end{align*}
for any fixed values of $\mathbf{q}, \mathbf{c},\varrho\in[0,1]$, define 
\begin{equation}\label{eq:convergence_law}
    \mathcal{L}_0\left(\mathbf{q},\mathbf{c},\varrho\right) \coloneq \left(\sqrt{q_e}Z^e  + \sqrt{q_a}Z^a + \sqrt{q_g}Z^g\right) + \varrho V
\end{equation}
where 
\begin{align*}
    V \coloneq \sum^{\infty}_{k=1}c_k Z^{\psi}_k Z^{\phi}_k
\end{align*}
and $Z^e,Z_k^{\psi},Z_k^{\phi}$ are i.i.d. standard normal random variables, $Z^a,Z_g$ are standard normal random variables with 
\begin{align*}
    \mathrm{Cov}\left(Z^a,Z_k^{\phi}\right) &= \frac{q_{ak}}{\sqrt{q_a}} & \mathrm{Cov}\left(Z^g,Z_k^{\psi}\right) &= \frac{q_{gk}}{\sqrt{q_g}} & \mathrm{Cov}\left(Z^a,Z^g\right) = \mathrm{Cov}\left(Z^a,Z_k^{\psi}\right) = \mathrm{Cov}\left(Z^g,Z_k^{\psi}\right) &=0
\end{align*}
Then, the CLT for sampling distribution is established as
\begin{theorem}{CLT for Sampling Distribution}{clt_samp_dist}
    Under Assumption \ref{assump:integrability},
    \begin{itemize}
        \item[(\textbf{a})] along \textit{any} convergent sequence $\mathbf{q}_{NT}\rightarrow \mathbf{q}$ and fixed $\mathbf{c} = \left(c_1,c_2,\cdots\right)$ , we have 
        \begin{equation*}
            \left\Vert \mathbb{P}\left(r_{NT}\left(\bar{Y}_{NT}-\mathbb{E}[Y_{it}]\right)\right) - \mathcal{L}_0 \left(\mathbf{q},\mathbf{c},\varrho \right) \right\Vert _{\infty} \rightarrow 0
        \end{equation*}
        where $\varrho \coloneq \lim_{N,T}\varrho_{NT}$, and $\left\Vert \cdot \right\Vert _{infty}$ denotes the Kolmogorov metric; the limiting distribution $\mathcal{L}_0\left(\mathbf{q},\mathbf{c},\varrho\right)$ is continuous\footnote{The convergence is pointwise w.r.t. the conditional mean function $\mathbb{E}\left[Y_{it} \mid \alpha_i=\alpha,\gamma_t=\gamma\right]$}.
        \item[(\textbf{b})] if in addition, Assumption \ref{assump:restrictions_on_lowrankapprox} holds, (\textbf{a}) is robust under drifting sequences $\mathbf{c}_{NT}\rightarrow \mathbf{c}$\footnote{The convergence is uniform within the class of distributions satisfying Assumption \ref{assump:restrictions_on_lowrankapprox}}
    \end{itemize}
\end{theorem}

\subparagraph*{Estimating the asymptotic distribution}
Lemma \ref{lemma:varest_stcha_order} establishes the consistency of the estimation for the components vairances $\sigma^2_a,\sigma^2_g,\sigma^2_w$, but are they \textbf{fast} enough?
\begin{proposition}{Estimability of Asymptotic Distribution}{estimability_asympdist}
    Let $\hat{\mathcal{L}}_{NT}$ denote an arbitrary estimator for $\mathcal{L}_0$ based on an array of size $N,T$ form the unknown distribution, then $\exists \delta>0$ s.t. 
    \begin{equation*}
        \liminf_{N,T\rightarrow\infty} \sup_{f\in\mathcal{F}} \mathbb{P}_{f,NT} \left( \left\Vert \hat{\mathcal{L}}_{NT} - \mathcal{L}_0 \left(\mathbf{q}_{NT}(f),\mathbf{c}_{NT}(f),\varrho_{NT}(f)\right) \right\Vert _{\infty} > \delta \right) >0
    \end{equation*}
    where
    \begin{itemize}
        \item $\mathcal{F}$: the class of functions $f(\alpha,\gamma,\epsilon)$ corresponding to distributions of $Y_{it}$ satisfying Assump. \ref{assump:integrability} and \ref{assump:restrictions_on_lowrankapprox}, for i.i.d. uniform $\alpha_i,\gamma_t,\epsilon_{it}$\footnote{From the Aldous-Hoover representation}
        \item $\mathbb{P}_{f,NT}\left(\cdot\right)$: probabilities for events w.r.t. an array of size $N,T$, generated according to $f$
        \item $\mathbf{q}_{NT}(f) \coloneq \left(q_{e,NT}(f),q_{a,NT}(f),\cdots\right)$: the vector of normalized variances from Eq. \ref{eq:normalized_variances}
    \end{itemize}
\end{proposition}
Proposition \ref{prop:estimability_asympdist} states that there exists \textbf{no estimator}
\footnote{Consider the counterexample for this impossibility: for the model $$Y_{it} = \alpha_i\gamma_t$$ where $\alpha_i,\gamma_t$ are mutually independent with i.i.d. factors $\alpha_i \sim \mathcal{N}(0,1),\gamma_t\sim\mathcal{N}(\mu_{\gamma},1)$. This model satisfies Assump.\ref{assump:integrability}, hence 
Thm.\ref{thm:clt_samp_dist} gives convergence results. However, for this model 
\begin{align*}
    a_i & \coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i\right] = \alpha_i\mu_{\gamma} & g_t & \coloneq \mathbb{E}\left[Y_{it}\mid \gamma_t\right] = \gamma_t\mathbb{E}\left[\alpha_i\right]\equiv 0\\
    v_{it} & =\alpha_i(\gamma_t-\mu_{\gamma}) & \sigma^2_a&=\mu^2_{\gamma} & \sigma^2_v=1
\end{align*}
here, $\mu_{\gamma}$ can \textbf{not} be estimated from the original data at a rate faster than $T^{-1/2}$, the fastest possible rate at which $\mu_{\gamma}$ can be estimated from observing $\gamma_1,\cdots,\gamma_T$ directly. Therefore, no test can consistently distinguish the model $\mu_{\gamma}=0$ (asymptotic variance $\sigma^2_v$) from a drifting sequence $\tilde{\mu}_{T,\gamma}\coloneq T^{-1/2}m_{\gamma}$ (asymptotic variance $m^2_{\gamma}+\sigma^2_v$).}
for the asymptotic distribution that achieves consistency uniformly over the space of distributions satisfying Assumption \ref{assump:integrability} and \ref{assump:restrictions_on_lowrankapprox}: 
\begin{itemize}
    \item Under Theorem \ref{thm:clt_samp_dist}, the sample mean $\bar{Y}_{NT}$ converges to a continuous limiting distribution $\mathcal{L}_0(\mathbf{q,c},\varrho)$ along sequences $f_{NT}\in \mathcal{F}$ with proper limits for $\mathbf{q}_{NT},\mathbf{c}_{NT}$
\end{itemize}

\subparagraph*{Bootstrap Consistency}
Consider the bootstrap analog of $\hat{S}_{NT,sel}$ in Eq. \ref{eq:asymp_variance}
\begin{equation*}
    \hat{S}^{2*}_{NT,sel}\coloneq \hat{D}_a(\kappa_a)T\hat{\sigma}^{2*}_a + \hat{D}_g (\kappa_g) N\hat{\sigma}^{2*}_g + \hat{\sigma}^{2*}_w
\end{equation*}
where $\hat{D}_a(\kappa_a),\hat{D}_g(\kappa_g)$ are fixed at the sample values, $\kappa_a,\kappa_g$ are chosen according to whether the bootstrap is \textbf{with} or \textbf{without} model selection. Consider 2 versions based on the studentized sample mean:
\begin{itemize}
    \item \myhl[myblue]{\textbf{non-pivotal}} bootstrap: approximating the distribution of \textbf{the sample mean} $r_{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right)$ with the bootstrap distribution $r_{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right)$
    \item \myhl[myblue]{\textbf{pivotal}} bootstrap: approximating the distribtuion of the \textbf{studentized sample mean} $\frac{(NT)^{1/2}}{\hat{S}_{NT,sel}}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right)$ with the boostrap distribution $\frac{(NT)^{1/2}}{\hat{S}^*_{NT,sel}}\left(\bar{Y}^*_{NT}- \bar{Y}_{NT}\right)$
\end{itemize}
And we can establish the consistency
\begin{theorem}{Bootstrap Consistency}{bootstrap_consistency}
    Under Assumption \ref{assump:integrability}, 
    
\end{theorem}

\section*{A Theoretical}
\citet{chiang2023using}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}