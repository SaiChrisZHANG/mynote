\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{5}{Two-Way Cluster-Robust (TWCR) Standard Errors}{}{Sai Zhang}{The validity of Two-Way Cluster-Robust (TWCR) standard errors}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{One-Way Clustering}
First, consider the case of one-way clustering. The linear model with one-way clustering $$ y_{ig} = \mathbf{x}_{ig}\boldsymbol{\beta} + u_{ig} $$
where $i$ denotes the $i$th of the $N$ individuals in the sample, $j$ denotes the $g$th of the $G$ clusters, assume that
\begin{itemize}
    \item $\mathbb{E}\left[u_{ig}\mid \mathbf{x}_{ig}\right] =0$
    \item error independence across clusters: for $i\neq j$
    \begin{equation}\label{eq:error_independence}
        \mathbb{E}\left[ u_{ig} u_{jg'}\mid \mathbf{x}_{ig},\mathbf{x}_{jg'} \right] = 0
    \end{equation}
    unless $g=g'$, that is, errors for individuals within the same cluster may be correlated.
\end{itemize}
Grouping observations by cluster, get
$$
\mathbf{y}_g = \mathbf{X}_g \boldsymbol{\beta} + \mathbf{u}
$$
where $\mathbf{X}_g$ has dimension $N_g\times K$ and $\mathbf{y}_g$ has dimension $N_g \times 1$, with $N_g$ observations in cluster $g$. 
Stacking over cluster, get the matrix form of the model
$$
\mathbf{y=X}\boldsymbol{\beta}+\mathbf{u}
$$
with $\mathbf{y,u}$ being $N\times 1$ vectors, $\mathbf{X}$ being an $N\times K$ matrix. OLS estimator gives 
\begin{equation}\label{eq:OLSest}
    \hat{\boldsymbol{\beta}} = \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}=\left( \sum^G_{g=1}\mathbf{X}_g'\mathbf{X}_g \right)^{-1} \sum^G_{g=1}\mathbf{X}'_g\mathbf{y}_g
\end{equation}
then, by CLT, we have that $\sqrt{G} \left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right) \xrightarrow{d} \mathcal{N}(0,\boldsymbol{\Sigma})$ where the variance matrix of the limit normal distribution $\boldsymbol{\Sigma}$ is 
\begin{equation}\label{eq:limit_varcovmat}
    \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right] \right)^{-1} \left(\lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{u}'_g\mathbf{u}_g\mathbf{X}_g\right] \right) \times \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right]  \right)^{-1}
\end{equation}
If the primary source of clustering is due to group-level common shocks, a useful approximation is that for the $j$th regressor, the default OLS variance estimate based on $s^2 \left(\mathbf{X'X}\right)^{-1}$ should be inflated by $\tau_j \simeq 1+\rho_{x_j}\rho_u\left(\bar{N}_g -1\right)$, where 
\begin{itemize}
    \item $s$ is the estimated standard deviation of the error
    \item $\rho_{x_j}$ is a measure of within-cluster correlation of $x_j$
    \item $\rho_u$ is the within-cluster error correlation 
    \item $\bar{N}_g$ is the average cluster size
\end{itemize}
It's easy to see the $\tau_j$ can be large even with small $\rho_u$ \citep{kloek1981ols,scott1982effect,moulton1990illustration}. If assume the model for the cluster error variance matrices $\boldsymbol{\Omega}_g = \mathbb{V}\left[\mathbf{u}_g \mid \mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{u}_g\mathbf{u}_g'\mid \mathbf{X}_g\right]$, 
and there is a consistent estimate $\hat{\boldsymbol{\Omega}}_g$ of $\boldsymbol{\Omega}_g$, we can estimate $\mathbb{E}\left[\mathbf{X}_g'\mathbf{u}_g\mathbf{u}_g'\mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{X}_g'\boldsymbol{\Omega}_g \mathbf{X}_g\right]$ via GLS.

\paragraph*{Cluster-robust variance matrix estimate} consider 
\begin{equation}\label{eq:oneway_clurob}
    \hat{\mathbb{V}} \left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\left(\sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g \right) \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where $\hat{\mathbf{u}}_g = \mathbf{y}_g - \mathbf{X}_g\hat{\boldsymbol{\beta}}$. This estimate is consistent if $$ G^{-1}\sum^G_{g=1}\mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g - G^{-1}\sum^G_{g=1}\mathbb{E}\left[ \mathbf{X}_g' \mathbf{u}_g \mathbf{u}_g' \mathbf{X}_g \right] \xrightarrow{\mathrm{p}} \mathbf{0} $$ as $G\rightarrow \infty$. 
An informal presentation of Eq.(\ref{eq:oneway_clurob}) is to rewrite the central matrix as 
\begin{equation}\label{eq:onewayclu_centralmat}
    \hat{\mathbf{B}} = \sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g = \mathbf{X}'\begin{bmatrix}
        \hat{\mathbf{u}}_1\hat{\mathbf{u}}_1' & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \hat{\mathbf{u}}_2\hat{\mathbf{u}}_2' & & \vdots \\
        \vdots & & \ddots & \mathbf{0} \\
        \mathbf{0} & \cdots & & \hat{\mathbf{u}}_G\hat{\mathbf{u}}_G'
    \end{bmatrix}\mathbf{X} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right) \mathbf{X} 
\end{equation}
where $\otimes$ denotes element-wise multiplication. The $(p,q)$th element of this matrix is 
\begin{equation*}
    \sum^N_{i=1}\sum^N_{j=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1}\left(i,j\text{ in the same cluster}\right)
\end{equation*}
with $\hat{u}_i = y_i - \mathbf{x}'_i \hat{\boldsymbol{\beta}}$.

$\mathbf{S}^G$ is an $N\times N$ indicator matrix with $\mathbf{S}_{ij}^G=1$ only if the $i$th and $j$th observation belong to the same cluster: it zeros out a large amount of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ (asymptotically equivalently, ${\mathbf{u}}{\mathbf{u}}'$), specifically, only $\sum^G_{g=1}N_g^2$ out of $N^2 = \left(\sum^G_{g=1}N_g\right)^2$ terms are not zero (sub-matrices on the diagonal). Asymptotically
\begin{itemize}
    \item for fixed $N_g$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g}\xrightarrow{G\rightarrow\infty}0$
    \item for balanced clusters $N_g = N/G$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g} = \frac{1}{G} \xrightarrow{G\rightarrow\infty}0$
\end{itemize}


A strand of literature popularizes 
this method:
\begin{itemize}
    \item \citet{liang1986longitudinal}: in a generalized estimatin equations setting
    \item \citet{arellano1987computing}: fixed effects estimator in linear panel models
    \item \citet{hansen2007asymptotic}: asymptotic theory for panel data where $T\rightarrow\infty$ in addition to $N\rightarrow\infty$ (or $N_g\rightarrow\infty$ in addition to $G\rightarrow\infty$ in the notation above).
\end{itemize}

\section{Two-Way Clustering}
Now, consider the case of two-way clustering, 
$$
y_{i,gh} = \mathbf{x}'_{i,gh}\boldsymbol{\beta} + u
$$
where each observation may belong to \textbf{two} dimension of groups: group $g\in \left\{1,\cdots,G\right\}$ and $h\in \left\{ 1,\cdots,H \right\}$, and for $i\neq j$
\begin{equation}\label{eq:twoway_errors}
    \mathbb{E} \left[ u_{i,gh} u_{j,g'h'} \mid \mathbf{x}_{i,gh},\mathbf{j,g'h'} \right] = 0
\end{equation}
unless $g=g'$ or $h=h'$, that is, errors for individuals within the same group (along either $g$ or $h$) may be correlated.

\paragraph*{Cluster-robust variance matrix estimate} extending the one-way clustering case, keep elements of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ where the $i$th and $j$th observations share a cluster in \myhl[myblue]{\textbf{any}} dimension, then similar to Eq.(\ref{eq:onewayclu_centralmat})
\begin{equation}\label{eq:twowayclu_centermat}
    \hat{\mathbf{B}} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH}\right)\mathbf{X}
\end{equation}
here $\mathbf{S}^{GH}$ is an $N\times N$ indicator matrix with $\mathbf{S}^{GH}_{ij}=1$ only if the $i$th and $j$th observation share any cluster, the $(p,q)$th element of this matrix is 
$$
\sum^N_{i=1}\sum^N_{i=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1} \left(i,j\text{ share any cluster}\right)
$$
$\hat{\mathbf{B}}$ can also be presented in one-way cluster-robust fashion:
\begin{align}\label{eq:twowayclu_centmat}
    \hat{\mathbf{B}} &= \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH} \right)\mathbf{X} = \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right)\mathbf{X} + \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^H \right)\mathbf{X} - \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{G\cap H} \right)\mathbf{X}
\end{align}
where $\mathbf{G}^{GH} =\mathbf{G}^G+ \mathbf{G}^H - \mathbf{G}^{G\cap H} $, with 
\begin{itemize}
    \item $\mathbf{G}^G$: $\mathbf{G}^G_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $g\in \left\{1,2,\cdots,G\right\}$
    \item $\mathbf{G}^H$: $\mathbf{G}^H_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $h\in \left\{1,2,\cdots,H\right\}$
    \item $\mathbf{G}^{G\cap H}$: $\mathbf{G}^{G\cap H}_{ij}=1$ only if the $i$th and $j$th observation belong to \textbf{both} the same cluster $g\in \left\{1,2,\cdots,G\right\}$ and the same cluster $h\in \left\{1,2,\cdots,H\right\}$
\end{itemize}
then, similar to one-way clustering case,
\begin{align}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] =& \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^G\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &+ \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^H\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &- \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^{G\cap H}\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1}
\end{align}
that is,
\begin{equation}\label{eq:twowayclu_decomp}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \hat{\mathbb{V}}^G\left[\hat{\boldsymbol{\beta}}\right] + \hat{\mathbb{V}}^H\left[\hat{\boldsymbol{\beta}}\right] - \hat{\mathbb{V}}^{G\cap H}\left[\hat{\boldsymbol{\beta}}\right]
\end{equation}
each of Eq.(\ref{eq:twowayclu_decomp}) can be separately computed by OLS of $\mathbf{y}$ on $\mathbf{X}$, with variance matrix estimates $\hat{\mathbb{V}}$ based on 
\begin{itemize}
    \item[i] clustering on $g\in \left\{1,2,\cdots, G\right\}$
    \item[ii] clustering on $h \in \left\{1,2,\cdots, H\right\}$
    \item[iii] clustering on $(g,h)\in \left\{(1,1),\cdots,(G,H)\right\}$
\end{itemize}

\paragraph*{Practical considerations} It is required to know what \textit{ways} will be potentially important for clustering, which can be tested via checking the dimension of correlations in the errors. There are several ways to test 
\begin{itemize}
    \item estimate sample covariances of $\mathbf{X}'\hat{\mathbf{u}}$ within dimensions, test the null that the \myhl[myblue]{\textbf{average}} of such covariances is 0: rejecting this null is sufficient (not necessary) to reject the null of no clustering \citep{white1980heteroskedasticity}
    \item for \myhl[myblue]{\textbf{small samples}}, Eq. (\ref{eq:oneway_clurob}) is baised downwards. This is corrected (in Stata) by replacing $\hat{\mathbf{u}}_g$ with $\sqrt{c}\hat{\mathbf{u}}_g$, where $c = \frac{G}{G-1}\frac{N-1}{N-K}\simeq \frac{G}{G-1}$. For two-way clustering (Eq. \ref{eq:twowayclu_centmat}), there are 2 ways of correction:
    \begin{itemize}
        \item choose correction terms for each of the 3 components: $$c_1= \frac{G}{G-1}\frac{N-1}{N-K}, c_2= \frac{H}{H-1}\frac{N-1}{N-K},c_3=\frac{I}{I-1}\frac{N-1}{N-K}$$ with $I$ being the number of unique clusters determined by $G\cap H$
        \item choose a constant terms for all components: $$c=\frac{J}{J-1}\frac{N-1}{N-K}$$ with $J=\min(G,H)$
    \end{itemize}
    \item \myhl[myblue]{\textbf{Var-cov matrix not positive-semidefinite}}: $\hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right]$ might have negative elements on the diagonal (Eq. \ref{eq:twowayclu_decomp}), informly, this is more likely to  arise when clustering is done over the same groups as the fixed effects. One way to address this issue is using \textit{eigendecomposition} technique:
    $$
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}'
    $$
    where 
    \begin{itemize}
        \item $\mathbf{U}$ containing the eigenvectors of $\hat{\mathbf{V}}$
        \item $\boldsymbol{\Lambda} = \mathrm{diag}\left[\lambda_1,\cdots,\lambda_d\right]$ contains the eigenvalues of $\hat{\mathbf{V}}$ 
    \end{itemize}
    then create $\boldsymbol{\Lambda}^+ = \mathrm{diag} \left[\lambda_1^+,\cdots,\lambda_d^+\right]$ with $\lambda^+_j=\max\left(0,\lambda_j\right)$ and use $\hat{\mathbf{V}}^+\left[\hat{\boldsymbol{\beta}}\right]=\mathbf{U}\boldsymbol{\Lambda}^+\mathbf{U}'$ as the estimate
\end{itemize}

\section{Multiway Clustering}
\citet{cameron2011robust} extended the framework\footnote{Also proposed by \citet{thompson2011simple}.} to allow clustering in $D$ dimensions, then we can do the following reframing
\begin{itemize}
    \item $G_d$: the number of clusters in dimension $d\in \left\{ 1,2,\cdots,D \right\}$
    \item $D-$vector $\boldsymbol{\delta}_i = \boldsymbol{\delta}(i)$, with funciton $\boldsymbol{\delta}:\left\{1,2,\cdots,N\right\}\rightarrow \bigtimes ^D_{d=1}\left\{1,2,\cdots, G_d\right\}$ lists the cluster membership in each dimension of each observation
\end{itemize}
then we have 
$$
\mathbf{1}\left[i,j\text{ shares a clustser}\right] = 1 \Leftrightarrow \delta_{id}=\delta_{jd}
$$
for some $d\in \left\{1,2, \cdots, D\right\}$, where $\delta_{id}$ denotes the $d$th element of $\boldsymbol{\delta}_i$. Also 
\begin{itemize}
    \item $D-$vector $\mathbf{r}$: define the set $$ R\equiv \left\{ \mathbf{r}:r_d\in\left\{0,1\right\},d=1,2,\cdots,D,\mathbf{r\neq 0} \right\} $$
    elements of the set $R$ can be used to index all cases where 2 observations share a cluster in at least one dimension. Define the function 
    $$
    \mathbf{I_r} (i,j) \equiv \mathbf{1} \left[r_d\delta_{id} = r_d \delta_{jd},\forall d\right]
    $$
    which indicates whether observations $i$ and $j$ have identical cluster menbership for \myhl[myblue]{\textbf{all}} dimensions $d$ s.t. $r_d=1$.
    Then we have a \textit{aggregate} identifier
    $$
    \mathbf{I}(i,j) = 1 \Leftrightarrow \mathbf{I_r}(i,j)=1\text{ for some }\mathbf{r}\in R
    $$
    i.e., 2 observations share \myhl[myblue]{\textbf{at least}} one dimension.
\end{itemize}
The define the $2^D-1$ matrices
\begin{equation}\label{eq:centermat_multiclu}
    \tilde{\mathbf{B}}_{\mathbf{r}}\equiv \sum^N_{i=1}\sum^N_{j=1}\mathbf{x}_i\mathbf{x}_j' \hat{u}_i\hat{u}_j\mathbf{I_r}(i,j)
\end{equation}
with $\mathbf{r}\in R$.

\paragraph*{Var-cov matrix estimator} consider, similarly, an estimator 
\begin{equation}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\tilde{\mathbf{B}} \left(\mathbf{X'X}\right)^{-1} \equiv  \left(\mathbf{X'X}\right)^{-1}  \left(\sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r \right)  \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where cases of clustering on an odd number of dimensions are added, those of clustering on an even number of dimensions are subtracted. Consider the case of $D=3$,
\begin{equation*}
    \left(\tilde{\mathbf{B}}_{(1,0,0)} + \tilde{\mathbf{B}}_{(0,1,0)} + \tilde{\mathbf{B}}_{(0,0,1)} \right) - \left( \tilde{\mathbf{B}}_{(1,1,0)} + \tilde{\mathbf{B}}_{(1,0,1)} + \tilde{\mathbf{B}}_{(0,1,1)} \right) + \tilde{\mathbf{B}}_{(1,1,1)}
\end{equation*}
$\tilde{\mathbf{B}}$ is identical to $\hat{\mathbf{B}}$ defined analogically as in Eq.(\ref{eq:twowayclu_centmat}), since 
\begin{itemize}
    \item no observation pair with $\mathbf{I}(i,j)=0$: this is immediate, since $\mathbf{I}(i,j)=0 \Leftrightarrow \mathbf{I_r}(i,j)=0,\forall \mathbf{r}$
    \item the covariance term corresponding to each observation pair with $\mathbf{I}(i,j)=1$ is included \myhl[myblue]{\textbf{exactly once}} in $\tilde{\mathbf{B}} $: by inclusion-exclusion principle for set cardinality
    $$\mathbf{I}(i,j) \Rightarrow \sum_{\left\Vert \mathbf{r} \right\Vert=k,\mathbf{r}\in R}(-1)^{k+1}\mathbf{I_r}(i,j)=1 $$
\end{itemize} 

\paragraph*{Curse of dimensionality} this could arise in a setting with \textbf{many dimensions} of clustering, and in which one or more dimensions have \textbf{few} clusters\footnote{The square design (each dimension has the same number of clusters) with orthogonal dimensions has the \textbf{least} independence of observations.}.
\citet{cameron2011robust} suggested an ad-hoc rule of thumb for approximating sufficient numbers of clusters.

\subsection{Non-linear Estimators}
\paragraph*{$m$-Estimators}
Consider an $m$-estimator that solves
$$
\sum^N_{i=1}\mathbf{h}_i\left(\hat{\boldsymbol{\theta}}\right) = \mathbf{0}
$$
under standard assumptions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal with estimated variance matrix 
\begin{equation}\label{eq:m-est_varmat}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \hat{\mathbf{A}}^{-1}\hat{\mathbf{B}}{\hat{\mathbf{A}^{\prime}}^{-1}}
\end{equation}
where $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}}$ and $\hat{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i\mathbf{h}_i\right]$.

\begin{itemize}
    \item \myhl[myblue]{\textbf{one-way clustering}} $\hat{\mathbf{B}} = \sum^G_{g=1}\hat{\mathbf{h}}_g\hat{\mathbf{h}}_g'$ where $\hat{\mathbf{h}}_g = \sum^{N_g}_{i=1}\hat{\mathbf{h}}_{ig}$, clustering may not lead to parameter inconsistency, depending on whether $\mathbb{E}\left[\mathbf{h}_i(\boldsymbol{\theta})\right]= \mathbf{0}$ with clustering
    \begin{itemize}
        \item \textbf{population-averaged approach}: assum $\mathbf{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi \left( \mathbf{x}_{ig}'\boldsymbol{\beta} \right)$
        \item \textbf{random effects approach}: let $y_{ig}=1$ if $y^*_{ig} > 0$ where $y^*_{ig}=\mathbf{x}_{ig}'\boldsymbol{\beta}+\epsilon_g + \epsilon_{ig}$, where 
        \begin{itemize}
            \item idiosyncratic error $\epsilon_{ig}\sim \mathcal{N}(0,1)$
            \item cluster-specific error $\epsilon_g \sim \mathcal{N}(0,\sigma^2_g)$
        \end{itemize}
        then we have the alternative moment condition $$ \mathbb{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi\left(\frac{\mathbf{x}_{ig}'\boldsymbol{\beta}}{\sqrt{1+\sigma^2_g}}\right) $$ 
    \end{itemize}
    \item \myhl[myblue]{\textbf{multiway clustering}} replacing $\hat{u}_i\mathbf{x}_i$ in Eq.(\ref{eq:centermat_multiclu}) with $\hat{\mathbf{h}}_i$, then we have 
    \begin{align*}
        \hat{\mathbb{V}} \left[\hat{\boldsymbol{\theta}}\right] &= \hat{\mathbf{A}}^{-1} \tilde{\mathbf{B}} \hat{\mathbf{A}'}^{-1}
    \end{align*}
    where 
    \begin{align*}
        \hat{\mathbf{A}} &= \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} & \tilde{\mathbf{B}} &= \sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r & \tilde{\mathbf{B}}_r &\equiv \sum^N_{i=1}\sum^N_{j=1}\hat{\mathbf{h}}_i\hat{\mathbf{h}'}_j\mathbb{I}_{\mathbf{r}}(i,j)
    \end{align*}
    with $\mathbf{r}\in R$\footnote{This multiway clustering can be implemented using several one-way clustered bootstraps. Each of the one-way cluster robust matrices is estimated by a pairs cluster bootstrap that resamples with replacement from the appropriate cluster dimension. They are then combined as if they had been estimated analytically \citep{cameron2011robust}.}.
\end{itemize} 

\paragraph*{GMM estimation} Consider an example of over-identified models: linear two stage least squares with more instruments than endogenous regressors, we have 
$$
\hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}) =  \arg\min_{\boldsymbol{\theta}} \left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)'\mathbf{W}\left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)
$$
where $\mathbf{W}$ is a symmetric positive definite weighting matrix. Under standard regularity conditions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal, with estimated variance matrix 
\begin{equation*}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}\hat{\mathbf{A}}'\mathbf{W}\tilde{\mathbf{B}}\mathbf{W}\hat{\mathbf{A}}\left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}
\end{equation*}
again, $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} $, and $\tilde{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i \mathbf{h}_i\right]$.

\section{Menzel (2021): Asymptotic Gaussianity}
One key of TWCR inference is the asymptotic Gaussianity, \citet{menzel2021bootstrap} pointed out the potential non-Gaussianity of the limit distribution.
Still, consider a random array ($Y_{it}$) indexed by two dimensions by $i=1,\cdots,N$ and $t=1,\cdots,T$. Clusters are sampled independently at random from an infinite population,
but otherwise \textbf{unrestricted} in dependence within each row $\mathbf{Y}_{i\cdot} \coloneq \left(Y_{i1}\cdots,Y_{iT}\right)$ and within each column $\mathbf{Y}_{\cdot t}\coloneq \left(Y_{1t},\cdots,Y_{Nt}\right)$.

\subsection{Distribution of Sample Average}
First, consider 
$$
\bar{Y}_{NT} \coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y_{it}
$$
and approximate the asymptotic distribution regardless of whether, or what type of, cluster-dependence is present.

\section*{A Theoretical}
\citet{chiang2023using}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}