\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{5}{Two-Way Cluster-Robust (TWCR) Standard Errors}{}{Sai Zhang}{The validity of Two-Way Cluster-Robust (TWCR) standard errors}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{One-Way Clustering}
First, consider the case of one-way clustering. The linear model with one-way clustering $$ y_{ig} = \mathbf{x}_{ig}\boldsymbol{\beta} + u_{ig} $$
where $i$ denotes the $i$th of the $N$ individuals in the sample, $j$ denotes the $g$th of the $G$ clusters, assume that
\begin{itemize}
    \item $\mathbb{E}\left[u_{ig}\mid \mathbf{x}_{ig}\right] =0$
    \item error independence across clusters: for $i\neq j$
    \begin{equation}\label{eq:error_independence}
        \mathbb{E}\left[ u_{ig} u_{jg'}\mid \mathbf{x}_{ig},\mathbf{x}_{jg'} \right] = 0
    \end{equation}
    unless $g=g'$, that is, errors for individuals within the same cluster may be correlated.
\end{itemize}
Grouping observations by cluster, get
$$
\mathbf{y}_g = \mathbf{X}_g \boldsymbol{\beta} + \mathbf{u}
$$
where $\mathbf{X}_g$ has dimension $N_g\times K$ and $\mathbf{y}_g$ has dimension $N_g \times 1$, with $N_g$ observations in cluster $g$. 
Stacking over cluster, get the matrix form of the model
$$
\mathbf{y=X}\boldsymbol{\beta}+\mathbf{u}
$$
with $\mathbf{y,u}$ being $N\times 1$ vectors, $\mathbf{X}$ being an $N\times K$ matrix. OLS estimator gives 
\begin{equation}\label{eq:OLSest}
    \hat{\boldsymbol{\beta}} = \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}=\left( \sum^G_{g=1}\mathbf{X}_g'\mathbf{X}_g \right)^{-1} \sum^G_{g=1}\mathbf{X}'_g\mathbf{y}_g
\end{equation}
then, by CLT, we have that $\sqrt{G} \left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right) \xrightarrow{d} \mathcal{N}(0,\boldsymbol{\Sigma})$ where the variance matrix of the limit normal distribution $\boldsymbol{\Sigma}$ is 
\begin{equation}\label{eq:limit_varcovmat}
    \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right] \right)^{-1} \left(\lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{u}'_g\mathbf{u}_g\mathbf{X}_g\right] \right) \times \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right]  \right)^{-1}
\end{equation}
If the primary source of clustering is due to group-level common shocks, a useful approximation is that for the $j$th regressor, the default OLS variance estimate based on $s^2 \left(\mathbf{X'X}\right)^{-1}$ should be inflated by $\tau_j \simeq 1+\rho_{x_j}\rho_u\left(\bar{N}_g -1\right)$, where 
\begin{itemize}
    \item $s$ is the estimated standard deviation of the error
    \item $\rho_{x_j}$ is a measure of within-cluster correlation of $x_j$
    \item $\rho_u$ is the within-cluster error correlation 
    \item $\bar{N}_g$ is the average cluster size
\end{itemize}
It's easy to see the $\tau_j$ can be large even with small $\rho_u$ \citep{kloek1981ols,scott1982effect,moulton1990illustration}. If assume the model for the cluster error variance matrices $\boldsymbol{\Omega}_g = \mathbb{V}\left[\mathbf{u}_g \mid \mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{u}_g\mathbf{u}_g'\mid \mathbf{X}_g\right]$, 
and there is a consistent estimate $\hat{\boldsymbol{\Omega}}_g$ of $\boldsymbol{\Omega}_g$, we can estimate $\mathbb{E}\left[\mathbf{X}_g'\mathbf{u}_g\mathbf{u}_g'\mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{X}_g'\boldsymbol{\Omega}_g \mathbf{X}_g\right]$ via GLS.

\paragraph*{Cluster-robust variance matrix estimate} consider 
\begin{equation}\label{eq:oneway_clurob}
    \hat{\mathbb{V}} \left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\left(\sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g \right) \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where $\hat{\mathbf{u}}_g = \mathbf{y}_g - \mathbf{X}_g\hat{\boldsymbol{\beta}}$. This estimate is consistent if $$ G^{-1}\sum^G_{g=1}\mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g - G^{-1}\sum^G_{g=1}\mathbb{E}\left[ \mathbf{X}_g' \mathbf{u}_g \mathbf{u}_g' \mathbf{X}_g \right] \xrightarrow{\mathrm{p}} \mathbf{0} $$ as $G\rightarrow \infty$. 
An informal presentation of Eq.(\ref{eq:oneway_clurob}) is to rewrite the central matrix as 
\begin{equation}\label{eq:onewayclu_centralmat}
    \hat{\mathbf{B}} = \sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g = \mathbf{X}'\begin{bmatrix}
        \hat{\mathbf{u}}_1\hat{\mathbf{u}}_1' & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \hat{\mathbf{u}}_2\hat{\mathbf{u}}_2' & & \vdots \\
        \vdots & & \ddots & \mathbf{0} \\
        \mathbf{0} & \cdots & & \hat{\mathbf{u}}_G\hat{\mathbf{u}}_G'
    \end{bmatrix}\mathbf{X} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right) \mathbf{X} 
\end{equation}
where $\otimes$ denotes element-wise multiplication. The $(p,q)$th element of this matrix is 
\begin{equation*}
    \sum^N_{i=1}\sum^N_{j=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1}\left(i,j\text{ in the same cluster}\right)
\end{equation*}
with $\hat{u}_i = y_i - \mathbf{x}'_i \hat{\boldsymbol{\beta}}$.

$\mathbf{S}^G$ is an $N\times N$ indicator matrix with $\mathbf{S}_{ij}^G=1$ only if the $i$th and $j$th observation belong to the same cluster: it zeros out a large amount of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ (asymptotically equivalently, ${\mathbf{u}}{\mathbf{u}}'$), specifically, only $\sum^G_{g=1}N_g^2$ out of $N^2 = \left(\sum^G_{g=1}N_g\right)^2$ terms are not zero (sub-matrices on the diagonal). Asymptotically
\begin{itemize}
    \item for fixed $N_g$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g}\xrightarrow{G\rightarrow\infty}0$
    \item for balanced clusters $N_g = N/G$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g} = \frac{1}{G} \xrightarrow{G\rightarrow\infty}0$
\end{itemize}


A strand of literature popularizes 
this method:
\begin{itemize}
    \item \citet{liang1986longitudinal}: in a generalized estimatin equations setting
    \item \citet{arellano1987computing}: fixed effects estimator in linear panel models
    \item \citet{hansen2007asymptotic}: asymptotic theory for panel data where $T\rightarrow\infty$ in addition to $N\rightarrow\infty$ (or $N_g\rightarrow\infty$ in addition to $G\rightarrow\infty$ in the notation above).
\end{itemize}

\section{Two-Way Clustering}
Now, consider the case of two-way clustering, 
$$
y_{i,gh} = \mathbf{x}'_{i,gh}\boldsymbol{\beta} + u
$$
where each observation may belong to \textbf{two} dimension of groups: group $g\in \left\{1,\cdots,G\right\}$ and $h\in \left\{ 1,\cdots,H \right\}$, and for $i\neq j$
\begin{equation}\label{eq:twoway_errors}
    \mathbb{E} \left[ u_{i,gh} u_{j,g'h'} \mid \mathbf{x}_{i,gh},\mathbf{j,g'h'} \right] = 0
\end{equation}
unless $g=g'$ or $h=h'$, that is, errors for individuals within the same group (along either $g$ or $h$) may be correlated.

\paragraph*{Cluster-robust variance matrix estimate} extending the one-way clustering case, keep elements of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ where the $i$th and $j$th observations share a cluster in \myhl[myblue]{\textbf{any}} dimension, then similar to Eq.(\ref{eq:onewayclu_centralmat})
\begin{equation}\label{eq:twowayclu_centermat}
    \hat{\mathbf{B}} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH}\right)\mathbf{X}
\end{equation}
here $\mathbf{S}^{GH}$ is an $N\times N$ indicator matrix with $\mathbf{S}^{GH}_{ij}=1$ only if the $i$th and $j$th observation share any cluster, the $(p,q)$th element of this matrix is 
$$
\sum^N_{i=1}\sum^N_{i=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1} \left(i,j\text{ share any cluster}\right)
$$
$\hat{\mathbf{B}}$ can also be presented in one-way cluster-robust fashion:
\begin{align}\label{eq:twowayclu_centmat}
    \hat{\mathbf{B}} &= \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH} \right)\mathbf{X} = \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right)\mathbf{X} + \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^H \right)\mathbf{X} - \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{G\cap H} \right)\mathbf{X}
\end{align}
where $\mathbf{G}^{GH} =\mathbf{G}^G+ \mathbf{G}^H - \mathbf{G}^{G\cap H} $, with 
\begin{itemize}
    \item $\mathbf{G}^G$: $\mathbf{G}^G_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $g\in \left\{1,2,\cdots,G\right\}$
    \item $\mathbf{G}^H$: $\mathbf{G}^H_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $h\in \left\{1,2,\cdots,H\right\}$
    \item $\mathbf{G}^{G\cap H}$: $\mathbf{G}^{G\cap H}_{ij}=1$ only if the $i$th and $j$th observation belong to \textbf{both} the same cluster $g\in \left\{1,2,\cdots,G\right\}$ and the same cluster $h\in \left\{1,2,\cdots,H\right\}$
\end{itemize}
then, similar to one-way clustering case,
\begin{align}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] =& \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^G\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &+ \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^H\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &- \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^{G\cap H}\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1}
\end{align}
that is,
\begin{equation}\label{eq:twowayclu_decomp}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \hat{\mathbb{V}}^G\left[\hat{\boldsymbol{\beta}}\right] + \hat{\mathbb{V}}^H\left[\hat{\boldsymbol{\beta}}\right] - \hat{\mathbb{V}}^{G\cap H}\left[\hat{\boldsymbol{\beta}}\right]
\end{equation}
each of Eq.(\ref{eq:twowayclu_decomp}) can be separately computed by OLS of $\mathbf{y}$ on $\mathbf{X}$, with variance matrix estimates $\hat{\mathbb{V}}$ based on 
\begin{itemize}
    \item[i] clustering on $g\in \left\{1,2,\cdots, G\right\}$
    \item[ii] clustering on $h \in \left\{1,2,\cdots, H\right\}$
    \item[iii] clustering on $(g,h)\in \left\{(1,1),\cdots,(G,H)\right\}$
\end{itemize}

\paragraph*{Practical considerations} It is required to know what \textit{ways} will be potentially important for clustering, which can be tested via checking the dimension of correlations in the errors. There are several ways to test 
\begin{itemize}
    \item estimate sample covariances of $\mathbf{X}'\hat{\mathbf{u}}$ within dimensions, test the null that the \myhl[myblue]{\textbf{average}} of such covariances is 0: rejecting this null is sufficient (not necessary) to reject the null of no clustering \citep{white1980heteroskedasticity}
    \item for \myhl[myblue]{\textbf{small samples}}, Eq. (\ref{eq:oneway_clurob}) is baised downwards. This is corrected (in Stata) by replacing $\hat{\mathbf{u}}_g$ with $\sqrt{c}\hat{\mathbf{u}}_g$, where $c = \frac{G}{G-1}\frac{N-1}{N-K}\simeq \frac{G}{G-1}$. For two-way clustering (Eq. \ref{eq:twowayclu_centmat}), there are 2 ways of correction:
    \begin{itemize}
        \item choose correction terms for each of the 3 components: $$c_1= \frac{G}{G-1}\frac{N-1}{N-K}, c_2= \frac{H}{H-1}\frac{N-1}{N-K},c_3=\frac{I}{I-1}\frac{N-1}{N-K}$$ with $I$ being the number of unique clusters determined by $G\cap H$
        \item choose a constant terms for all components: $$c=\frac{J}{J-1}\frac{N-1}{N-K}$$ with $J=\min(G,H)$
    \end{itemize}
    \item \myhl[myblue]{\textbf{Var-cov matrix not positive-semidefinite}}: $\hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right]$ might have negative elements on the diagonal (Eq. \ref{eq:twowayclu_decomp}), informly, this is more likely to  arise when clustering is done over the same groups as the fixed effects. One way to address this issue is using \textit{eigendecomposition} technique:
    $$
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}'
    $$
    where 
    \begin{itemize}
        \item $\mathbf{U}$ containing the eigenvectors of $\hat{\mathbf{V}}$
        \item $\boldsymbol{\Lambda} = \mathrm{diag}\left[\lambda_1,\cdots,\lambda_d\right]$ contains the eigenvalues of $\hat{\mathbf{V}}$ 
    \end{itemize}
    then create $\boldsymbol{\Lambda}^+ = \mathrm{diag} \left[\lambda_1^+,\cdots,\lambda_d^+\right]$
\end{itemize}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}