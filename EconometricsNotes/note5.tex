\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{5}{Two-Way Cluster-Robust (TWCR) Standard Errors}{}{Sai Zhang}{The validity of Two-Way Cluster-Robust (TWCR) standard errors}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{One-Way Clustering}\label{sec:oneway_cluster}
First, consider the case of one-way clustering. The linear model with one-way clustering $$ y_{ig} = \mathbf{x}_{ig}\boldsymbol{\beta} + u_{ig} $$
where $i$ denotes the $i$th of the $N$ individuals in the sample, $j$ denotes the $g$th of the $G$ clusters, assume that
\begin{itemize}
    \item $\mathbb{E}\left[u_{ig}\mid \mathbf{x}_{ig}\right] =0$
    \item error independence across clusters: for $i\neq j$
    \begin{equation}\label{eq:error_independence}
        \mathbb{E}\left[ u_{ig} u_{jg'}\mid \mathbf{x}_{ig},\mathbf{x}_{jg'} \right] = 0
    \end{equation}
    unless $g=g'$, that is, errors for individuals within the same cluster may be correlated.
\end{itemize}
Grouping observations by cluster, get
$$
\mathbf{y}_g = \mathbf{X}_g \boldsymbol{\beta} + \mathbf{u}
$$
where $\mathbf{X}_g$ has dimension $N_g\times K$ and $\mathbf{y}_g$ has dimension $N_g \times 1$, with $N_g$ observations in cluster $g$. 
Stacking over cluster, get the matrix form of the model
$$
\mathbf{y=X}\boldsymbol{\beta}+\mathbf{u}
$$
with $\mathbf{y,u}$ being $N\times 1$ vectors, $\mathbf{X}$ being an $N\times K$ matrix. OLS estimator gives 
\begin{equation}\label{eq:OLSest}
    \hat{\boldsymbol{\beta}} = \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}=\left( \sum^G_{g=1}\mathbf{X}_g'\mathbf{X}_g \right)^{-1} \sum^G_{g=1}\mathbf{X}'_g\mathbf{y}_g
\end{equation}
then, by CLT, we have that $\sqrt{G} \left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right) \xrightarrow{d} \mathcal{N}(0,\boldsymbol{\Sigma})$ where the variance matrix of the limit normal distribution $\boldsymbol{\Sigma}$ is 
\begin{equation}\label{eq:limit_varcovmat}
    \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right] \right)^{-1} \left(\lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{u}'_g\mathbf{u}_g\mathbf{X}_g\right] \right) \times \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right]  \right)^{-1}
\end{equation}
If the primary source of clustering is due to group-level common shocks, a useful approximation is that for the $j$th regressor, the default OLS variance estimate based on $s^2 \left(\mathbf{X'X}\right)^{-1}$ should be inflated by $\tau_j \simeq 1+\rho_{x_j}\rho_u\left(\bar{N}_g -1\right)$, where 
\begin{itemize}
    \item $s$ is the estimated standard deviation of the error
    \item $\rho_{x_j}$ is a measure of within-cluster correlation of $x_j$
    \item $\rho_u$ is the within-cluster error correlation 
    \item $\bar{N}_g$ is the average cluster size
\end{itemize}
It's easy to see the $\tau_j$ can be large even with small $\rho_u$ \citep{kloek1981ols,scott1982effect,moulton1990illustration}. If assume the model for the cluster error variance matrices $\boldsymbol{\Omega}_g = \mathbb{V}\left[\mathbf{u}_g \mid \mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{u}_g\mathbf{u}_g'\mid \mathbf{X}_g\right]$, 
and there is a consistent estimate $\hat{\boldsymbol{\Omega}}_g$ of $\boldsymbol{\Omega}_g$, we can estimate $\mathbb{E}\left[\mathbf{X}_g'\mathbf{u}_g\mathbf{u}_g'\mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{X}_g'\boldsymbol{\Omega}_g \mathbf{X}_g\right]$ via GLS.

\paragraph*{Cluster-robust variance matrix estimate} consider 
\begin{equation}\label{eq:oneway_clurob}
    \hat{\mathbb{V}} \left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\left(\sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g \right) \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where $\hat{\mathbf{u}}_g = \mathbf{y}_g - \mathbf{X}_g\hat{\boldsymbol{\beta}}$. This estimate is consistent if $$ G^{-1}\sum^G_{g=1}\mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g - G^{-1}\sum^G_{g=1}\mathbb{E}\left[ \mathbf{X}_g' \mathbf{u}_g \mathbf{u}_g' \mathbf{X}_g \right] \xrightarrow{\mathrm{p}} \mathbf{0} $$ as $G\rightarrow \infty$. 
An informal presentation of Eq.(\ref{eq:oneway_clurob}) is to rewrite the central matrix as 
\begin{equation}\label{eq:onewayclu_centralmat}
    \hat{\mathbf{B}} = \sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g = \mathbf{X}'\begin{bmatrix}
        \hat{\mathbf{u}}_1\hat{\mathbf{u}}_1' & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \hat{\mathbf{u}}_2\hat{\mathbf{u}}_2' & & \vdots \\
        \vdots & & \ddots & \mathbf{0} \\
        \mathbf{0} & \cdots & & \hat{\mathbf{u}}_G\hat{\mathbf{u}}_G'
    \end{bmatrix}\mathbf{X} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right) \mathbf{X} 
\end{equation}
where $\otimes$ denotes element-wise multiplication. The $(p,q)$th element of this matrix is 
\begin{equation*}
    \sum^N_{i=1}\sum^N_{j=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1}\left(i,j\text{ in the same cluster}\right)
\end{equation*}
with $\hat{u}_i = y_i - \mathbf{x}'_i \hat{\boldsymbol{\beta}}$.

$\mathbf{S}^G$ is an $N\times N$ indicator matrix with $\mathbf{S}_{ij}^G=1$ only if the $i$th and $j$th observation belong to the same cluster: it zeros out a large amount of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ (asymptotically equivalently, ${\mathbf{u}}{\mathbf{u}}'$), specifically, only $\sum^G_{g=1}N_g^2$ out of $N^2 = \left(\sum^G_{g=1}N_g\right)^2$ terms are not zero (sub-matrices on the diagonal). Asymptotically
\begin{itemize}
    \item for fixed $N_g$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g}\xrightarrow{G\rightarrow\infty}0$
    \item for balanced clusters $N_g = N/G$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g} = \frac{1}{G} \xrightarrow{G\rightarrow\infty}0$
\end{itemize}


A strand of literature popularizes 
this method:
\begin{itemize}
    \item \citet{liang1986longitudinal}: in a generalized estimatin equations setting
    \item \citet{arellano1987computing}: fixed effects estimator in linear panel models
    \item \citet{hansen2007asymptotic}: asymptotic theory for panel data where $T\rightarrow\infty$ in addition to $N\rightarrow\infty$ (or $N_g\rightarrow\infty$ in addition to $G\rightarrow\infty$ in the notation above).
\end{itemize}

\section{Two-Way Clustering}\label{sec:twoway_cluster}
Now, consider the case of two-way clustering, 
$$
y_{i,gh} = \mathbf{x}'_{i,gh}\boldsymbol{\beta} + u
$$
where each observation may belong to \textbf{two} dimension of groups: group $g\in \left\{1,\cdots,G\right\}$ and $h\in \left\{ 1,\cdots,H \right\}$, and for $i\neq j$
\begin{equation}\label{eq:twoway_errors}
    \mathbb{E} \left[ u_{i,gh} u_{j,g'h'} \mid \mathbf{x}_{i,gh},\mathbf{j,g'h'} \right] = 0
\end{equation}
unless $g=g'$ or $h=h'$, that is, errors for individuals within the same group (along either $g$ or $h$) may be correlated.

\paragraph*{Cluster-robust variance matrix estimate} extending the one-way clustering case, keep elements of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ where the $i$th and $j$th observations share a cluster in \myhl[myblue]{\textbf{any}} dimension, then similar to Eq.(\ref{eq:onewayclu_centralmat})
\begin{equation}\label{eq:twowayclu_centermat}
    \hat{\mathbf{B}} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH}\right)\mathbf{X}
\end{equation}
here $\mathbf{S}^{GH}$ is an $N\times N$ indicator matrix with $\mathbf{S}^{GH}_{ij}=1$ only if the $i$th and $j$th observation share any cluster, the $(p,q)$th element of this matrix is 
$$
\sum^N_{i=1}\sum^N_{i=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1} \left(i,j\text{ share any cluster}\right)
$$
$\hat{\mathbf{B}}$ can also be presented in one-way cluster-robust fashion:
\begin{align}\label{eq:twowayclu_centmat}
    \hat{\mathbf{B}} &= \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH} \right)\mathbf{X} = \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right)\mathbf{X} + \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^H \right)\mathbf{X} - \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{G\cap H} \right)\mathbf{X}
\end{align}
where $\mathbf{G}^{GH} =\mathbf{G}^G+ \mathbf{G}^H - \mathbf{G}^{G\cap H} $, with 
\begin{itemize}
    \item $\mathbf{G}^G$: $\mathbf{G}^G_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $g\in \left\{1,2,\cdots,G\right\}$
    \item $\mathbf{G}^H$: $\mathbf{G}^H_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $h\in \left\{1,2,\cdots,H\right\}$
    \item $\mathbf{G}^{G\cap H}$: $\mathbf{G}^{G\cap H}_{ij}=1$ only if the $i$th and $j$th observation belong to \textbf{both} the same cluster $g\in \left\{1,2,\cdots,G\right\}$ and the same cluster $h\in \left\{1,2,\cdots,H\right\}$
\end{itemize}
then, similar to one-way clustering case,
\begin{align}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] =& \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^G\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &+ \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^H\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &- \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^{G\cap H}\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1}
\end{align}
that is,
\begin{equation}\label{eq:twowayclu_decomp}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \hat{\mathbb{V}}^G\left[\hat{\boldsymbol{\beta}}\right] + \hat{\mathbb{V}}^H\left[\hat{\boldsymbol{\beta}}\right] - \hat{\mathbb{V}}^{G\cap H}\left[\hat{\boldsymbol{\beta}}\right]
\end{equation}
each of Eq.(\ref{eq:twowayclu_decomp}) can be separately computed by OLS of $\mathbf{y}$ on $\mathbf{X}$, with variance matrix estimates $\hat{\mathbb{V}}$ based on 
\begin{itemize}
    \item[i] clustering on $g\in \left\{1,2,\cdots, G\right\}$
    \item[ii] clustering on $h \in \left\{1,2,\cdots, H\right\}$
    \item[iii] clustering on $(g,h)\in \left\{(1,1),\cdots,(G,H)\right\}$
\end{itemize}

\paragraph*{Practical considerations} It is required to know what \textit{ways} will be potentially important for clustering, which can be tested via checking the dimension of correlations in the errors. There are several ways to test 
\begin{itemize}
    \item estimate sample covariances of $\mathbf{X}'\hat{\mathbf{u}}$ within dimensions, test the null that the \myhl[myblue]{\textbf{average}} of such covariances is 0: rejecting this null is sufficient (not necessary) to reject the null of no clustering \citep{white1980heteroskedasticity}
    \item for \myhl[myblue]{\textbf{small samples}}, Eq. (\ref{eq:oneway_clurob}) is baised downwards. This is corrected (in Stata) by replacing $\hat{\mathbf{u}}_g$ with $\sqrt{c}\hat{\mathbf{u}}_g$, where $c = \frac{G}{G-1}\frac{N-1}{N-K}\simeq \frac{G}{G-1}$. For two-way clustering (Eq. \ref{eq:twowayclu_centmat}), there are 2 ways of correction:
    \begin{itemize}
        \item choose correction terms for each of the 3 components: $$c_1= \frac{G}{G-1}\frac{N-1}{N-K}, c_2= \frac{H}{H-1}\frac{N-1}{N-K},c_3=\frac{I}{I-1}\frac{N-1}{N-K}$$ with $I$ being the number of unique clusters determined by $G\cap H$
        \item choose a constant terms for all components: $$c=\frac{J}{J-1}\frac{N-1}{N-K}$$ with $J=\min(G,H)$
    \end{itemize}
    \item \myhl[myblue]{\textbf{Var-cov matrix not positive-semidefinite}}: $\hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right]$ might have negative elements on the diagonal (Eq. \ref{eq:twowayclu_decomp}), informly, this is more likely to  arise when clustering is done over the same groups as the fixed effects. One way to address this issue is using \textit{eigendecomposition} technique:
    $$
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}'
    $$
    where 
    \begin{itemize}
        \item $\mathbf{U}$ containing the eigenvectors of $\hat{\mathbf{V}}$
        \item $\boldsymbol{\Lambda} = \mathrm{diag}\left[\lambda_1,\cdots,\lambda_d\right]$ contains the eigenvalues of $\hat{\mathbf{V}}$ 
    \end{itemize}
    then create $\boldsymbol{\Lambda}^+ = \mathrm{diag} \left[\lambda_1^+,\cdots,\lambda_d^+\right]$ with $\lambda^+_j=\max\left(0,\lambda_j\right)$ and use $\hat{\mathbf{V}}^+\left[\hat{\boldsymbol{\beta}}\right]=\mathbf{U}\boldsymbol{\Lambda}^+\mathbf{U}'$ as the estimate
\end{itemize}

\section{Multiway Clustering}
\citet{cameron2011robust} extended the framework\footnote{Also proposed by \citet{thompson2011simple}.} to allow clustering in $D$ dimensions, then we can do the following reframing
\begin{itemize}
    \item $G_d$: the number of clusters in dimension $d\in \left\{ 1,2,\cdots,D \right\}$
    \item $D-$vector $\boldsymbol{\delta}_i = \boldsymbol{\delta}(i)$, with funciton $\boldsymbol{\delta}:\left\{1,2,\cdots,N\right\}\rightarrow \bigtimes ^D_{d=1}\left\{1,2,\cdots, G_d\right\}$ lists the cluster membership in each dimension of each observation
\end{itemize}
then we have 
$$
\mathbf{1}\left[i,j\text{ shares a clustser}\right] = 1 \Leftrightarrow \delta_{id}=\delta_{jd}
$$
for some $d\in \left\{1,2, \cdots, D\right\}$, where $\delta_{id}$ denotes the $d$th element of $\boldsymbol{\delta}_i$. Also 
\begin{itemize}
    \item $D-$vector $\mathbf{r}$: define the set $$ R\equiv \left\{ \mathbf{r}:r_d\in\left\{0,1\right\},d=1,2,\cdots,D,\mathbf{r\neq 0} \right\} $$
    elements of the set $R$ can be used to index all cases where 2 observations share a cluster in at least one dimension. Define the function 
    $$
    \mathbf{I_r} (i,j) \equiv \mathbf{1} \left[r_d\delta_{id} = r_d \delta_{jd},\forall d\right]
    $$
    which indicates whether observations $i$ and $j$ have identical cluster menbership for \myhl[myblue]{\textbf{all}} dimensions $d$ s.t. $r_d=1$.
    Then we have a \textit{aggregate} identifier
    $$
    \mathbf{I}(i,j) = 1 \Leftrightarrow \mathbf{I_r}(i,j)=1\text{ for some }\mathbf{r}\in R
    $$
    i.e., 2 observations share \myhl[myblue]{\textbf{at least}} one dimension.
\end{itemize}
The define the $2^D-1$ matrices
\begin{equation}\label{eq:centermat_multiclu}
    \tilde{\mathbf{B}}_{\mathbf{r}}\equiv \sum^N_{i=1}\sum^N_{j=1}\mathbf{x}_i\mathbf{x}_j' \hat{u}_i\hat{u}_j\mathbf{I_r}(i,j)
\end{equation}
with $\mathbf{r}\in R$.

\paragraph*{Var-cov matrix estimator} consider, similarly, an estimator 
\begin{equation}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\tilde{\mathbf{B}} \left(\mathbf{X'X}\right)^{-1} \equiv  \left(\mathbf{X'X}\right)^{-1}  \left(\sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r \right)  \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where cases of clustering on an odd number of dimensions are added, those of clustering on an even number of dimensions are subtracted. Consider the case of $D=3$,
\begin{equation*}
    \left(\tilde{\mathbf{B}}_{(1,0,0)} + \tilde{\mathbf{B}}_{(0,1,0)} + \tilde{\mathbf{B}}_{(0,0,1)} \right) - \left( \tilde{\mathbf{B}}_{(1,1,0)} + \tilde{\mathbf{B}}_{(1,0,1)} + \tilde{\mathbf{B}}_{(0,1,1)} \right) + \tilde{\mathbf{B}}_{(1,1,1)}
\end{equation*}
$\tilde{\mathbf{B}}$ is identical to $\hat{\mathbf{B}}$ defined analogically as in Eq.(\ref{eq:twowayclu_centmat}), since 
\begin{itemize}
    \item no observation pair with $\mathbf{I}(i,j)=0$: this is immediate, since $\mathbf{I}(i,j)=0 \Leftrightarrow \mathbf{I_r}(i,j)=0,\forall \mathbf{r}$
    \item the covariance term corresponding to each observation pair with $\mathbf{I}(i,j)=1$ is included \myhl[myblue]{\textbf{exactly once}} in $\tilde{\mathbf{B}} $: by inclusion-exclusion principle for set cardinality
    $$\mathbf{I}(i,j) \Rightarrow \sum_{\left\Vert \mathbf{r} \right\Vert=k,\mathbf{r}\in R}(-1)^{k+1}\mathbf{I_r}(i,j)=1 $$
\end{itemize} 

\paragraph*{Curse of dimensionality} this could arise in a setting with \textbf{many dimensions} of clustering, and in which one or more dimensions have \textbf{few} clusters\footnote{The square design (each dimension has the same number of clusters) with orthogonal dimensions has the \textbf{least} independence of observations.}.
\citet{cameron2011robust} suggested an ad-hoc rule of thumb for approximating sufficient numbers of clusters.

\subsection{Non-linear Estimators}
\paragraph*{$m$-Estimators}
Consider an $m$-estimator that solves
$$
\sum^N_{i=1}\mathbf{h}_i\left(\hat{\boldsymbol{\theta}}\right) = \mathbf{0}
$$
under standard assumptions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal with estimated variance matrix 
\begin{equation}\label{eq:m-est_varmat}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \hat{\mathbf{A}}^{-1}\hat{\mathbf{B}}{\hat{\mathbf{A}^{\prime}}^{-1}}
\end{equation}
where $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}}$ and $\hat{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i\mathbf{h}_i\right]$.

\begin{itemize}
    \item \myhl[myblue]{\textbf{one-way clustering}} $\hat{\mathbf{B}} = \sum^G_{g=1}\hat{\mathbf{h}}_g\hat{\mathbf{h}}_g'$ where $\hat{\mathbf{h}}_g = \sum^{N_g}_{i=1}\hat{\mathbf{h}}_{ig}$, clustering may not lead to parameter inconsistency, depending on whether $\mathbb{E}\left[\mathbf{h}_i(\boldsymbol{\theta})\right]= \mathbf{0}$ with clustering
    \begin{itemize}
        \item \textbf{population-averaged approach}: assum $\mathbf{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi \left( \mathbf{x}_{ig}'\boldsymbol{\beta} \right)$
        \item \textbf{random effects approach}: let $y_{ig}=1$ if $y^*_{ig} > 0$ where $y^*_{ig}=\mathbf{x}_{ig}'\boldsymbol{\beta}+\epsilon_g + \epsilon_{ig}$, where 
        \begin{itemize}
            \item idiosyncratic error $\epsilon_{ig}\sim \mathcal{N}(0,1)$
            \item cluster-specific error $\epsilon_g \sim \mathcal{N}(0,\sigma^2_g)$
        \end{itemize}
        then we have the alternative moment condition $$ \mathbb{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi\left(\frac{\mathbf{x}_{ig}'\boldsymbol{\beta}}{\sqrt{1+\sigma^2_g}}\right) $$ 
    \end{itemize}
    \item \myhl[myblue]{\textbf{multiway clustering}} replacing $\hat{u}_i\mathbf{x}_i$ in Eq.(\ref{eq:centermat_multiclu}) with $\hat{\mathbf{h}}_i$, then we have 
    \begin{align*}
        \hat{\mathbb{V}} \left[\hat{\boldsymbol{\theta}}\right] &= \hat{\mathbf{A}}^{-1} \tilde{\mathbf{B}} \hat{\mathbf{A}'}^{-1}
    \end{align*}
    where 
    \begin{align*}
        \hat{\mathbf{A}} &= \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} & \tilde{\mathbf{B}} &= \sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r & \tilde{\mathbf{B}}_r &\equiv \sum^N_{i=1}\sum^N_{j=1}\hat{\mathbf{h}}_i\hat{\mathbf{h}'}_j\mathbb{I}_{\mathbf{r}}(i,j)
    \end{align*}
    with $\mathbf{r}\in R$\footnote{This multiway clustering can be implemented using several one-way clustered bootstraps. Each of the one-way cluster robust matrices is estimated by a pairs cluster bootstrap that resamples with replacement from the appropriate cluster dimension. They are then combined as if they had been estimated analytically \citep{cameron2011robust}.}.
\end{itemize} 

\paragraph*{GMM estimation} Consider an example of over-identified models: linear two stage least squares with more instruments than endogenous regressors, we have 
$$
\hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}) =  \arg\min_{\boldsymbol{\theta}} \left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)'\mathbf{W}\left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)
$$
where $\mathbf{W}$ is a symmetric positive definite weighting matrix. Under standard regularity conditions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal, with estimated variance matrix 
\begin{equation*}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}\hat{\mathbf{A}}'\mathbf{W}\tilde{\mathbf{B}}\mathbf{W}\hat{\mathbf{A}}\left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}
\end{equation*}
again, $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} $, and $\tilde{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i \mathbf{h}_i\right]$.

\section{Menzel (2021): Asymptotic Gaussianity}
One key of TWCR inference is the asymptotic Gaussianity, \citet{menzel2021bootstrap} pointed out the potential non-Gaussianity of the limit distribution.
Still, consider a random array ($Y_{it}$) indexed by two dimensions by $i=1,\cdots,N$ and $t=1,\cdots,T$. Clusters are sampled independently at random from an infinite population,
but otherwise \textbf{unrestricted} in dependence within each row $\mathbf{Y}_{i\cdot} \coloneq \left(Y_{i1}\cdots,Y_{iT}\right)$ and within each column $\mathbf{Y}_{\cdot t}\coloneq \left(Y_{1t},\cdots,Y_{Nt}\right)$.

\subsection{Distribution of Sample Average}
First, consider 
$$
\bar{Y}_{NT} \coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y_{it}
$$
and approximate the asymptotic distribution regardless of whether, or what type of, cluster-dependence is present.

\paragraph*{3 scenarios} of the array $(Y_{it})$
\begin{itemize}
    \item \myhl[myblue]{\textbf{no cluster-dependence}}: $(Y_{it})$ are mutually independent, CLT at a rate of $(NT)^{-1/2}$ applies (under regularity conditions)
    \item \myhl[myblue]{\textbf{correlation within clusters}}: the convergence rate of $(Y_{it})$ is determined by the number of relevant clusters 
    \item \myhl[myblue]{\textbf{non-separable models of heterogeneity (dependence with clusters, even uncorrelated)}}\footnote{This is specific to clustering in 2 or more dimensions.}: The asymptotic behavior is non-standard
\end{itemize}
Consider 2 examples:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Additive factor model}}
    $$ Y_{it} = \mu + \alpha_i + \gamma_t + \epsilon_{it} $$
    where $\mu$ is a constant, and $\alpha_i,\gamma_i,\epsilon_{it}$ are zero-mean i.i.d. random variables for $i=1,\cdots,N$ and $t=1,\cdots,T$ with bounded second moments, and $N=T$. Based on a standard central limit theory, we have 
    \begin{itemize}
        \item in the \underline{non-degenerate} case with $\mathrm{Var}(\alpha_i) >0$ or $\mathrm{\gamma_t}>0$, the sample distribution $$ \sqrt{N}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\alpha_i) + \mathrm{Var}(\gamma_t)\right) $$
        \item in the \underline{degenerate} case of \underline{no clustering} with $\mathrm{Var}(\alpha_i) = \mathrm{Var}(\gamma_t) =0$, the sample distribution $$ \sqrt{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\epsilon_{it})\right) $$ 
    \end{itemize}
    if marginal distributions of $\alpha_i,\gamma_t,\epsilon_{it}$ are known, we can simulate from the joint distribution of $\left(Y_{it}\right)$ by sampling the individual components at random, a bootstrap procedure would be consistent. If \textbf{unknown}, consider estimators 
    \begin{align*}
        \hat{\alpha}_i & \coloneq \frac{1}{T}\sum^T_{t=1}\left(Y_{it}-\bar{Y}_{NT}\right) = \alpha_i + \frac{1}{T}\sum^T_{t=1} \left(\epsilon_{it}-\bar{\epsilon}_{NT}\right)\\
        \hat{\gamma}_t & \coloneq \frac{1}{N}\sum^N_{i=1}\left(Y_{it}-\bar{Y}_{NT}\right) = \gamma_t + \frac{1}{N}\sum^N_{i=1} \left(\epsilon_{it}-\bar{\epsilon}_{NT}\right)\\
        \hat{\epsilon}_{it} & \coloneq Y_{it} - \bar{Y}_{NT} -\hat{\alpha}_i -\hat{\gamma}_t
    \end{align*}
    then use these empirical distributions for estimation and form a bootstrap sample 
    $$ Y^*_{it} \coloneq \bar{Y}_{NT} + \alpha^*_i + \gamma^*_t + \epsilon^*_{it} $$
    by drawing from these estimators and obtain $\bar{Y}^*_{NT}\coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y^*_{it}$, and verify the conditional variances of the bootstrap distribution given the sample:
    \begin{align*}
        \frac{1}{N}\sum^N_{i=1}\left(\hat{\alpha}_i - \frac{1}{N}\sum^N_{j=1}\hat{\alpha}_j\right)^2 - \left[\mathrm{Var}(\alpha_i) + \frac{\mathrm{Var}(\epsilon_{it})}{T}\right] & \xrightarrow{p} 0\\
        \frac{1}{T}\sum^N_{i=1}\left(\hat{\gamma}_t - \frac{1}{T}\sum^T_{s=1}\hat{\gamma}_t\right)^2 - \left[\mathrm{Var}(\gamma_t) + \frac{\mathrm{Var}(\epsilon_{it})}{N}\right] & \xrightarrow{p} 0
    \end{align*}
    then the bootstrap distribution is
    \begin{itemize}
        \item in the \underline{non-degenerate} case, $$ \sqrt{N}\left( \bar{Y}^*_{NT}-\bar{Y}_{NT} \right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\alpha_i) + \mathrm{Var}(\gamma_t)\right) $$
        the estimation error $\hat{\alpha}_i$ does \textbf{NOT} affect the asymptotic variance.
        \item in the \underline{degenerate} case, $$ \sqrt{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right) \xrightarrow{d} \mathcal{N}\left(0,3\mathrm{Var}(\epsilon_{it})\right) $$
        asymptotically overestimates the variance of the sampling distribution, leading to inconsistency of this naive bootstrapping procedure.
    \end{itemize}
    \item \myhl[myblue]{\textbf{Non-Gaussian limit distribution}} $$ Y_{it} = \alpha_i \gamma_t + \epsilon_{it} $$
    where $\alpha_i,\gamma_t,\epsilon_{it}$ are independently distributed with $\mathbb{E}\left[\epsilon_{it}\right] = 0,\mathrm{Var}(\alpha_i) = \sigma^2_{\alpha},\mathrm{Var}(\gamma_t) = \sigma^2_{\gamma}, \mathrm{Var}(\epsilon)_{it}=\sigma^2_{\epsilon}$. \textbf{If} $\mathbb{E}\left[\alpha_i\right] = \mathbb{E}\left[\gamma_t\right] = 0$, then CLT and Continuous Mapping Theorem (CMT) imply 
    \begin{align*}
        \sqrt{NT} \cdot \bar{Y}_{NT} =& \frac{1}{\sqrt{NT}} \sum^N_{i=1}\sum^T_{t=1} \left(\alpha_i \gamma_t +\epsilon_{it}\right)\\
        =& \left( \frac{1}{\sqrt{N}} \sum^N_{i=1}\alpha_i \right) \left(\frac{1}{\sqrt{T}} \sum^T_{t=1}\gamma_t \right) + \frac{1}{\sqrt{NT}}\sum^N_{i=1}\sum^T_{t=1}\epsilon_{it}\\
        \xrightarrow{d}& \sigma_{\alpha}\sigma_{\gamma}Z_1Z_2 + \sigma_{\epsilon}Z_3
    \end{align*}
\end{itemize}
then even without correlation within clusters, non-separable heterogeneity can still generate dependence in $2^{\text{nd}}$ or higher moments in the limiting distribution\footnote{2 major issues arise:\begin{itemize}
    \item The limiting distribution needs \textbf{not} be Gaussian: plug-in asymptotic inference based on the normal distribution is invalid 
    \item It only comes from two-or-more-dimension cluster dependence, not single-dimension cluster dependence.
\end{itemize}
}.

\subsection{Menzel (2021)'s Bootstrap procedure}\label{subsec:menzel_bootstrap}
\subsubsection{Notation}
For the array $\left(Y_{it}\right)_{i,t}$, denote 
\begin{itemize}
    \item $\mathbb{P}$: joint distribution of $\left(Y_{it}\right)_{i,t}$
    \item $\mathbb{P}_{NT}$: drifting DGP indexed by $N,T$
    \item $\mathbb{P}^*_{NT}$: bootstrap distribution for $\left(Y^*_{it}\right)$ given the realizations $\left(  Y_{it}:i=1,\cdots,N; t= 1,\cdots,T  \right)$
    \item respective distributions $\mathbb{E},\mathbb{E}_{NT},\mathbb{E}^*_{NT}$
\end{itemize}

\subsubsection{Inference: Sample Mean}
First, consider the assumption of \textit{separate exchangeability}
\begin{assumption}{Separate Exchangeability}{sep_exchange}
    \begin{itemize}
        \item A \textbf{separately exchangeable} array is an infinite array $\left(Y_{it}\right)_{i,t}$ such that for any integers $\tilde{N},\tilde{T}$ and permutations $\pi_1:\left\{1,\cdots,\tilde{N}\right\}\rightarrow \left\{1,\cdots,\tilde{N}\right\}$ and $\pi_2:\left\{1,\cdots,\tilde{T}\right\}\rightarrow \left\{1,\cdots,\tilde{T}\right\}$, we have 
        $$ \left(Y_{\pi_1(i),\pi_2(t)}\right)_{i,t} \overset{d}{=} \left(Y_{it}\right)_{i,t} $$
        such an array is called \textbf{dissociated} if for any $N_0,T_0\geq 1$, $\left(Y_{it}\right)^{i=N_0,t=T_0}_{i=1,t=1}$ is independent of $\left(Y_{it}\right)_{i>N_0,t>T_0}$.
        \item For dyadic data, consider the alternative assumption \textbf{jointly exchangeable} arrays $\left(Y_{ij}\right)_{i,j}$ satisfying 
        $$ \left(Y_{\pi(i),\pi(j)}\right)_{i,j} \overset{d}{=} \left(Y_{ij}\right)_{i,j} $$
        for any permutation $\pi$ on $\left\{1,\cdots,\tilde{N}\right\}$, in addition, $\left(Y_{ij}\right)^{N_0}_{i,j=1}$ is independent of $\left(Y_{ij}\right)_{i,j>N_0}$
    \end{itemize}
\end{assumption}
This assumption can be interpreted as rows (and columns) corresponding to units that are drawn independently from a common population, where we then observe the joint outcome for every row-column pair, consider the requirements in the following applications
\begin{itemize}
    \item \myhl[myblue]{\textbf{DiD/matched data}}: the units corresponding to either dimension of the sample to represent independent draws from a common, infinite population 
    \item \myhl[myblue]{\textbf{non-exhaustively matched data}}: only observe joint outcomes for a posibly self-selected subset of unit pairs, sample selection should be (jointly or separately) exchangeable
    \item \myhl[myblue]{\textbf{U-/V-statistics}}: the kernel $Y_{i_1,\cdots,i_D} \coloneq h\left(X_{i_1},\cdots,X_{i_D}\right) $ evaluated at i.i.d. observations $X_1,\cdots,X_N$ forms a dissociated, jointly exchangeable array
    \item \myhl[myblue]{\textbf{Network}}: unlabeled\footnote{\textit{Unlabeled}: nodel identifiers do not carry any significance for the statistical model.} data implies finite exchangeability, the sampled graph has joint (\textit{infinite}) exchangeability if it is a subgraph of an infinite graph
\end{itemize}
Directly from Assumption \ref{assump:sep_exchange}, any dissociated separately exchangeable array can be represented as 
\begin{equation*}
    Y_{it} = f\left(\alpha_i,\gamma_t,\epsilon_{it}\right)
\end{equation*}
for some function $f(\cdot)$ where $\alpha_1,\cdots,\alpha_N$, $\gamma_1,\cdots,\gamma_T$, $\epsilon_{11},\cdots,\epsilon_{NT}$ are mutually independent, uniformly distributed random variables.

\paragraph*{Projection} now, decompose the array $\left(Y_{it}\right)_{i,t}$ as 
\begin{align*}
    Y_{it} &= b + a_i + g_t + w_{it} & \mathbb{E}\left[w_{it}\mid a_i,g_t\right]&=0
\end{align*}
where $a_i$ and $g_t$ are mean-zero and mutually independent, s.t. the joint distribution of $Y_{it}$ can then be expanded as 
\begin{align*}
    Y_{it} =& \mathbb{E}\left[Y_{it}\right] + \left(\mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\right] \right) + \left( \mathbb{E}\left[Y_{it}\mid \gamma_t\right] - \mathbb{E}\left[Y_{it}\right] \right) \\
    &+ \left( \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]\right) + \left( Y_{it}-\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] \right) \\
    \eqcolon & b+ a_i + g_t + v_{it} + e_{it}
\end{align*}
with
\begin{itemize}
    \item $e_{it} = Y_{it} - \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right]$
    \item $a_i = \mathbb{E}\left[Y_{it}\mid\alpha_i \right]-\mathbb{E}\left[Y_{it}\right]$, $g_t= \mathbb{E}\left[Y_{it}\mid \gamma_t\right]-\mathbb{E}\left[Y_{it}\right]$
    \item $v_{it} = \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]$
    \item $b= \mathbb{E}\left[Y_{it}\right]$
\end{itemize}
here, 
\begin{itemize}
    \item temporal and cross-sectional units were drawn independently: $a_1,\cdots,a_N$ and $g_1,\cdots,g_T$ are independent of each other.
    \item by construction, $\mathbb{E}\left[e_{it}\mid a_i,g_t,v_{it}\right]=0$, $\mathbb{E}\left[v_{it}\mid a_i\right] = \mathbb{E}\left[v_{it}\mid g_t\right]=0$
    \item $e_{it}$, $(a_i,g_t)$ and $v_{it}$ are \textbf{uncorrelated}
\end{itemize}
then, rewrite the sample mean as 
\begin{align*}
    \hat{Y}_{NT} &= b+\bar{a}_N + \bar{g}_T + \bar{v}_{NT} + \bar{e}_{NT}\\
    &\coloneq b + \frac{1}{N}\sum^N_{i=1}a_i + \frac{1}{T}\sum^T_{t=1}g_t + \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}v_{it} + \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}e_{it}
\end{align*}
and the unconditional variances of the projections with 
\begin{align*}
    \sigma^2_a&\coloneq \mathrm{Var}(a_i) & \sigma^2_g&\coloneq \mathrm{Var}(g_t) & \sigma^2_v&\coloneq \mathrm{Var}(v_{it}) & \sigma^2_e&\coloneq \mathrm{Var}(e_{it})   
\end{align*}
let $w_{it}\coloneq v_{it}+ e_{it}$, and denote its variance by $\sigma^2_w = \mathrm{Var}(w_{it})$. Then, assume integrability 
\begin{assumption}{Integrability}{integrability}
    Let $Y_{it} = f(\alpha_i,\gamma_t,\epsilon_{it})$, where $\alpha_i,\gamma_t,\epsilon_{it}$ are random arrays with elements i.i.d. drawn from $[0,1]$ uniform distribution, assume 
    \begin{itemize}
        \item $a_i/\sigma_a$, $g_t/\sigma_g$, $v_{it}/\sigma_v$, $e_{it}/\sigma_e$ are well-defined and have bounded moments up to the order $4+\delta$ for some $\delta>0$, whenever the respective variances $\sigma^2_a,\sigma^2_g,\sigma^2_v,\sigma^2_e$ are non-zero.
        \item $\sigma^2_a + \sigma^2_g >0$, or $\sigma^2_v + \sigma^2_e > 0$
    \end{itemize}
\end{assumption}

\paragraph*{Low-rank approximation}
Consider the row/column projection
\begin{equation*}
    \bar{v}_{NT} \equiv \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}\left(\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] -\mathbb{E}\left[Y_{it}\mid \alpha_i\right] -\mathbb{E}\left[Y_{it}\mid \gamma_t\right] +\mathbb{E}\left[Y_{it}\right] \right) \eqcolon \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}v(\alpha_i,\gamma_t)
\end{equation*}
as a generalized U-statistic with a kerel $v(\alpha,\gamma)$ evaluated at the samples $\alpha_1,\cdots,\alpha_N$ and $\gamma_1,\cdots,\gamma_t$. There are 2 major issues w.r.t. characterizing the distribution of $\bar{Y}_{NT}$
\begin{itemize}
    \item the presence of the projection error $e_{it}$
    \item the factors $\alpha_i,\gamma_t$ are not observable
\end{itemize}
Define,
$$
v(\alpha,\gamma) \coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha,\gamma_t=\gamma\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t=\gamma\right] + \mathbb{E}\left[Y_{it}\right]
$$
under Assumption \ref{assump:integrability}, we have compact integral operators
\begin{align*}
    S(u)(g)&=\int v(a,g)u(a)F_{\alpha}(\mathrm{d}a) & S^*(u)(a)&=\int v(a,g)u(g)F_{\gamma}(\mathrm{d}g)
\end{align*}
where $F_{\alpha},F_{\gamma}$ are the marginal distributions corresponding to the joint $F_{\alpha\gamma}$ of $\alpha_i,\gamma_t$. Then the low-rank approximation is
\begin{equation}\label{eq:low-rank_approximation}
    v(\alpha,\gamma) = \sum^{\infty}_{k=1}c_k\phi_k(\alpha)\psi_k(\gamma)
\end{equation}
under the $L_2(F_{\alpha\gamma})$ norm on the space of smooth functions of $(\alpha,\gamma)\in[0,1]^2$. Here 
\begin{itemize}
    \item $(c_k)_{k\geq 1}$: a sequence of singular values, $\lim\left\vert c_k \right\vert \rightarrow 0$
    \item $\left(\phi_k\left(\cdot\right)\right)_{k\geq 1}$ and $\left(\psi_k\left(\cdot\right)\right)_{k\geq 1}$: orthonormal bases for $L_2\left([0,1],F_{\alpha}\right)$ and $L_2\left([0,1],F_{\gamma}\right)$:
    \begin{itemize}
        \item By construction: $$ \mathbb{E}\left[v(a,\gamma_t)\right] = \mathbb{E}\left[v(\alpha_i,g)\right] =0,\forall a,g\in[0,1] \Rightarrow \mathbb{E}\left[\phi_k(\alpha_i)\right] = \mathbb{E}\left[\psi_k(\gamma_t)\right] = 0,\forall k =1,2,\cdots $$
        \item the basis functions are orthonormal and $\alpha_i$ and $\gamma_t$ are independent, then $\forall K<\infty$ $$ \mathrm{Cov}\left[ \left(\phi_1(\alpha_i),\psi_1(\gamma_t), \cdots, \phi_K(\alpha_i),\psi_K(\gamma_t) \right) \right] $$ is the $2K$-dimensional identity matrix
        \item $\left(\phi_1(\alpha_i),\cdots, \phi_K(\alpha_i)\right)$ can be correlated with $a_i$: $\sigma_{ak}\coloneq \mathrm{Cov}\left(a_i,\phi_k(\alpha_i)\right)$
        \item $\left(\psi_1(\gamma_t),\cdots, \psi_K(\gamma_t)\right)$ can be correlated with $g_t$: $\sigma_{gk}\coloneq \mathrm{Cov}\left(g_t,\psi_k(\gamma_t)\right)$
    \end{itemize}
\end{itemize}
with this representation of Eq.(\ref{eq:low-rank_approximation}), we have\footnote{The limiting distribution of this term is not Gaussian, but can be represented as a linear combination of independent chi-squared random variables. This type of distributions is known as Wiener/Gaussian chaos.} 
\begin{equation*}
    \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}v(\alpha_i,\gamma_t) = \sum^{\infty}_{k=1}c_k \left(\frac{1}{N}\sum^N_{i=1}\phi_k(\alpha_i)\right) \left(\frac{1}{T}\sum^T_{t=1}\psi_k(\gamma_t)\right)
\end{equation*}
and the second-order projection term can also be represented as a function of \textbf{countably many} sample averages of \textbf{i.i.d. mean-zero} random variables.

\begin{assumption}{Eigenfucntions and coefficients in the spectral represention (\ref{eq:low-rank_approximation})}{restrictions_on_lowrankapprox}
    The function $v(\alpha,\gamma)\coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha,\gamma_t=\gamma\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t=\gamma\right] + \mathbb{E}\left[Y_{it}\right]$ admits a spectral representation
    $$
    v(\alpha,\gamma) = \sum^{\infty}_{k=1}c_k\phi_k(\alpha)\psi_k(\gamma)
    $$
    under the $L_2(F_{\alpha\gamma})$ norm. And 
    \begin{itemize}
        \item the singular values are uniformly bounded by a square summable null sequence $\bar{c}_k$: $c_k\leq \bar{c}_k,\forall k=1,2,\cdots$, where $\sum^{\infty}_{k=1}c^2_k< \infty$
        \item $\forall k=1,2,\cdots$, the first 3 moments of the eigenfunctions $\phi_k(\alpha_i)$ and $\psi_k(\gamma_t)$ are bounded by a constant $B>0$
    \end{itemize}
\end{assumption}
To summarize the two assumptions
\begin{itemize}
    \item Assumption \ref{assump:sep_exchange} guarantees the pointwise consistency of the bootstrap
    \item Assumption \ref{assump:restrictions_on_lowrankapprox} gives the uniform consistency of the bootstrap: it imposes common bounds on moments and singular values and restricts the set of joint distribution $F$ to a \myhl[myblue]{\textbf{uniformity}} class\footnote{Here, the sequence $\mathbf{c}\coloneq \left(\bar(c)_k\right)_{k\geq 0}$ controls the magnitude of the error from a finite-dimensional approximation to $v(\alpha,\gamma)$.}.
\end{itemize}

\subsubsection{Bootstrap procedure}
For the sample mean $\bar{Y}_{NT} - \mathbb{E}\left[Y_{it}\right]$, the limiting distribution depends on the scale parameters:
\begin{itemize}
    \item If observations are independent across rows and columns: $\sqrt{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\sigma^2_e\right)$
    \item If $N=T$, within-cluster covariances are bounded from 0 in \textbf{at least one dimension}: $\sqrt{N}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d}\mathcal{N}\left(0,\sigma^2_a+\sigma^2_g\right)$
\end{itemize}
The bootstrap procedure should then be adaptive for both degenerate and non-degenerate cases. For the expansion
\begin{align}\label{eq:sample_mean_decomp}
    Y_{it} =& \mathbb{E}\left[Y_{it}\right] + \left(\mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\right] \right) + \left( \mathbb{E}\left[Y_{it}\mid \gamma_t\right] - \mathbb{E}\left[Y_{it}\right] \right) \\ \nonumber
    &+ \left( \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]\right) + \left( Y_{it}-\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] \right) \\ \nonumber
    \eqcolon & b+ a_i + g_t + v_{it} + e_{it}
\end{align}
the sample analogs are:
\begin{align*}
    \hat{a}_i &\coloneq \frac{1}{T}\sum^T_{t=1}Y_{it}-\bar{Y}_{NT} & \hat{g}_t &\coloneq \frac{1}{N}\sum^N_{i=1}Y_{it} - \bar{Y}_{NT} & \hat{w}_{it}\coloneq & Y_{it} - \hat{a}_i-\hat{g}_t -\bar{Y}_{NT}
\end{align*}

\paragraph*{Evaluating bootstrap performance} it is crucial at what rates these estimators are consistent depending on the extent of clustering in the true DGP.
The variance of the projection terms are: 
\begin{align*}
    \mathrm{Var}\left(\hat{a}_i\right) &= \sigma^2_a + \frac{\sigma^2_w}{T} & \mathrm{Var}\left(\hat{g}_t\right) &= \sigma^2_g + \frac{\sigma^2_w}{N}
\end{align*}
s.t. the \textbf{convolution error} depending on $\sigma^2_w$ dominates in the degenerate case. Therefore, to correct for the contribution of the row/column averages of $w_{it}$, consider the scalar for the distribution of $\hat{a}_i,\hat{g}_t$ by 
\begin{align*}
    \lambda_a &= \frac{T\sigma^2_a}{T\sigma^2_a +\sigma^2_w} & \lambda_g &= \frac{N\sigma^2_g}{N\sigma^2_g +\sigma^2_w} 
\end{align*}

\paragraph*{Component variance estimator} let 
\begin{align*}
    \hat{s}^2_a &\coloneq \frac{1}{N-1}\sum^N_{i=1} \left(\hat{a}_i - \bar{Y}_{NT}\right)^2 \\
    \hat{s}^2_g &\coloneq \frac{1}{T-1}\sum^T_{t=1} \left(\hat{g}_t - \bar{Y}_{NT}\right)^2 \\
    \hat{s}^2_w &\coloneq \frac{1}{NT-N-T} \sum^N_{i=1}\sum^T_{t=1} \left(Y_{it}-\hat{a}_i-\hat{g}_t -\bar{Y}_{NT}\right)^2
\end{align*}
then form the estimators as 
\begin{align}
    \hat{\sigma}^2_a &= \max \left\{0,\hat{s}_a^2 -\frac{1}{T}\hat{s}^2_w\right\} & \hat{\sigma}_g^2 &=\max \left\{0,\hat{s}^2_g -\frac{1}{N}\hat{s}_w^2\right\} & \hat{\sigma}^2_w&\coloneq \hat{s}^2_w
\end{align}
the rates of convergence for these estimators are given in the following lemma:
\begin{lemma}{Stochastic Order of Variance Estimators}{varest_stcha_order}
    Under Assumption \ref{assump:sep_exchange},
    \begin{align*}
        \hat{\sigma}^2_a - \sigma^2_a &= O_p \left(\frac{1}{\sqrt{N}}\left(\sigma_a + \frac{\sigma_e}{\sqrt{T}}\right)^2 + \frac{\sigma^2_v}{T}\right) \\
        \hat{\sigma}^2_g - \sigma^2_g &= O_p \left(\frac{1}{\sqrt{T}}\left(\sigma_g + \frac{\sigma_e}{\sqrt{N}}\right)^2 + \frac{\sigma^2_v}{N}\right) \\
        \hat{\sigma}^2_w - \sigma^2_w &= O_p \left(\frac{\sigma^2_e}{\sqrt{NT}} + \left(\frac{1}{N} + \frac{1}{T}\right)\sigma^2_v \right)
    \end{align*}
    and there exist \textbf{no estimators} for $\sigma^2_a,\sigma^2_g,\sigma^2_w$ that converge at rates faster than these rates. Specifically, $\sigma^2_a$ can \textbf{NOT} be estimated at a rate faster than $T^{-1}$ even when $\sigma^2_a=0$\footnote{See the appendix of \citet{menzel2021bootstrap} for the proof.}.
\end{lemma}
Hence, a bootstrap procedure can use a consistent pre-test for the presence of cluster dependence in the \textbf{first moment}, with the model selectors 
\begin{align*}
    \hat{D}_a(\kappa) &\coloneq \mathbf{1} \left\{T\hat{\sigma}_a^2 \geq \kappa \right\} & \hat{D}_g(\kappa) &\coloneq \mathbf{1}\left\{N\hat{\sigma}_g^2 \geq \kappa \right\}
\end{align*}
$\forall \kappa \geq 0$. And for some $\kappa_a,\kappa_g$, let 
\begin{align*}
    \hat{\lambda}_a & \coloneq \frac{\hat{D}_a (\kappa_a)T\hat{\sigma}^2_a}{\hat{D}_a (\kappa_a)T\hat{\sigma}^2_a+\hat{\sigma}_w^2} & \hat{\lambda}_g & \coloneq \frac{\hat{D}_g (\kappa_g)T\hat{\sigma}^2_g}{\hat{D}_g (\kappa_g)N\hat{\sigma}^2_g + \hat{\sigma}_w}
\end{align*}
and estimate the asymptotic variance of the sample mean as 
\begin{equation}\label{eq:asymp_variance}
    \hat{S}^2_{NT,sel}\coloneq \hat{D}_a(\kappa_a)T\hat{\sigma}^2_a + \hat{D}_g (\kappa_g) N\hat{\sigma}^2_g + \hat{\sigma}^2_w
\end{equation}

\paragraph*{Bootstrap procedures}
\citet{menzel2021bootstrap} proposed the following resampling algorithm to estimate the sampling distribution for exhaustive sampling with cluster dependence in two dimensions 
\begin{algorithm}{Resampling Algorithm}{resampling_menzel}
    \begin{itemize}
        \item[\textbf{(a)}] For the $b$-th bootstrap iteration, draw 
        \begin{align*}
            a^*_{i,b} &\coloneq \hat{a}_{k^*_b(i)} & g^*_{t,b} &\coloneq \hat{g}_{s^*_b(t)}
        \end{align*}
        where $k^*_b(i)$ and $s^*_b(t)$ are i.i.d. draws from the discrete uniform distribution on the index sets $\left\{1,\cdots,N\right\}$ and $\left\{1,\cdots,T\right\}$ respectively
        \item[\textbf{(b)}] Generate $$ w^*_{it,b} \coloneq \omega_{1i,b}\omega_{2t,b}\hat{w}_{k^*_b(i)s^*_b(t)} $$ where $\omega_{1i,b},\omega_{2t,b}$ are i.i.d. random variables with $\mathbb{E}\left[\omega\right]=0$, $\mathbb{E}\left[\omega^2\right] = \mathbb{E}\left[\omega^3\right] = 1$\footnote{Typical choices of $\omega_{1i,b},\omega_{2t,b}$ are the Gamma distribution (with shape $=4$, scale $=1/2$).}
        \item[\textbf{(c)}] Generate a bootstrap sample of draws $$ Y^*_{it,b} = \bar{Y}_{NT} + \sqrt{\hat{\lambda}_a}a^*_{i,b} + \sqrt{\hat{\lambda}_g}g^*_{t,b} + w^*_{it,b} $$ and get the bootstrapped statistic $$ \bar{Y}^*_{NT,b} \coloneq \frac{1}{NT} \sum^N_{i=1}\sum^T_{t=1}Y^*_{it,b} $$ 
        \item[\textbf{(d)}] Repeat this procedure, get a sample of $B$ replications and approximate the conditional distribution of $\bar{Y}^*_{NT}$ given the sample with the empirical distribution over the bootstrap draws $\bar{Y}^*_{NT,1},\cdots,\bar{Y}^*_{NT,B}$ 
    \end{itemize}
\end{algorithm}
For the \textbf{pivotal boostrap}, the last step uses instead the empirical distribution of the studentized bootstrap draws to approximate the distribution of $$ \sqrt{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right)/\hat{S}^*_{NT,sel} $$
where $\hat{S}^*_{NT,sel}$ is the bootstrap analog of the variance estimator $\hat{S}_{NT,sel}$.

\begin{definition}{Bootstrap Procedures}{menzel_bootstrap_procedure}
    Consider 3 versions of the bootstrap procedure based on \ref{algm:resampling_menzel}:
    \begin{itemize}
        \item \myhl[myred]{\textbf{BS-N}} (bootstrap \textit{without} model selection): apply steps (a) - (d), and set $\kappa_a = \kappa_g = 0$
        \item \myhl[myred]{\textbf{BS-S}} (bootstrap \textit{with} model selection): apply steps (a) - (d), and set $\kappa_a,\kappa_g$ according to increasing sequences $\kappa_g,\kappa_a\rightarrow \infty$ s.t. $\kappa_a/T\rightarrow 0$ and $\kappa_g/N\rightarrow 0$
        \item \myhl[myred]{\textbf{BS-C}} (\textit{conservative} bootstrap): addition to the settings of \myhl[myred]{\textbf{BS-S}}, set 
        \begin{align*}
            \hat{\lambda}_a \coloneq& \frac{\hat{q}_a}{\hat{q}_a+\hat{\sigma}^2_w}\frac{\hat{q}_a}{T\hat{\sigma}^2_a} & \hat{\lambda}_g &\coloneq \frac{\hat{q}_g}{\hat{q}_g+\hat{\sigma}^2_w}\frac{\hat{q}_g}{N\hat{\sigma}^2_g}
        \end{align*}
        where 
        \begin{align*}
            \hat{q}_a &\coloneq \max\left\{T\hat{\sigma}^2_a,\kappa_a\right\} & \hat{q}_g &\coloneq \max\left\{N\hat{\sigma}^2_g,\kappa_g\right\}
        \end{align*}
    \end{itemize}
\end{definition}

\paragraph*{Consistency of the bootstrap procedures}
\begin{itemize}
    \item {\textbf{BS-N}} (bootstrap \textit{with} model selection): \myhl[myblue]{\textbf{pointwise} consistent} in $\sigma^2_a,\sigma^2_g,\sigma^2_w$
    \item {\textbf{BS-S}} (bootstrap \textit{without} model selection): \myhl[myblue]{\textbf{uniformly} consistent} if the limiting distribution is Gaussian
    \item {\textbf{BS-C}} (\textit{conservative} bootstrap): \myhl[myblue]{\textbf{consistent}} in the nondegerate case $\sigma^2_a + \sigma^2_g >0$, but asymptotically \myhl[myblue]{\textbf{conservative}} for the degenerate cases
\end{itemize}
To establish the consistency, define the \textbf{adaptive rate} $r_{NT}$ as\footnote{Following Eq. (\ref{eq:sample_mean_decomp}), $\mathrm{Var}(\bar{Y}_{NT}) = \mathrm{Var}(b+\bar{a}_N +\bar{g}_T + \bar{v}_{NT} + \bar{e}_{NT})$.}
\begin{equation*}
    r^{-2}_{NT} \coloneq N^{-1}\sigma^2_a + T^{-1}\sigma^2_g + (NT)^{-1}\sigma^2_w \equiv \mathrm{Var}(\bar{Y}_{NT})
\end{equation*}
then consider the limiting distribution with the respective limits of normalized sequences:
\begin{align}\label{eq:normalized_variances}
    q_{a,NT} &\coloneq r^2_{NT}N^{-1}\sigma^2_a & q_{g,NT} &\coloneq r^2_{NT}T^{-1}\sigma^2_g & q_{e,NT} &\coloneq r^2_{NT}(NT)^{-1}\sigma^2_e & q_{v,NT} &\coloneq r^2_{NT}(NT)^{-1}\sigma^2_v \\
    & & q_{ak,NT} & \coloneq r^2_{NT}N^{-1}\sigma_{ak} & q_{gk,NT} &\coloneq r^2_{NT}T^{-1}\sigma_{gk}\nonumber
\end{align}
for $k=1,2,\cdots$. Let $\varrho_{NT} \coloneq r_{NT}\left(NT\right)^{-1/2}$, then 
$$
q_{a,NT} + q_{g,NT} + q_{e,NT} + q_{v,NT} = 1
$$
stacking the sequences as the vector
$$
\mathbf{q}_{NT} \coloneq \left( q_{e,NT},q_{a,NT},q_{g,NT}, q_{a1,NT},q_{g1,NT}, q_{a2,NT},q_{g2,NT},\cdots \right)
$$
and the singular values for the spectral decomposition (\ref{eq:low-rank_approximation}):
\begin{align*}
    \mathbf{c}_{NT} &\coloneq \left(c_{1,NT},c_{2,NT},\cdots\right) \in l^2 & \text{for }&\mathbb{E}_{NT}\left[Y_{it}\mid \alpha_i,\gamma_t\right]\\
    \mathbf{c} &\coloneq \left(c_1,c_2,\cdots\right) \in l^2 & \text{for }&\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right]
\end{align*}
then for convergent sequences $\mathbf{q}_{NT},\mathbf{c}_{NT},\mathbf{c}$, denote the limits 
\begin{align*}
    q_a &\coloneq \lim_{N,T}q_{a,NT} & q_g &\coloneq \lim_{N,T}q_{g,NT} & q_e & \coloneq \lim_{N,T}q_{e,NT} & q_v & \coloneq \lim_{N,T} q_{v,NT} \\
    \mathbf{q} & \coloneq \lim_{N,T}\mathbf{q}_{NT} & \mathbf{c} & \coloneq \lim_{N,T}\mathbf{c}_{NT} & \varrho &\coloneq \lim_{N,T}\varrho_{NT}
\end{align*}
for any fixed values of $\mathbf{q}, \mathbf{c},\varrho\in[0,1]$, define 
\begin{equation}\label{eq:convergence_law}
    \mathcal{L}_0\left(\mathbf{q},\mathbf{c},\varrho\right) \coloneq \left(\sqrt{q_e}Z^e  + \sqrt{q_a}Z^a + \sqrt{q_g}Z^g\right) + \varrho V
\end{equation}
where 
\begin{align*}
    V \coloneq \sum^{\infty}_{k=1}c_k Z^{\psi}_k Z^{\phi}_k
\end{align*}
and $Z^e,Z_k^{\psi},Z_k^{\phi}$ are i.i.d. standard normal random variables, $Z^a,Z_g$ are standard normal random variables with 
\begin{align*}
    \mathrm{Cov}\left(Z^a,Z_k^{\phi}\right) &= \frac{q_{ak}}{\sqrt{q_a}} & \mathrm{Cov}\left(Z^g,Z_k^{\psi}\right) &= \frac{q_{gk}}{\sqrt{q_g}} & \mathrm{Cov}\left(Z^a,Z^g\right) = \mathrm{Cov}\left(Z^a,Z_k^{\psi}\right) = \mathrm{Cov}\left(Z^g,Z_k^{\psi}\right) &=0
\end{align*}
Then, the CLT for sampling distribution is established as
\begin{theorem}{CLT for Sampling Distribution}{clt_samp_dist}
    Under Assumption \ref{assump:integrability},
    \begin{itemize}
        \item[(\textbf{a})] along \textit{any} convergent sequence $\mathbf{q}_{NT}\rightarrow \mathbf{q}$ and fixed $\mathbf{c} = \left(c_1,c_2,\cdots\right)$ , we have 
        \begin{equation*}
            \left\Vert \mathbb{P}\left(r_{NT}\left(\bar{Y}_{NT}-\mathbb{E}[Y_{it}]\right)\right) - \mathcal{L}_0 \left(\mathbf{q},\mathbf{c},\varrho \right) \right\Vert _{\infty} \rightarrow 0
        \end{equation*}
        where $\varrho \coloneq \lim_{N,T}\varrho_{NT}$, and $\left\Vert \cdot \right\Vert _{infty}$ denotes the Kolmogorov metric; the limiting distribution $\mathcal{L}_0\left(\mathbf{q},\mathbf{c},\varrho\right)$ is continuous\footnote{The convergence is pointwise w.r.t. the conditional mean function $\mathbb{E}\left[Y_{it} \mid \alpha_i=\alpha,\gamma_t=\gamma\right]$}.
        \item[(\textbf{b})] if in addition, Assumption \ref{assump:restrictions_on_lowrankapprox} holds, (\textbf{a}) is robust under drifting sequences $\mathbf{c}_{NT}\rightarrow \mathbf{c}$\footnote{The convergence is uniform within the class of distributions satisfying Assumption \ref{assump:restrictions_on_lowrankapprox}}
    \end{itemize}
\end{theorem}

\subparagraph*{Estimating the asymptotic distribution}
Lemma \ref{lemma:varest_stcha_order} establishes the consistency of the estimation for the components vairances $\sigma^2_a,\sigma^2_g,\sigma^2_w$, but are they \textbf{fast} enough?
\begin{proposition}{Estimability of Asymptotic Distribution}{estimability_asympdist}
    Let $\hat{\mathcal{L}}_{NT}$ denote an arbitrary estimator for $\mathcal{L}_0$ based on an array of size $N,T$ form the unknown distribution, then $\exists \delta>0$ s.t. 
    \begin{equation*}
        \liminf_{N,T\rightarrow\infty} \sup_{f\in\mathcal{F}} \mathbb{P}_{f,NT} \left( \left\Vert \hat{\mathcal{L}}_{NT} - \mathcal{L}_0 \left(\mathbf{q}_{NT}(f),\mathbf{c}_{NT}(f),\varrho_{NT}(f)\right) \right\Vert _{\infty} > \delta \right) >0
    \end{equation*}
    where
    \begin{itemize}
        \item $\mathcal{F}$: the class of functions $f(\alpha,\gamma,\epsilon)$ corresponding to distributions of $Y_{it}$ satisfying Assump. \ref{assump:integrability} and \ref{assump:restrictions_on_lowrankapprox}, for i.i.d. uniform $\alpha_i,\gamma_t,\epsilon_{it}$\footnote{From the Aldous-Hoover representation}
        \item $\mathbb{P}_{f,NT}\left(\cdot\right)$: probabilities for events w.r.t. an array of size $N,T$, generated according to $f$
        \item $\mathbf{q}_{NT}(f) \coloneq \left(q_{e,NT}(f),q_{a,NT}(f),\cdots\right)$: the vector of normalized variances from Eq. \ref{eq:normalized_variances}
    \end{itemize}
\end{proposition}
Proposition \ref{prop:estimability_asympdist} states that there exists \textbf{no estimator}
\footnote{Consider the counterexample for this impossibility: for the model $$Y_{it} = \alpha_i\gamma_t$$ where $\alpha_i,\gamma_t$ are mutually independent with i.i.d. factors $\alpha_i \sim \mathcal{N}(0,1),\gamma_t\sim\mathcal{N}(\mu_{\gamma},1)$. This model satisfies Assump.\ref{assump:integrability}, hence 
Thm.\ref{thm:clt_samp_dist} gives convergence results. However, for this model 
\begin{align*}
    a_i & \coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i\right] = \alpha_i\mu_{\gamma} & g_t & \coloneq \mathbb{E}\left[Y_{it}\mid \gamma_t\right] = \gamma_t\mathbb{E}\left[\alpha_i\right]\equiv 0\\
    v_{it} & =\alpha_i(\gamma_t-\mu_{\gamma}) & \sigma^2_a&=\mu^2_{\gamma} & \sigma^2_v=1
\end{align*}
here, $\mu_{\gamma}$ can \textbf{not} be estimated from the original data at a rate faster than $T^{-1/2}$, the fastest possible rate at which $\mu_{\gamma}$ can be estimated from observing $\gamma_1,\cdots,\gamma_T$ directly. Therefore, no test can consistently distinguish the model $\mu_{\gamma}=0$ (asymptotic variance $\sigma^2_v$) from a drifting sequence $\tilde{\mu}_{T,\gamma}\coloneq T^{-1/2}m_{\gamma}$ (asymptotic variance $m^2_{\gamma}+\sigma^2_v$).}
for the asymptotic distribution that achieves consistency uniformly over the space of distributions satisfying Assumption \ref{assump:integrability} and \ref{assump:restrictions_on_lowrankapprox}: 
\begin{itemize}
    \item Under Theorem \ref{thm:clt_samp_dist}, the sample mean $\bar{Y}_{NT}$ converges to a continuous limiting distribution $\mathcal{L}_0(\mathbf{q,c},\varrho)$ along sequences $f_{NT}\in \mathcal{F}$ with proper limits for $\mathbf{q}_{NT},\mathbf{c}_{NT}$
\end{itemize}

\subparagraph*{Bootstrap Consistency}
Consider the bootstrap analog of $\hat{S}_{NT,sel}$ in Eq. \ref{eq:asymp_variance}
\begin{equation*}
    \hat{S}^{2*}_{NT,sel}\coloneq \hat{D}_a(\kappa_a)T\hat{\sigma}^{2*}_a + \hat{D}_g (\kappa_g) N\hat{\sigma}^{2*}_g + \hat{\sigma}^{2*}_w
\end{equation*}
where $\hat{D}_a(\kappa_a),\hat{D}_g(\kappa_g)$ are fixed at the sample values, $\kappa_a,\kappa_g$ are chosen according to whether the bootstrap is \textbf{with} or \textbf{without} model selection. Consider 2 versions based on the studentized sample mean:
\begin{itemize}
    \item \myhl[myblue]{\textbf{non-pivotal}} bootstrap: approximating the distribution of \textbf{the sample mean} $r_{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right)$ with the bootstrap distribution $r_{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right)$
    \item \myhl[myblue]{\textbf{pivotal}} bootstrap: approximating the distribtuion of the \textbf{studentized sample mean} $\frac{(NT)^{1/2}}{\hat{S}_{NT,sel}}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right)$ with the boostrap distribution $\frac{(NT)^{1/2}}{\hat{S}^*_{NT,sel}}\left(\bar{Y}^*_{NT}- \bar{Y}_{NT}\right)$
\end{itemize}
And we can establish the consistency
\begin{theorem}{Bootstrap Consistency}{bootstrap_consistency}
    Under Assumption \ref{assump:integrability}, 
    \begin{itemize}
        \item[(a)] the bootstrap \myhl[myblue]{\textbf{with model selection}} satisfies 
        \begin{equation}\label{eq:bsconsistency_clt}
            \left\Vert \mathbb{P}^*_{NT}\left( r_{NT} \left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right) \right) - \mathbb{P}_{NT}\left( r_{NT} \left(\bar{Y}_{NT}-\mathbb{E}\left[\bar{Y}_{it}\right]\right) \right) \right\Vert _{\infty} \xrightarrow{\text{a.s.}} 0
        \end{equation}
        and its pivotal analog
        \begin{equation}\label{eq:bsconsistency_piv}
            \left\Vert  \mathbb{P}^*_{NT} \left( \sqrt{NT} \frac{\bar{Y}^*_{NT}-\bar{Y}_{NT}}{\hat{S}^*_{NT,sel}} \right) - \mathbb{P}_{NT} \left( \sqrt{NT} \frac{\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]}{\hat{S}_{NT,sel}} \right)   \right\Vert _{\infty} \xrightarrow{\text{a.s.}} 0
        \end{equation}
        \textbf{pointwise} for any fixed $\sigma^2_a,\sigma^2_g,\sigma^2_e,\sigma^2_v$
        \item[(b)] the bootstrap \myhl[myblue]{\textbf{without model selection}} satisfies Eq.\ref{eq:bsconsistency_clt} and Eq.\ref{eq:bsconsistency_piv} \textbf{uniformly} if $q_v=0$
        \item[(c)] the \myhl[myblue]{\textbf{conservative}} bootstrap satisfies
        \begin{equation}\label{eq:bscons_consistency_clt}
            \left\Vert \mathbb{P}^*_{NT}\left( r_{NT} \left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right) \right) - \mathcal{L}_0\left(\bar{\mathbf{q}},\mathbf{c},\varrho\right) \right\Vert _{\infty} \xrightarrow{\text{p}} 0
        \end{equation}
        and its pivotal analog
        \begin{equation}\label{eq:bscons_consistency_piv}
            \left\Vert  \mathbb{P}^*_{NT} \left( \sqrt{NT} \frac{\bar{Y}^*_{NT}-\bar{Y}_{NT}}{\hat{S}^*_{NT,sel}} \right) - \mathcal{L}_0\left(\bar{\mathbf{q}},\mathbf{c},\varrho\right) \right\Vert _{\infty} \xrightarrow{\text{p}} 0
        \end{equation}
        uniformly over the \textbf{entire parameter space}, where $\bar{q}=\left(q_c,\bar{q}_a,\bar{q}_g,0,0,\cdots\right)$, with $\bar{q}_a\coloneq \max\left\{\kappa_a/T,q_a\right\}$ and $\bar{q}_g \coloneq \max\left\{\kappa_g/T,q_g\right\}$, which increases as $N,T\rightarrow \infty$.
    \end{itemize}
\end{theorem}
Theorem \ref{thm:clt_samp_dist} gives that 
\begin{itemize}
    \item \myhl[myblue]{\textbf{bootstrap with model selection}}: pointwise valid asymptotically
    \item \myhl[myblue]{\textbf{bootstrap without model selection}}: valid uniformly w.r.t. clustering in means, but \textbf{inconsistent} if $q_v>0$
    \item \myhl[myblue]{\textbf{conservative bootstrap}}: uniformly valid without any qualifications. In degenerate cases $\left(q_e+q_v>0\right)$, the scale of the estimated asymptotic distribution \textbf{diverges} at a rate $\kappa_a/T + \kappa_g/N$
\end{itemize}
Notice that $\mathcal{L}_0\left(\bar{\mathbf{q}},\mathbf{c},\varrho\right)$ in Thm. \ref{thm:bootstrap_consistency} is a mean-preserving spread of $\mathcal{L}_0\left( \mathbf{q,c},\varrho\right)$ in Thm. \ref{thm:clt_samp_dist}, hence estimates of percentiles from the conservative bootstrap are \textbf{biased outwards} away from 0,
leading to asymptotic conservative CIs.

\subparagraph*{Refinements} Using standard results on Edgeworth expansions, get
\begin{proposition}{Refinements}{bscons_refinement}
    Under Assumption \ref{assump:integrability} for any $0 < \delta < \infty$, and the distributions of $a_i$ and $g_t$ satisfy Cramer's condition\footnote{Cramer's condition states that $\mathbf{X}$ has a non-degenerate, absolutely continuous component.}
    $$ \limsup_{\left\Vert t \right\Vert \rightarrow \infty} \left\vert \mathbb{E}\left[\exp(i\mathbf{t'X})\right] \right\vert <1 $$
    then if $\sigma^2_a + \sigma^2_g \geq C$ for some $C>0$, we have 
    \begin{equation*}
        \left\Vert \mathbb{P}^*_{NT}\left(\sqrt{NT} \frac{\bar{Y}^*_{NT}-\bar{Y}_{NT}}{\hat{S}^*_{NT,sel}} - \mathbb{P}_{NT}\left(\sqrt{NT}\frac{\bar{Y}_{NT}-\mathbb{E}[Y_{it}]}{\hat{S}_{NT,sel}}\right) \right) \right\Vert _{\infty} = O_p\left(r^{-2}_{NT} \vee (NT)^{-1/2} \right)
    \end{equation*}
    for all three versions of the bootstrap.
\end{proposition}

\subsubsection{Inference in Regression Models}
Consider the linear projection model
\begin{equation}\label{eq:menzel_linearmodel}
    y_{it} = \mathbf{x}'_{it}\boldsymbol{\beta} + u_{it}
\end{equation}
with the dependent variable $y_{it}$ and the vector of $k$ regressors $\mathbf{x}_{it}\in \mathbb{R}^k$. Consider LS estimator 
\begin{equation*}
    \hat{\boldsymbol{\beta}}_{LS} \coloneq \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y} = \boldsymbol{\beta} + \left(\mathbf{X'X}\right)^{-1}\left(\frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}\mathbf{x}_{it}u_{it}\right)
\end{equation*}
assume $\left(\mathbf{x}_{it}u_{it}\right)_{i,t}$ constitute a dissociated, separately exchangeable array, then we can have the Aldous-Hoover representation 
\begin{equation*}
    \mathbb{z}_{it} \coloneq \mathbf{x}_{it}u_{it} = f\left(\alpha_i,\gamma_t,\epsilon_{it}\right)
\end{equation*}
then denote
\begin{align*}
    \mathbf{a}_i & \coloneq \mathbb{E}\left[\mathbf{x}_{it}u_{it}\mid\alpha_i \right] & \mathbf{g}_t & \coloneq \mathbb{E}\left[\mathbf{x}_{it}u_{it}\mid\gamma_t\right] \\
    \mathbf{v}_{it} &\coloneq \mathbb{E}\left[\mathbf{x}_{it}u_{it}\mid\alpha_i,\gamma_t\right] - \mathbf{a}_i - \mathbf{g}_t & \mathbf{e}_{it} &\coloneq \mathbf{x}_{it}u_{it} - \mathbb{E}\left[\mathbf{x}_{it}u_{it}\mid \alpha_i,\gamma_t\right] \\
    \mathbf{w}_{it} & \coloneq \mathbf{x}_{it}u_{it} - \mathbf{a}_i -\mathbf{g}_t = \mathbf{v}_{it} + \mathbf{e}_{it}
\end{align*}
and the unconditional component variances as $\sigma^2_{al},\sigma^2_{gl},\sigma^2_{vl},\sigma^2_{el},\sigma^2_{wl}=\sigma^2_{vl} + \sigma^2_{el}$. The empirical analog of this decomposition is given by 
\begin{align*}
    \hat{\mathbf{a}}_i & \coloneq \frac{1}{T}\sum^T_{t=1}\mathbf{x}_{it}\hat{u}_{it} & \hat{\mathbf{g}_{t}} &\coloneq \frac{1}{N}\sum^N_{i=1}\mathbf{x}_{it}\hat{u}_{it} \\
    \hat{\mathbf{w}}_{it} & \coloneq \mathbf{x}_{it}\hat{u}_{it} - \hat{\mathbf{a}}_i -\hat{\mathbf{g}}_t
\end{align*}
for each $l=1,\cdots,k$, then construct
\subparagraph*{Bootstrap procedure for regression}
\begin{itemize}
    \item for the $b$th bootstrap iteration, draw $\mathbf{a}^*_{i,b}\coloneq \hat{\mathbf{a}}_{k^*_b(i)}$ and $\mathbf{g}^*_{t,b}\coloneq \hat{\mathbf{g}}_{s^*_b(i)}$ where $k^*_b(i)$ and $s^*_b(t)$ are i.i.d. draws from the discrete uniform distribution on the index sets $\left\{1,\cdots,N\right\}$ and $\left\{1,\cdots,T\right\}$, respectively
    \item generate $\mathbf{w}^*_{it,b} \coloneq \omega_{1i,b}\omega_{2t,b} \hat{\mathbf{w}}_{k^*_b(i)s^*_b(i)}$, where $\omega_{1i,b},\omega_{2t,b}$ are i.i.d. random variables with $\mathbb{E}\left[\omega\right] =0$, $\mathbb{E}[\omega^2]=\mathbb{E}[\omega^3]=1$
    \item simulate values of $\mathbf{z}^*_{it,b}=\left(z^*_{it1,b},\cdots,z^*_{itk,b}\right)'$, where the $l$th component is given by $$ z^*_{itl,b} \coloneq \sqrt{\hat{\lambda}_{al}}a^*_{il,b}+\sqrt{\hat{\lambda}_{gl}}g^*_{tl,b} +w^*_{itl,b}$$
    where the scalars are $\hat{\lambda}_{al}\coloneq \frac{\hat{D}_{al}(\kappa_a)T\hat{\sigma}^2_{al}}{\hat{D}_{al}(\kappa_a)T\hat{\sigma}^2_{al}+\hat{\sigma}^2_{wl}}$ and $\hat{\lambda}_{gl}\coloneq \frac{\hat{D}_{gl}(\kappa_g)N\hat{\sigma}^2_{gl}}{\hat{D}_{gl}(\kappa_g)N\hat{\sigma}^2_{gl}+\hat{\sigma}^2_{wl}}$
    \item then compute $$ \hat{\boldsymbol{\beta}}^*_{LS,b} = \hat{\boldsymbol{\beta}}_{LS} + \left(\mathbf{X'X}\right)^{-1}\left( \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}\mathbf{z}^*_{it,b} \right) $$ for each bootstrap sample.
\end{itemize}
Next, we can approximate the asymptotic distribution of $r_{NT}\left(\hat{\boldsymbol{\beta}}_{LS}-\boldsymbol{\beta}\right)$ with the simulated distribution of $r_{NT}\left(\hat{\boldsymbol{\beta}}^*_{LS,b}-\hat{\boldsymbol{\beta}}_{LS}\right)$.
\begin{assumption}{Regression}{bootstrap_regression}
    Assume the model in Eq.(\ref{eq:menzel_linearmodel}) with $\mathbf{x}_{it}u_{it}=f(\alpha_i,\gamma_t,\epsilon_{it})$ and $\alpha_i,\gamma_t,\epsilon_{it}$ are i.i.d. uniform on $[0,1]$. And 
    \begin{itemize}
        \item[(a)] $\mathbf{X}$ has full column rank
        \item[(b)] $\forall l=1,\cdots,k$ and some $\delta>0$, the $(4+\delta)$th absolute moments of $x_{itl}$ are bounded, and the $(4+\delta)$th conditional moments of each component $\frac{a_{il}}{\sqrt{\mathrm{Var}\left(a_{il}\mid\mathbf{X}\right)}}$, $\frac{g_{tl}}{\sqrt{\mathrm{Var}\left(g_{tl}\mid\mathbf{X}\right)}}$, $\frac{v_{itl}}{\sqrt{\mathrm{Var}\left(v_{itl}\mid\mathbf{X}\right)}}$ and $\frac{e_{itl}}{\sqrt{\mathrm{Var}\left(e_{itl}\mid\mathbf{X}\right)}}$ given $\mathbf{X}$ are bounded whenever the conditional variance of either component is strictly positive.
        \item[(c)] unconditional variance: $\mathrm{Var}(a_{il}) + \mathrm{Var}(g_{tl})>0$ or $\mathrm{Var}\left(w_{itl}\right)>0$ for each $l=1,\cdots,k$.
        \item[(d)] For each component of $\mathbf{z}_{it}=\mathbf{x}_{it}u_{it}$, there exists a spectral representation satisfying Assumption \ref{assump:restrictions_on_lowrankapprox} 
    \end{itemize}
\end{assumption}
then analogous to the sample mean inference, we have 
\begin{proposition}{Regression Inference}{regression_inf}
    Under Assumption \ref{assump:bootstrap_regression}, then 
    \begin{itemize}
        \item $\hat{\boldsymbol{\beta}}_{LS}$ is consistent at the $r_{NT}$ rate 
        \item The bootstrap with model selection satisfies Eq.(\ref{eq:bsconsistency_clt}) and (\ref{eq:bsconsistency_piv}) pointwise as $\sigma^2_{al},\sigma^2_{gl},\sigma^2_{el},\sigma^2_{vl}$ are held fixed for all $l=1,\cdots,k$
        \item The bootstrap without mode selection satisfies Eq.(\ref{eq:bsconsistency_clt}) and (\ref{eq:bsconsistency_piv}) uniformly if $q_{vl}=0$ for all $l=1,\cdots,k$
        \item The conservative bootstrap satisfies Eq.(\ref{eq:bscons_consistency_clt}) and (\ref{eq:bscons_consistency_piv}) uniformly over the entire parameter space
    \end{itemize}
\end{proposition}

\subparagraph*{Asymptotic Gaussian of the LS estimator} for conditional asymptotic normality of bilinear forms $V_k\coloneq \mathbf{Z}_{1k}'\mathbf{XZ}_{2k}$ of random vectors $\mathbf{Z}_{1k},\mathbf{Z}_{2k}$ given the matrix $\mathbf{X}$. 
Under the conditions of this paper, $V_k$ is asymptotically Gaussian if $\ddot{\mathbf{x}}_{it},\ddot{\mathbf{x}}_{js}$ are mean-independent for any $(j,s)\neq (i,t)$\footnote{For difference-in-differences designs with a regressor $x_{it1}\coloneq\mathbf{1}\left\{t\geq T_i\right\}$ for unit-specific intervention date $T_i$, or when $\mathbf{x}_{it}\coloneq\mathbf{x}\left(\boldsymbol{\xi}_i,\boldsymbol{\zeta}_t\right)$ are a non-additive function of row- and column-level attributes $\boldsymbol{\xi}_i$ and $\boldsymbol{\zeta}_t$, respectively, these conditions need not hold in general.}.

\section{Latest Development}
\subsection{LLN and CLT for Exchangeable Arrays}
\citet{davezies2021empirical} establish uniform LLN and CLT to show consistency and asymptotic normality of \textbf{nonlinear} estimators under weak regularity conditions.

\subsubsection{Set up}
\paragraph*{Notations}
For any $A\subset \mathbb{R}$, $B\subset \mathbb{R}^k$ for some $k\geq 2$, then let 
\begin{align*}
    A^+ &= A \cap \left(0,\infty\right)\\
    \bar{B} &= \left\{ b=\left(b_1,\cdots,b_k\right)\in B: \forall (i,j)\in\left\{1,\cdots,k\right\}^2,i\neq j, b_i\neq b_j \right\}
\end{align*}
and let 
\begin{itemize}
    \item $\mathbb{I}_k = \overline{\mathbb{N}^{+k}}$ denote the set of $k$-tuples of $\mathbb{N}^+$ \textbf{without} repetition
    \item for any $n\in \mathbb{N}^+$, let $\mathbb{I}_{n,k} = \overline{\left\{1,\cdots,n\right\}^k}$
    \item for any $\mathbf{i}=\left(i_1,\cdots,i_k\right),\mathbf{j}=\left(j_1,\cdots,j_k\right)$ in $\mathbb{N}^k$, let $\mathbf{i}\odot \mathbf{j} = \left(i_1j_1,\cdots,i_kj_k\right)$, and denote the distinct elements of $\mathbf{i}$ as $\left\{\mathbf{i}\right\}$
    \item for any $r\in \left\{1,\cdots,k\right\}$, let $$ \mathcal{E}_r = \left\{ \left(e_1,\cdots,e_k\right) \in\left\{0,1\right\}^k:\sum^k_{j=1}e_j = r \right\} $$
    \item for any $A\subset \mathbb{N}^+$, let $\mathfrak{S}(A)$ denote the set of permutations on $A$, then for any $\mathbf{i}=(i_1,\cdots,i_k)\in\mathbb{N}^{+k}$ and $\pi \in \mathfrak{S}(\mathbb{N}^+)$, let $\pi(\mathbf{i}) = \left(\pi(i_1),\cdots,\pi(i_k)\right)$
\end{itemize}
\paragraph*{Polyadic data}
For random variables $Y_{\mathbf{i}}$ indexed by $\mathbf{i}\in\mathbb{I}_k$\footnote{Some examples are: $Y_{i_1,i_2}$ corresponds to export flows from country $i_1$ to $i_2$, or whether there is a link between node $i_1$ and $i_2$ in a network. $\left\{i_1,\cdots,i_k\right\}$ can also correspond to the different dimensions of clustering.}, it's assumed that the random variables are generated according to a \myhl[myblue]{\textbf{jointly exchangeable}} and \myhl[myblue]{\textbf{dissociated}} array: 
\begin{assumption}{Jointly Exchangeable and Dissociated Arrays}{jexch_diss_array}
    For any $\pi \in \mathfrak{S}\left(\mathbb{N}^+\right)$, $$ \left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_k} \overset{\mathrm{d}}{=} \left(Y_{\pi(\mathbf{i})}\right)_{\mathbf{i}\in\mathbb{I}_k} $$
    and for any disjoint subsets of $\mathbb{N}^+$, $A,B$, with $\min\left(\left\vert A\right\vert,\left\vert B\right\vert\right)\geq k$, $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\overline{A^k}}$ is \textbf{independent} of $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\overline{B^k}}$
\end{assumption}
The assumption implies that 
\begin{itemize}
    \item \textbf{\underline{jointly exchangeability}}: the joint distribution of the data remains identical under any possible permutation of labels, i.e., labeling conveys no information
    \item \textbf{\underline{dissociation}}: the variables are indepednent if they have \textbf{no unit} in common, that is $Y_{i_1,i_2}$ must be independent of $Y_{j_1,j_2}$ if $\left\{i_1,i_2\right\} \cap \left\{j_1,j_2\right\} = \varnothing$
\end{itemize}
the dependence structure under such assumptions are 
\begin{lemma}{Key Dependence Structure}{davezies_depstruct}
    Assumption \ref{assump:jexch_diss_array} holds \textbf{if and only if} there exists i.i.d. variables $\left(U_J\right)_{J\subset \mathbb{N}^+,1\leq \left\vert J \right\vert \leq k}$ and a measurable function $\tau$ s.t. almost surely
    $$
    Y_{\mathbf{i}} = \tau\left( \left(U_{\left\{\mathbf{i}\odot \mathbf{e}\right\}^+}\right) _{\mathbf{e}\in\bigcup^k_{r=1}\mathcal{E}_r} \right),\forall \mathbf{i}\in\mathbb{I}_k
    $$
    this result is referred to as the AHK representation (Aldous 1981, Hoover 1979, Kallenberg 1989)\footnote{Consider dyadic data ($k=2$), then for every $i_1<i_2$ (the ranking is precise), $ Y_{i_1,i_2} = \tau\left(U_{i_1},U_{i_2},U_{ \left\{i_1,i_2\right\} }\right) $, that is, the outcome $Y$ depends of factors specific to $i_1$ and $i_2$, and factors relating both.}.
\end{lemma}

\subsubsection{Uniform LLN and CLT}
Let $\mathcal{F}$ denote a class of real-valued functions admitting a first moment w.r.t. the distribution $P$, let $Pf$ denote the corresponding moment $\mathbb{E}\left[f(Y_{\mathbf{1}})\right]$, with $\mathbf{1}$ as the $k-$tuple $(1,\cdots,k)$. Assume that
\begin{assumption}{Measurability Assumption}{measurability_assumption}
    $\exists$ a countable subclass $\mathcal{G}\subset \mathcal{F}$ s.t. elements of $\mathcal{F}$ are pointwise limits of sequences of elements of $\mathcal{G}$
\end{assumption}
Consider
\begin{align*}
    \mathbb{P}_n f &= \frac{(n-k)!}{n!} \sum_{\mathbf{i}\in \mathbb{I}_{n,k}}f(Y_{\mathbf{i}}) \\
    \mathbb{G}_n f &= \sqrt{n}\left(\mathbb{P}_n f - Pf\right)
\end{align*}
and the restrictions on $\mathcal{F}$: for any $\eta>0$ and any seminorm $\left\Vert \cdot \right\Vert$ on a space containing $\mathcal{F}$, let
\begin{itemize}
    \item $N\left(\eta, \mathcal{F}, \left\Vert \cdot \right\Vert\right)$: the minimal number of $\left\Vert \cdot \right\Vert$-closed balls of radius $\eta$ with centers in $\mathcal{F}$ needed to cover $\mathcal{F}$
    \item $N_{[]}\left(\eta, \mathcal{F}, \left\Vert \cdot \right\Vert\right)$: the minimal number of $\eta-$brackets needed cover $\mathcal{F}$, where an $\eta-$bracket for $f\in\mathcal{F}$ is a pair of functions $\left(l,u\right)$ s.t. $l\leq f\leq u$ and $\left\Vert u-l \right\Vert<\eta$
\end{itemize}
\citet{davezies2021empirical} considered the seminorms $\left\Vert f \right\Vert _{\mu,r}=\left(\int \left\vert f \right\vert^t \mathrm{d}\mu\right)^{1/r}$ for any $r\geq 1$ and probability measure (cdf) $\mu$. An envelope of $\mathcal{F}$ is measurable function $F$ satisfying $F(u) \geq \sup_{f\in\mathcal{F}}\left\vert f(u) \right\vert$, satisfying 
\begin{assumption}{Assumptions of $\mathcal{F}$}{assumptions_on_f}
    \begin{itemize}
        \item[\textbf{A}] The class $\mathcal{F}$  
        \begin{itemize}
            \item[(i)] either admits an envelope $F$ with $PF<\infty$ and $\forall \eta >0$, $$ \sup_{Q\in\mathcal{Q}}N\left( \eta\left\Vert F \right\Vert _{Q,1},\mathcal{F},\left\Vert \cdot \right\Vert _{Q,1} \right) <\infty $$ 
            \item[(ii)] or satisfies $N_{[]}\left(\eta,\mathcal{F},\left\Vert\cdot \right\Vert _{L_1\left(P\right)}\right)<\infty$ for all $\eta>0$ 
        \end{itemize}
        \item[\textbf{B}] and it 
        \begin{itemize}
            \item[(i)] \myhl[myblue]{\textbf{uniform entropy integral}}: either admits an envelope $F$ with $PF^2<\infty$ and $$ \int^{\infty}_0 \sup_{Q\in\mathcal{Q}} \sqrt{\log N\left(\eta \left\Vert F \right\Vert _{Q,2},\mathcal{F},\left\Vert \cdot \right\Vert _{Q,2} \right)} \mathrm{d}\eta <\infty $$
            \item[(ii)] \myhl[myblue]{\textbf{bracketing entropy integral}}: or satisfies $\int^{\infty}_0 \sqrt{\log N_{[]} \left(\eta,\mathcal{F},\left\Vert \cdot \right\Vert _{L_2(P)}\right)} \mathrm{d}\eta < \infty$ 
        \end{itemize}
    \end{itemize}
\end{assumption}
Assumption \ref{assump:assumptions_on_f} are same as the conditions imposed on i.i.d. data for uniform LLNs and CLTs. Under these assumptions, \citet{davezies2021empirical} established the uniform LLNs and CLTs as 
\begin{theorem}{Uniform LLNs and CLTs}{uniform_lln_clt}
    Under Assumption \ref{assump:jexch_diss_array} and \ref{assump:measurability_assumption},
    \begin{itemize}
        \item if (\textbf{A}) of Assumption \ref{assump:assumptions_on_f} holds, $\sup_{f\in\mathcal{F}} \left\vert \mathbb{P}_nf - Pf \right\vert \xrightarrow{\mathbf{a.s.}} 0$ and in $L^1$
        \item if (\textbf{B}) of Assumption \ref{assump:assumptions_on_f} holds, $\mathbb{G}_n$ converges weakly in $l^{\infty}(\mathcal{F})$ to a centered Gaussian process $\mathbb{G}$ on $\mathcal{F}$ as $n\rightarrow \infty$, the convariance kernel $K$ of $\mathbb{G}$ satisfies 
        $$
        K(f_1,f_2) = \frac{1}{(k-1)!^2} \sum_{\left(\pi,\pi'\right) \in \mathfrak{S}\left(\left\{\mathbf{1}\right\}\right)\times \mathfrak{S}\left(\left\{\mathbf{1}'\right\}\right) }\mathrm{Cov}\left( f_1 \left(\mathbf{Y}_{\pi(\mathbf{1})}\right), f_2 \left(\mathbf{Y}_{\pi '(\mathbf{1}')}\right) \right)
        $$
    \end{itemize}
\end{theorem}
Here, (\textbf{A}) of Assumption \ref{assump:assumptions_on_f} is stronger than necessary to obtain the uniform LLNs. To establish the exact characterization, consider the norms:
\begin{align*}
    \left\Vert f \right\Vert _{1,1} &= \frac{1}{n} \sum^n_{i_1=1} \left\vert \frac{1}{n-1} \sum_{i_2 \neq i_1} f(Y_{i_1,i_2}) + f(Y_{i_2,i_1}) \right\vert \\
    \left\Vert f \right\Vert _{1,2} &= \frac{1}{n(n-1)} \sum_{1\leq i_1<i_2 \leq n} \left\vert \mathbb{E}\left[  f(Y_{i_1,i_2}) + f(Y_{i_2,i_1}) \right] \mid U_{\left\{ i_1,i_2 \right\}} \right\vert
\end{align*}
and the exact characterization is established as 
\begin{proposition}{Exact Characterization of Uniform LLNs}{uni_lln_exactchar}
    Under Assumption \ref{assump:jexch_diss_array} and \ref{assump:measurability_assumption}, and $\mathcal{F}$ admits an envelop $F$ with $PF<\infty$, then $$\sup_{f\in\mathcal{F}} \left\vert \mathbb{P}_n f -Pf \right\vert \xrightarrow{\mathrm{a.s.}} 0$$ \textbf{if and only if} both $\log N\left(\epsilon, \mathcal{F}, \left\Vert \cdot \right\Vert _{1,2}\right)/n^2$ and $\log N\left(\epsilon, \mathcal{F}, \left\Vert \cdot \right\Vert _{1,1}\right)/n$ tend to 0 in outer probability.
\end{proposition}
and 2 aspects of dissociated, exchangeable arrays are emphasized:
\begin{itemize}
    \item \textbf{i.i.d. variations}: through the random entropy term related to $\left\Vert \cdot \right\Vert _{1,2}$, which only involves $\left(U_{\left\{i_1,i_2\right\}}\right)_{\mathbf{i}\in\mathbb{I}_{n,2}}$
    \item \textbf{U-statistic}: through the random entropy term related to $\left\Vert \cdot \right\Vert _{1,1}$, up to negligible terms, $\left\Vert f \right\Vert _{1,1}$ only depends on $\left(U_{i_1}\right)_{1\leq i_1 \leq n}$
\end{itemize}

\subsubsection{Convergence of the bootstrap process} \citet{davezies2021empirical}, extending the pigeonhole bootstrap \citep{mccullagh2000resampling,owen2007pigeonhole}, established the following bootstrap process:
\begin{itemize}
    \item[1] $n$ units are sampled independently in $\left\{1,\cdots,n\right\}$ with replacement and equal probability, $W_i$ denotes the number of times unit $i$ is sampled.
    \item[2] the $k-$tuple $\mathbf{i}= \left(i_1,\cdots,i_k\right)\in\mathbb{I}_{n,k}$ is then selected $W_i=\prod^k_{j=1}W_{i_j}$ times in the bootstrap sample 
\end{itemize}
then consider $\mathbb{P}^*_n$ and $\mathbb{G}^*_n$ defined on $\mathcal{F}$ by 
\begin{align*}
    \mathbb{P}^*_n f &= \frac{(n-k)!}{n!}\sum_{\mathbf{i}\in\mathbb{I}_{n,k}}W_{\mathbf{i}} f(Y_{\mathbf{i}})\\
    \mathbb{G}^*_n f &= \sqrt{n}\left(\mathbb{P}^*_{n} f - \mathbb{P}_n f\right)
\end{align*}
the validity of the bootstrap is then established as:
\begin{theorem}{Bootstrapping Validity}{bootstrap_validity}
    Under Assumption \ref{assump:jexch_diss_array} and \ref{assump:measurability_assumption}, if (\textbf{B-i}) of Assumption \ref{assump:assumptions_on_f} also holds, the process $\mathbb{G}^*_n$ converges weakly in $l^{\infty}(\mathcal{F})$ to $\mathbb{G}$, conditional on $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_k}$ and outer almost surely.
\end{theorem}
The proof of this theorem boils down to proving
$$
\sup_{h\in BL_1} \left\vert \mathbb{E}\left(h\left(\mathbb{G}^*_n\right) \mid \left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in \mathbb{I}_k}\right)  - \mathbb{E}\left(h(\mathbb{G})\right) \right\vert \xrightarrow{\mathrm{a.s.}_{outer}} 0 
$$
where $BL_1$ is the set of bounded and Lipschitz functions from $l^{\infty}(\mathcal{F})$ to $[0,1]$. With the standard bootstrap for i.i.d. data
$$
\mathbb{E} \left[\mathbb{P}^*_n(f)\mid \left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_k}\right] = \frac{1}{n^k} \sum_{\mathbf{i}\in\mathbb{I}_{n,k}} f(Y_{\mathbf{i}}) \xrightarrow{n\rightarrow \infty} \mathbb{P}_n f
$$
hence, \citet{davezies2021empirical} established the a.s. conditional convergence of $\sqrt{n}\left( \mathbb{P}^*_n f - \frac{1}{n^k} \sum_{\mathbf{i}\in\mathbb{I}_{n,k}} f(Y_{\mathbf{i}}) \right)$.

\subsubsection{Nonlinear estimators}
\citet{davezies2021empirical} considered 2 classes of estimators: \textbf{Z-estimators} and \textbf{smooth functionals of the empirical cdf}:
\paragraph*{Z-estimators} 
Let 
\begin{itemize}
    \item $\Theta$ denote a normed space, endowed with norm $\left\Vert \cdot \right\Vert _{\Theta}$
    \item $\left(\psi_{\theta,h}\right)_{(\theta,h)\in\Theta\times \mathcal{H}}$ denote a class of real, measurable functions
    \item $\Psi(\theta)(h)=P\psi_{\theta,h}$, $\Psi_n(\theta)(h) = \mathbb{P}_n \psi_{\theta,h}$, $\Psi^*_n(\theta)(h)=\mathbb{P}^*_n\psi_{\theta,h}$
    \item for any real function $g$ on $\mathcal{H}$, $\left\Vert g \right\Vert _{\mathcal{H}} = \sup_{h\in\mathcal{H}}\left\vert g(h) \right\vert$
\end{itemize}
The parameter of interest $\theta_0$, satisfying $\Psi(\theta_0) = 0$, is estimated by $\hat{\theta} = \arg\min_{\theta\in\Theta} \left\Vert \Psi_n(\theta) \right\Vert _{\mathcal{H}}$ , define the bootstrap counterpart of $\hat{\theta}$ as $$ \hat{\theta}^* = \arg\min_{\theta\in\Theta} \left\Vert \Psi^*_n\left(\theta\right) \right\Vert _{\mathcal{H}} $$
then we have the convergence 
\begin{theorem}{Convergence of Z-estimators Bootstrap}{z_est_boot_conv}
    Under Assumption \ref{assump:jexch_diss_array}, if also
    \begin{itemize}
        \item[1] $\left\Vert \Psi(\theta_m) \right\Vert _{\mathcal{H}} \rightarrow 0 \Rightarrow \left\Vert \theta_m-\theta_0 \right\Vert _{\Theta} \rightarrow 0$, $\forall (\theta_m)_{m\in\mathbb{N}}\in \Theta$ 
        \item[2] $\left\{\psi_{\theta,h}:(\theta,h) \in \Theta \times \mathcal{H}\right\}$ satisfies Assumption \ref{assump:measurability_assumption} and (A) of \ref{assump:assumptions_on_f}, with $PF< \infty$
        \item[3] $\exists\delta>0$ s.t. $\left\{\psi_{\theta,h}:\left\Vert\theta-\theta_0\right\Vert _{\Theta}<\delta,h\in\mathcal{H}\right\}$ satisfies Assumption \ref{assump:measurability_assumption} and (B) of \ref{assump:assumptions_on_f}, with $P F^2_{\delta} < \infty$
        \item[4] $\lim_{\theta\rightarrow \theta_0}\sup_{h\in\mathcal{H}} P\left(\psi_{\theta,h}-\psi_{\theta_0,h}\right)^2 =0$
        \item[5] $\forall \eta>0$, $\left\Vert \Psi_n \left(\hat{\theta}\right) \right\Vert _{\mathcal{H}} = o_p(n^{-1/2}) $ and $P\left( \left\Vert \sqrt{n} \Psi^*_n\left(\hat{\theta}^*\right) \right\Vert _{\mathcal{H}} > \eta\left\vert \left(Y_\mathbf{i}\right)_{\mathbf{i}\in\mathbb{I}_k} \right\vert \right) = o_p(1)$
        \item[6] $\theta\mapsto \Psi(\theta)$ is Frechet-differentiable at $\theta_0$, with continuously invertible derivative $\dot{\Psi}_{\theta_0}$
    \end{itemize}
    Then the convergence can be established as
    \begin{itemize}
        \item $\sqrt{n}\left(\hat{\theta}-\theta\right)$ converges in distribution to a centered Gaussian process $\mathbb{G}$
        \item conditional on $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_k}$, $\sqrt{n}\left(\hat{\theta}^*-\hat{\theta}\right) \xrightarrow{d} \mathbb{G}$ almost surely
    \end{itemize}
\end{theorem}

\paragraph*{Smooth functionals of $F_Y$} For the cdf of $Y_{\mathbf{i}}$, suppose that $\mathcal{Y}\subset \mathbb{R}^p$ for some $p\in\mathbb{N}^+$ and $\theta_0 = g(F_Y)$, where $g$ is Hadamard differentiable\footnote{No linearity assumed under Hadamard differentiability.}. Estimate $\theta_0$ with $\hat{\theta}=g\left(\hat{F}_Y\right)$, where $\hat{F}_Y$ denotes the empirical cdf of $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_{n,k}}$, and let $\hat{\theta}^*$ denote the bootstrap counterpart of $\hat{\theta}$.
\citet{davezies2021empirical} established the convergence results as 
\begin{theorem}{Convergence of Smooth Functionals of the Empirical CDF}{conv_smoothcdf}
    Suppose that $g$ is Hadamard differentiable at $F_Y$ tangentially to a set $\mathbb{D}_0$, with derivative equal to $g'_{F_Y}$. Under Assumption \ref{assump:jexch_diss_array},
    \begin{itemize}
        \item $\sqrt{n}\left( \hat{F}_Y-F_Y \right)$ converges weakly, as a process indexed by $y$, to a Gaussian process $\mathbb{G}$ with kernel $K$ satisfying 
        $$
        K\left(y_1,y_2\right) = \frac{1}{(k-1)!^2}\sum_{\left(\pi,\pi'\right)\in \mathfrak{S}\left(\left\{\mathbf{1}\right\}\right) \times \mathfrak{S}\left(\left\{\mathbf{1}'\right\}\right)}\mathrm{Cov}\left( \mathbf{1}_{\left\{Y_{\pi(\mathbf{1})\leq y_1}\right\}},\mathbf{1}_{\left\{Y_{\pi '(\mathbf{1}')\leq y_2}\right\}} \right)
        $$
        \item If $\mathbb{G}\in \mathbb{D}_0$\footnote{In practice, $\mathbb{D}_0$ often corresponds to the set of functions that are continuous everywhere or at a certain point $y_0$.} with probability 1, $$ \sqrt{n}\left(\hat{\theta}-\theta_0\right) \xrightarrow{\mathrm{d}} \mathcal{N}\left(0, \mathbb{V}\left(g'_{F_Y}\left(\mathbb{G}\right)\right)\right) $$
    \end{itemize}
    conditional on $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_k}$, $\sqrt{n}\left(\hat{\theta}^*-\hat{\theta}\right)\xrightarrow{\mathrm{d}} \mathcal{N}\left(0, \mathbb{V}\left(g'_{F_Y}\left(\mathbb{G}\right)\right)\right)$, almost surely.
\end{theorem}

\subsubsection{Extensions}
\citet{davezies2021empirical} also considered several extensions of the main results:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Degenerate cases}}: consider the simple $k=2$ situations where $K(f,f)=0, \forall f\in\mathcal{F}$. 
    Generally, when $K(f,f)=0$, the rate of convergence of $\mathbb{P}_nf -Pf$ is $n^{-1}$ rather than $n^{-1/2}$, and the asymptotic distribution is not necessarily normal. $\forall (i_1,i_2)\in\mathbb{I}_2$, let $Y_{i_1,i_2}=\tau\left( U_{i_1},U_{i_2},U_{\left\{i_1,i_2\right\}} \right)$ be the Aldous-Hoover-Kallenberg representation. WLoG, assume $U_{\cdot}$ to be uniform on $[0,1]$.
    %\footnote{This degeneracy appears if the variables in the array are actually \textbf{i.i.d.} in which case $\sqrt{n}\mathbb{G}_n$ converges to a Guassian process with covariance kernel $K(f_1,f_2)=\mathrm{Cov}\left(f_1(Y_{1,2}),f_2(Y_{1,2})\right)$. \citet{menzel2021bootstrap} assumes a special case where $Y_{i_1,i_2}=X_{i_1}X_{i_2}$, with $\left(X_i\right)_{i\in\mathbb{N}^+}$ i.i.d. with $\mathbb{E}\left(X_1\right)=0$, $\mathbb{V}\left(X_1\right)=1$, also $\mathcal{F}=\left\{ f_{\lambda}(x)=\lambda x,\lambda\in I \right\}$ for a compact $I\subset \mathbb{R}$. In this case, $\sqrt{n}\mathbb{G}_n$ converges weakly in $l^{\infty}\left(\mathcal{F}\right)$ to $\mathbb{G}\left(f_{\lambda}\right)=\lambda\left(Z^2-1\right)$ with a standard normal $Z$.}
    
    Under a more stringent version of (\textbf{B}-i) Assumption \ref{assump:assumptions_on_f}, that is, $\mathcal{F}$ admits an envelope $F$ with $PF^2< \infty$ and 
    $$
    \int^{\infty}_0 \sup_{Q\in\mathcal{Q}} \log N\left(\eta \left\Vert F \right\Vert _{Q,2},\mathcal{F}, \left\Vert \cdot \right\Vert _{Q,2}\right)\mathrm{d}\eta <\infty
    $$
    \citet{davezies2021empirical} showed that for $k=2$, $K(f,f)=0$ for all $f\in\mathcal{F}$, $\sqrt{n}\mathbf{G}_n$ converges \textbf{weakly} in $l^{\infty}(\mathcal{F})$ to $\mathbb{G}^d$. However, the proposed bootstrap process does \textbf{not} generally converge to $\mathbb{G}^d$.
    \item \myhl[myblue]{\textbf{An alternative bootstrap process}}: \citet{davezies2021empirical} established that under Assumption \ref{assump:jexch_diss_array} and \ref{assump:measurability_assumption}, $Pf^2<\infty$ for all $f\in \mathcal{F}$ and $\mathcal{F}$ admits an envelope $F$ s.t. $PF^{1+\delta}<\infty$ for some $\delta>0$, then if conditional on $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{I}_k}$, the process $\mathbb{G}_n^*$ outer almost surely converges weakly in $l^{\infty}\left(\mathcal{F}\right)$ to a centered Gaussian process $\mathbb{G}$, the process $\mathbb{G}_n$ also converges weakly in $l^{\infty}(\mathcal{F})$ to $\mathbb{G}$. 
    Combined with Theorem \ref{thm:bootstrap_validity}, we have $$ \mathbb{G}_n \rightarrow \mathbb{G} \Leftrightarrow \mathbb{G}^*_n \xrightarrow{\mathrm{a.s.}_{outer}} \mathbb{G} $$
    consider an alternative, the multiplier bootstrap process adapted to jointly exchangeable arrays. Let $\left(\xi_i\right)^n_{i=1}$ be a sequence of i.i.d. random variables, independent from the original data $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in \mathbb{I}_{n,2}}$, then the process 
    \begin{equation*}
        \mathbb{G}^{m*}_{n}:f \mapsto \frac{1}{\sqrt{n}}\sum^n_{i_1=1} \xi_{i_1} \left(\frac{1}{n-1}\sum_{1\leq i_2\neq i_1\leq n} \left[f(Y_{i_1,i_2})+f(Y_{i_2,i_1})\right] - 2\mathbb{P}_n f\right)
    \end{equation*}
    also outer almost surely converges weakly in $l^{\infty}(\mathcal{F})$ to $\mathbb{G}$, just as the proposed process $\mathbb{G}_n^*$.
    \item \myhl[myblue]{\textbf{Separately exchangeable arrays}} For the case of separately exchangeable arrays (the $n$ units stem from $k$ \textbf{different} populations, or multiway clustering as in \citet{menzel2021bootstrap}), we must assume a stronger version of Assumption \ref{assump:jexch_diss_array}
    \begin{assumption}{Stronger assumptions for separately exchangeable arrays}{strong_sep_exc_array}
        Consider randome variables $Y_{\mathbf{i}}$ where $\mathbf{i}=\left(i_1,\cdots,i_k\right) \in \mathbb{N}^{+k}$, implying that repetitions are allowed. Then assume that for any $\left(\pi_1,\cdots,\pi_k\right)\in\mathfrak{S}\left(\mathbb{N}^+\right)^k$,
        $$ \left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in\mathbb{N}^{+k}} \overset{d}{=} \left(Y_{\pi_1(i_1),\cdots,\pi_k(i_k)}\right)_{\mathbf{i}\in\mathbb{N}^{+k}} $$
        and for any $A,B$, disjoint subsets of $\mathbb{N}^+$, $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in A^k}$ is independent of $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in B^k}$.
    \end{assumption}
    Under this stronger assumption, we have equality in distribution even for $\pi_1 = \cdots =\pi_k$. Here, denote 
    \begin{itemize}
        \item $\mathbf{1} = \left(1,\cdots,1\right)$
        \item $\mathbf{n} = \left(n_1,\cdots,n_k\right)$, where $n_j\geq 1$ denotes the number of units observed in population (or cluster) $j$, in general, $n_j\neq n_{j'}$ for $j\neq j'$
        \item sample at hand: $\left(Y_{\mathbf{i}}\right)_{\mathbf{1\leq i\leq n}}$, where $\mathbf{i}\geq \mathbf{i}'$ means that $i_j\geq i'_j$, $\forall j=1,\cdots,k$.
        \item $\underbar{n} = \min\left(n_1,\cdots,n_k\right)$
    \end{itemize}
    then the empirical measure and empirical process for separately exchagneable arrays are 
    \begin{align*}
        \mathbb{P}_nf &=\frac{1}{\prod^k_{j=1}n_j}\sum_{\mathbf{1\leq i\leq n}}f(Y_{\mathbf{i}}) & \mathbb{G}_n f& = \sqrt{\underbar{n}}\left(\mathbb{P}_n f-Pf\right)
    \end{align*}
    Consider the \textbf{pigeonhole bootstrap} process \citep{mccullagh2000resampling}, which is close the bootstrap in Theorem \ref{thm:bootstrap_validity}, except that weights are now independent from one coordinate to another:
    \begin{itemize}
        \item[1] For each $j\in \left\{1,\cdots,k\right\}$, $n_j$ elements are sampled with replacement and equal probability in the set $\left\{1,\cdots,n_j\right\}$. For each $i_j$, let $W_{i_j}^j$ denote the number of times $i_j$ is selected 
        \item[2] $k$-tuple $\mathbf{i}=\left(i_1,\cdots,i_k\right)$ is then selected $W_{\mathbf{i}}= \prod^k_{j=1} W_{i_j}^j$ times in the bootstrap sample 
    \end{itemize}
    the bootstrap process $\mathbb{G}^*_{\mathbf{n}}$ is then defined on $\mathcal{F}$ by 
    \begin{equation*}
        \mathbb{G}^*_{\mathbf{n}} f = \sqrt{\underbar{n}} \left(\frac{1}{\prod^k_{j=1}n_j} \sum_{\mathbf{1\leq i\leq n}}\left(W_{\mathbf{i}}-1\right) f(Y_{\mathbf{i}}) \right)
    \end{equation*}
    as with multisample U-statistics, assume an index $m\in \mathbb{N}^+$ and increasing functions $g_1,\cdots,g_k$ s.t. for all $j$, $n_j=g_j\left(m\right)\xrightarrow{m\rightarrow\infty} \infty$, and w.l.o.g., $\forall m\in\mathbb{N}^+$, $\exists j$ s.t. $g_j(m+1)>g_j(m)$, then  
    \begin{theorem}{Bootstrap Convergence: Separately Exchangeable Arrays}{bootstrap_conv_sepexcharray}
        Under Assumption \ref{assump:measurability_assumption} and \ref{assump:strong_sep_exc_array} and for every $j=1,\cdots,k$, $\exists\lambda_j\geq0$ s.t. $\underbar{n}/n_j \rightarrow \lambda_j \geq 0$, then 
        \begin{itemize}
            \item[1] If (A) of Assumption \ref{assump:assumptions_on_f} holds, $\sup_{f\in\mathcal{F}} \left\vert \mathbb{P}_{\mathbf{n}}f - Pf \right\vert \xrightarrow{\mathrm{a.s.}} 0$ and in $L^1$
            \item[2] If (B-i) of Assumption \ref{assump:assumptions_on_f} holds, the process $\mathbb{G}_n$ coverges weakly in $l^{\infty}(\mathcal{F})$ to a centered Gaussian process $\mathbb{G}_{\lambda}$ on $\mathcal{F}$ as $n\rightarrow\infty$, and the covariance kernel $K_{\lambda}$ of $\mathbb{G}_{\lambda}$ satisfies 
            \begin{equation*}
                K_{\lambda}\left(f_1,f_2\right) = \sum^k_{j=1}\lambda_j \mathrm{Cov}\left(f_1\left(Y_{\mathbf{1}}\right),f_1\left(Y_{\mathbf{2}_j}\right)\right)
            \end{equation*}
            where $\mathbf{2}_j$ is the $k$-tuple with 2 in each entry but 1 in entry $j$.
            \item[3] If (B-i) of Assumption \ref{assump:assumptions_on_f} holds, $\mathbb{G}^*_n \rightarrow \mathbb{G}_{\lambda}$ weakly, conditional on $\left(Y_{\mathbf{i}}\right)_{\mathbf{i}\in \mathbb{N}^{+k}}$ and outer almost surely
        \end{itemize}
    \end{theorem}
    Here, the case where $\lambda_j=0$ for some $j$ corresponds to \textbf{strongly unbalanced} designs with different rates of convergence to $\infty$ along the different dimensions of the array. In such case, only the dimensions with the slowest rate of convergence contribute to the asymptotic distribution.
\end{itemize}

\subsection{CLT for the estimator with heterogeneous clusters}
\citet{yap2023general} provides general conditions such that the plug-in mean estimator is asymptotically normal, and the \citet{cameron2011robust} variance estimator is consistent even when clusters are heterogeneous. The conditions mimic one-way clustering conditions, assuming that two observations are independent when they do not share any cluster, and \textbf{\textcolor{red}{not}} assuming separate exchangeability.

\subsubsection{Setting}
Consider for vectors $\left\{\mathbf{W}_i\right\}^n_{i=1}$, where $\mathbf{W}_i \coloneq  ( W_{i1},\cdots, W_{iK} )'\in \mathbb{R}^K $ and $i$ is the unit of observation, for the population of size $n$. The goal is to establish a central list theorem (CLT) for a 
weighted sum of the random vector, $\sum_i \omega_i \mathbf{W}_i$ where $\omega_i$ are non-stochastic scalar weights as $n\rightarrow \infty$.

\paragraph*{Notation}
Consider 2 clustering dimension $G$ and $H$, let 
\begin{itemize}
    \item $g(i),h(i)$ denote the cluster where observations $i$ belongs on the $G$ and $H$ dimensions, respectively
    \item For $C\in \left\{G,H\right\}$, let $\mathcal{N}^C_c$ denote the set of observations in cluster $c$ on dimension $C$
\end{itemize}


\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}