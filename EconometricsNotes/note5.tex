\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{5}{Two-Way Cluster-Robust (TWCR) Standard Errors}{}{Sai Zhang}{The validity of Two-Way Cluster-Robust (TWCR) standard errors}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{One-Way Clustering}
First, consider the case of one-way clustering. The linear model with one-way clustering $$ y_{ig} = \mathbf{x}_{ig}\boldsymbol{\beta} + u_{ig} $$
where $i$ denotes the $i$th of the $N$ individuals in the sample, $j$ denotes the $g$th of the $G$ clusters, assume that
\begin{itemize}
    \item $\mathbb{E}\left[u_{ig}\mid \mathbf{x}_{ig}\right] =0$
    \item error independence across clusters: for $i\neq j$
    \begin{equation}\label{eq:error_independence}
        \mathbb{E}\left[ u_{ig} u_{jg'}\mid \mathbf{x}_{ig},\mathbf{x}_{jg'} \right] = 0
    \end{equation}
    unless $g=g'$, that is, errors for individuals within the same cluster may be correlated.
\end{itemize}
Grouping observations by cluster, get
$$
\mathbf{y}_g = \mathbf{X}_g \boldsymbol{\beta} + \mathbf{u}
$$
where $\mathbf{X}_g$ has dimension $N_g\times K$ and $\mathbf{y}_g$ has dimension $N_g \times 1$, with $N_g$ observations in cluster $g$. 
Stacking over cluster, get the matrix form of the model
$$
\mathbf{y=X}\boldsymbol{\beta}+\mathbf{u}
$$
with $\mathbf{y,u}$ being $N\times 1$ vectors, $\mathbf{X}$ being an $N\times K$ matrix. OLS estimator gives 
\begin{equation}\label{eq:OLSest}
    \hat{\boldsymbol{\beta}} = \left(\mathbf{X'X}\right)^{-1}\mathbf{X'y}=\left( \sum^G_{g=1}\mathbf{X}_g'\mathbf{X}_g \right)^{-1} \sum^G_{g=1}\mathbf{X}'_g\mathbf{y}_g
\end{equation}
then, by CLT, we have that $\sqrt{G} \left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right) \xrightarrow{d} \mathcal{N}(0,\boldsymbol{\Sigma})$ where the variance matrix of the limit normal distribution $\boldsymbol{\Sigma}$ is 
\begin{equation}\label{eq:limit_varcovmat}
    \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right] \right)^{-1} \left(\lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{u}'_g\mathbf{u}_g\mathbf{X}_g\right] \right) \times \left( \lim_{G\rightarrow\infty}\frac{1}{G}\sum^G_{g=1} \mathbf{E}\left[\mathbf{X}'_g\mathbf{X}_g\right]  \right)^{-1}
\end{equation}
If the primary source of clustering is due to group-level common shocks, a useful approximation is that for the $j$th regressor, the default OLS variance estimate based on $s^2 \left(\mathbf{X'X}\right)^{-1}$ should be inflated by $\tau_j \simeq 1+\rho_{x_j}\rho_u\left(\bar{N}_g -1\right)$, where 
\begin{itemize}
    \item $s$ is the estimated standard deviation of the error
    \item $\rho_{x_j}$ is a measure of within-cluster correlation of $x_j$
    \item $\rho_u$ is the within-cluster error correlation 
    \item $\bar{N}_g$ is the average cluster size
\end{itemize}
It's easy to see the $\tau_j$ can be large even with small $\rho_u$ \citep{kloek1981ols,scott1982effect,moulton1990illustration}. If assume the model for the cluster error variance matrices $\boldsymbol{\Omega}_g = \mathbb{V}\left[\mathbf{u}_g \mid \mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{u}_g\mathbf{u}_g'\mid \mathbf{X}_g\right]$, 
and there is a consistent estimate $\hat{\boldsymbol{\Omega}}_g$ of $\boldsymbol{\Omega}_g$, we can estimate $\mathbb{E}\left[\mathbf{X}_g'\mathbf{u}_g\mathbf{u}_g'\mathbf{X}_g\right] = \mathbb{E}\left[\mathbf{X}_g'\boldsymbol{\Omega}_g \mathbf{X}_g\right]$ via GLS.

\paragraph*{Cluster-robust variance matrix estimate} consider 
\begin{equation}\label{eq:oneway_clurob}
    \hat{\mathbb{V}} \left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\left(\sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g \right) \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where $\hat{\mathbf{u}}_g = \mathbf{y}_g - \mathbf{X}_g\hat{\boldsymbol{\beta}}$. This estimate is consistent if $$ G^{-1}\sum^G_{g=1}\mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g - G^{-1}\sum^G_{g=1}\mathbb{E}\left[ \mathbf{X}_g' \mathbf{u}_g \mathbf{u}_g' \mathbf{X}_g \right] \xrightarrow{\mathrm{p}} \mathbf{0} $$ as $G\rightarrow \infty$. 
An informal presentation of Eq.(\ref{eq:oneway_clurob}) is to rewrite the central matrix as 
\begin{equation}\label{eq:onewayclu_centralmat}
    \hat{\mathbf{B}} = \sum^G_{g=1} \mathbf{X}_g'\hat{\mathbf{u}}_g \hat{\mathbf{u}}_g' \mathbf{X}_g = \mathbf{X}'\begin{bmatrix}
        \hat{\mathbf{u}}_1\hat{\mathbf{u}}_1' & \mathbf{0} & \cdots & \mathbf{0}\\
        \mathbf{0} & \hat{\mathbf{u}}_2\hat{\mathbf{u}}_2' & & \vdots \\
        \vdots & & \ddots & \mathbf{0} \\
        \mathbf{0} & \cdots & & \hat{\mathbf{u}}_G\hat{\mathbf{u}}_G'
    \end{bmatrix}\mathbf{X} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right) \mathbf{X} 
\end{equation}
where $\otimes$ denotes element-wise multiplication. The $(p,q)$th element of this matrix is 
\begin{equation*}
    \sum^N_{i=1}\sum^N_{j=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1}\left(i,j\text{ in the same cluster}\right)
\end{equation*}
with $\hat{u}_i = y_i - \mathbf{x}'_i \hat{\boldsymbol{\beta}}$.

$\mathbf{S}^G$ is an $N\times N$ indicator matrix with $\mathbf{S}_{ij}^G=1$ only if the $i$th and $j$th observation belong to the same cluster: it zeros out a large amount of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ (asymptotically equivalently, ${\mathbf{u}}{\mathbf{u}}'$), specifically, only $\sum^G_{g=1}N_g^2$ out of $N^2 = \left(\sum^G_{g=1}N_g\right)^2$ terms are not zero (sub-matrices on the diagonal). Asymptotically
\begin{itemize}
    \item for fixed $N_g$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g}\xrightarrow{G\rightarrow\infty}0$
    \item for balanced clusters $N_g = N/G$, $\frac{1}{{N^2}}\sum^G_{g=1}{N^2_g} = \frac{1}{G} \xrightarrow{G\rightarrow\infty}0$
\end{itemize}


A strand of literature popularizes 
this method:
\begin{itemize}
    \item \citet{liang1986longitudinal}: in a generalized estimatin equations setting
    \item \citet{arellano1987computing}: fixed effects estimator in linear panel models
    \item \citet{hansen2007asymptotic}: asymptotic theory for panel data where $T\rightarrow\infty$ in addition to $N\rightarrow\infty$ (or $N_g\rightarrow\infty$ in addition to $G\rightarrow\infty$ in the notation above).
\end{itemize}

\section{Two-Way Clustering}
Now, consider the case of two-way clustering, 
$$
y_{i,gh} = \mathbf{x}'_{i,gh}\boldsymbol{\beta} + u
$$
where each observation may belong to \textbf{two} dimension of groups: group $g\in \left\{1,\cdots,G\right\}$ and $h\in \left\{ 1,\cdots,H \right\}$, and for $i\neq j$
\begin{equation}\label{eq:twoway_errors}
    \mathbb{E} \left[ u_{i,gh} u_{j,g'h'} \mid \mathbf{x}_{i,gh},\mathbf{j,g'h'} \right] = 0
\end{equation}
unless $g=g'$ or $h=h'$, that is, errors for individuals within the same group (along either $g$ or $h$) may be correlated.

\paragraph*{Cluster-robust variance matrix estimate} extending the one-way clustering case, keep elements of $\hat{\mathbf{u}}\hat{\mathbf{u}}'$ where the $i$th and $j$th observations share a cluster in \myhl[myblue]{\textbf{any}} dimension, then similar to Eq.(\ref{eq:onewayclu_centralmat})
\begin{equation}\label{eq:twowayclu_centermat}
    \hat{\mathbf{B}} = \mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH}\right)\mathbf{X}
\end{equation}
here $\mathbf{S}^{GH}$ is an $N\times N$ indicator matrix with $\mathbf{S}^{GH}_{ij}=1$ only if the $i$th and $j$th observation share any cluster, the $(p,q)$th element of this matrix is 
$$
\sum^N_{i=1}\sum^N_{i=1}x_{ia}x_{jb}\hat{u}_i\hat{u}_j \cdot \mathbf{1} \left(i,j\text{ share any cluster}\right)
$$
$\hat{\mathbf{B}}$ can also be presented in one-way cluster-robust fashion:
\begin{align}\label{eq:twowayclu_centmat}
    \hat{\mathbf{B}} &= \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{GH} \right)\mathbf{X} = \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^G \right)\mathbf{X} + \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^H \right)\mathbf{X} - \mathbf{X}'\left( \hat{\mathbf{u}}\hat{\mathbf{u}}' \otimes \mathbf{S}^{G\cap H} \right)\mathbf{X}
\end{align}
where $\mathbf{G}^{GH} =\mathbf{G}^G+ \mathbf{G}^H - \mathbf{G}^{G\cap H} $, with 
\begin{itemize}
    \item $\mathbf{G}^G$: $\mathbf{G}^G_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $g\in \left\{1,2,\cdots,G\right\}$
    \item $\mathbf{G}^H$: $\mathbf{G}^H_{ij}=1$ only if the $i$th and $j$th observation belong to the same cluster $h\in \left\{1,2,\cdots,H\right\}$
    \item $\mathbf{G}^{G\cap H}$: $\mathbf{G}^{G\cap H}_{ij}=1$ only if the $i$th and $j$th observation belong to \textbf{both} the same cluster $g\in \left\{1,2,\cdots,G\right\}$ and the same cluster $h\in \left\{1,2,\cdots,H\right\}$
\end{itemize}
then, similar to one-way clustering case,
\begin{align}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] =& \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^G\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &+ \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^H\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1} \\ \nonumber
    &- \left(\mathbf{X'X}\right)^{-1}\mathbf{X}'\left(\hat{\mathbf{u}}\hat{\mathbf{u}}'\otimes \mathbf{S}^{G\cap H}\right)\mathbf{X}\left(\mathbf{X'X}\right)^{-1}
\end{align}
that is,
\begin{equation}\label{eq:twowayclu_decomp}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \hat{\mathbb{V}}^G\left[\hat{\boldsymbol{\beta}}\right] + \hat{\mathbb{V}}^H\left[\hat{\boldsymbol{\beta}}\right] - \hat{\mathbb{V}}^{G\cap H}\left[\hat{\boldsymbol{\beta}}\right]
\end{equation}
each of Eq.(\ref{eq:twowayclu_decomp}) can be separately computed by OLS of $\mathbf{y}$ on $\mathbf{X}$, with variance matrix estimates $\hat{\mathbb{V}}$ based on 
\begin{itemize}
    \item[i] clustering on $g\in \left\{1,2,\cdots, G\right\}$
    \item[ii] clustering on $h \in \left\{1,2,\cdots, H\right\}$
    \item[iii] clustering on $(g,h)\in \left\{(1,1),\cdots,(G,H)\right\}$
\end{itemize}

\paragraph*{Practical considerations} It is required to know what \textit{ways} will be potentially important for clustering, which can be tested via checking the dimension of correlations in the errors. There are several ways to test 
\begin{itemize}
    \item estimate sample covariances of $\mathbf{X}'\hat{\mathbf{u}}$ within dimensions, test the null that the \myhl[myblue]{\textbf{average}} of such covariances is 0: rejecting this null is sufficient (not necessary) to reject the null of no clustering \citep{white1980heteroskedasticity}
    \item for \myhl[myblue]{\textbf{small samples}}, Eq. (\ref{eq:oneway_clurob}) is baised downwards. This is corrected (in Stata) by replacing $\hat{\mathbf{u}}_g$ with $\sqrt{c}\hat{\mathbf{u}}_g$, where $c = \frac{G}{G-1}\frac{N-1}{N-K}\simeq \frac{G}{G-1}$. For two-way clustering (Eq. \ref{eq:twowayclu_centmat}), there are 2 ways of correction:
    \begin{itemize}
        \item choose correction terms for each of the 3 components: $$c_1= \frac{G}{G-1}\frac{N-1}{N-K}, c_2= \frac{H}{H-1}\frac{N-1}{N-K},c_3=\frac{I}{I-1}\frac{N-1}{N-K}$$ with $I$ being the number of unique clusters determined by $G\cap H$
        \item choose a constant terms for all components: $$c=\frac{J}{J-1}\frac{N-1}{N-K}$$ with $J=\min(G,H)$
    \end{itemize}
    \item \myhl[myblue]{\textbf{Var-cov matrix not positive-semidefinite}}: $\hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right]$ might have negative elements on the diagonal (Eq. \ref{eq:twowayclu_decomp}), informly, this is more likely to  arise when clustering is done over the same groups as the fixed effects. One way to address this issue is using \textit{eigendecomposition} technique:
    $$
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \mathbf{U}\boldsymbol{\Lambda}\mathbf{U}'
    $$
    where 
    \begin{itemize}
        \item $\mathbf{U}$ containing the eigenvectors of $\hat{\mathbf{V}}$
        \item $\boldsymbol{\Lambda} = \mathrm{diag}\left[\lambda_1,\cdots,\lambda_d\right]$ contains the eigenvalues of $\hat{\mathbf{V}}$ 
    \end{itemize}
    then create $\boldsymbol{\Lambda}^+ = \mathrm{diag} \left[\lambda_1^+,\cdots,\lambda_d^+\right]$ with $\lambda^+_j=\max\left(0,\lambda_j\right)$ and use $\hat{\mathbf{V}}^+\left[\hat{\boldsymbol{\beta}}\right]=\mathbf{U}\boldsymbol{\Lambda}^+\mathbf{U}'$ as the estimate
\end{itemize}

\section{Multiway Clustering}
\citet{cameron2011robust} extended the framework\footnote{Also proposed by \citet{thompson2011simple}.} to allow clustering in $D$ dimensions, then we can do the following reframing
\begin{itemize}
    \item $G_d$: the number of clusters in dimension $d\in \left\{ 1,2,\cdots,D \right\}$
    \item $D-$vector $\boldsymbol{\delta}_i = \boldsymbol{\delta}(i)$, with funciton $\boldsymbol{\delta}:\left\{1,2,\cdots,N\right\}\rightarrow \bigtimes ^D_{d=1}\left\{1,2,\cdots, G_d\right\}$ lists the cluster membership in each dimension of each observation
\end{itemize}
then we have 
$$
\mathbf{1}\left[i,j\text{ shares a clustser}\right] = 1 \Leftrightarrow \delta_{id}=\delta_{jd}
$$
for some $d\in \left\{1,2, \cdots, D\right\}$, where $\delta_{id}$ denotes the $d$th element of $\boldsymbol{\delta}_i$. Also 
\begin{itemize}
    \item $D-$vector $\mathbf{r}$: define the set $$ R\equiv \left\{ \mathbf{r}:r_d\in\left\{0,1\right\},d=1,2,\cdots,D,\mathbf{r\neq 0} \right\} $$
    elements of the set $R$ can be used to index all cases where 2 observations share a cluster in at least one dimension. Define the function 
    $$
    \mathbf{I_r} (i,j) \equiv \mathbf{1} \left[r_d\delta_{id} = r_d \delta_{jd},\forall d\right]
    $$
    which indicates whether observations $i$ and $j$ have identical cluster menbership for \myhl[myblue]{\textbf{all}} dimensions $d$ s.t. $r_d=1$.
    Then we have a \textit{aggregate} identifier
    $$
    \mathbf{I}(i,j) = 1 \Leftrightarrow \mathbf{I_r}(i,j)=1\text{ for some }\mathbf{r}\in R
    $$
    i.e., 2 observations share \myhl[myblue]{\textbf{at least}} one dimension.
\end{itemize}
The define the $2^D-1$ matrices
\begin{equation}\label{eq:centermat_multiclu}
    \tilde{\mathbf{B}}_{\mathbf{r}}\equiv \sum^N_{i=1}\sum^N_{j=1}\mathbf{x}_i\mathbf{x}_j' \hat{u}_i\hat{u}_j\mathbf{I_r}(i,j)
\end{equation}
with $\mathbf{r}\in R$.

\paragraph*{Var-cov matrix estimator} consider, similarly, an estimator 
\begin{equation}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\beta}}\right] = \left(\mathbf{X'X}\right)^{-1}\tilde{\mathbf{B}} \left(\mathbf{X'X}\right)^{-1} \equiv  \left(\mathbf{X'X}\right)^{-1}  \left(\sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r \right)  \left(\mathbf{X'X}\right)^{-1}
\end{equation}
where cases of clustering on an odd number of dimensions are added, those of clustering on an even number of dimensions are subtracted. Consider the case of $D=3$,
\begin{equation*}
    \left(\tilde{\mathbf{B}}_{(1,0,0)} + \tilde{\mathbf{B}}_{(0,1,0)} + \tilde{\mathbf{B}}_{(0,0,1)} \right) - \left( \tilde{\mathbf{B}}_{(1,1,0)} + \tilde{\mathbf{B}}_{(1,0,1)} + \tilde{\mathbf{B}}_{(0,1,1)} \right) + \tilde{\mathbf{B}}_{(1,1,1)}
\end{equation*}
$\tilde{\mathbf{B}}$ is identical to $\hat{\mathbf{B}}$ defined analogically as in Eq.(\ref{eq:twowayclu_centmat}), since 
\begin{itemize}
    \item no observation pair with $\mathbf{I}(i,j)=0$: this is immediate, since $\mathbf{I}(i,j)=0 \Leftrightarrow \mathbf{I_r}(i,j)=0,\forall \mathbf{r}$
    \item the covariance term corresponding to each observation pair with $\mathbf{I}(i,j)=1$ is included \myhl[myblue]{\textbf{exactly once}} in $\tilde{\mathbf{B}} $: by inclusion-exclusion principle for set cardinality
    $$\mathbf{I}(i,j) \Rightarrow \sum_{\left\Vert \mathbf{r} \right\Vert=k,\mathbf{r}\in R}(-1)^{k+1}\mathbf{I_r}(i,j)=1 $$
\end{itemize} 

\paragraph*{Curse of dimensionality} this could arise in a setting with \textbf{many dimensions} of clustering, and in which one or more dimensions have \textbf{few} clusters\footnote{The square design (each dimension has the same number of clusters) with orthogonal dimensions has the \textbf{least} independence of observations.}.
\citet{cameron2011robust} suggested an ad-hoc rule of thumb for approximating sufficient numbers of clusters.

\subsection{Non-linear Estimators}
\paragraph*{$m$-Estimators}
Consider an $m$-estimator that solves
$$
\sum^N_{i=1}\mathbf{h}_i\left(\hat{\boldsymbol{\theta}}\right) = \mathbf{0}
$$
under standard assumptions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal with estimated variance matrix 
\begin{equation}\label{eq:m-est_varmat}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \hat{\mathbf{A}}^{-1}\hat{\mathbf{B}}{\hat{\mathbf{A}^{\prime}}^{-1}}
\end{equation}
where $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}}$ and $\hat{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i\mathbf{h}_i\right]$.

\begin{itemize}
    \item \myhl[myblue]{\textbf{one-way clustering}} $\hat{\mathbf{B}} = \sum^G_{g=1}\hat{\mathbf{h}}_g\hat{\mathbf{h}}_g'$ where $\hat{\mathbf{h}}_g = \sum^{N_g}_{i=1}\hat{\mathbf{h}}_{ig}$, clustering may not lead to parameter inconsistency, depending on whether $\mathbb{E}\left[\mathbf{h}_i(\boldsymbol{\theta})\right]= \mathbf{0}$ with clustering
    \begin{itemize}
        \item \textbf{population-averaged approach}: assum $\mathbf{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi \left( \mathbf{x}_{ig}'\boldsymbol{\beta} \right)$
        \item \textbf{random effects approach}: let $y_{ig}=1$ if $y^*_{ig} > 0$ where $y^*_{ig}=\mathbf{x}_{ig}'\boldsymbol{\beta}+\epsilon_g + \epsilon_{ig}$, where 
        \begin{itemize}
            \item idiosyncratic error $\epsilon_{ig}\sim \mathcal{N}(0,1)$
            \item cluster-specific error $\epsilon_g \sim \mathcal{N}(0,\sigma^2_g)$
        \end{itemize}
        then we have the alternative moment condition $$ \mathbb{E}\left[y_{ig}\mid \mathbf{x}_{ig}\right] = \Phi\left(\frac{\mathbf{x}_{ig}'\boldsymbol{\beta}}{\sqrt{1+\sigma^2_g}}\right) $$ 
    \end{itemize}
    \item \myhl[myblue]{\textbf{multiway clustering}} replacing $\hat{u}_i\mathbf{x}_i$ in Eq.(\ref{eq:centermat_multiclu}) with $\hat{\mathbf{h}}_i$, then we have 
    \begin{align*}
        \hat{\mathbb{V}} \left[\hat{\boldsymbol{\theta}}\right] &= \hat{\mathbf{A}}^{-1} \tilde{\mathbf{B}} \hat{\mathbf{A}'}^{-1}
    \end{align*}
    where 
    \begin{align*}
        \hat{\mathbf{A}} &= \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} & \tilde{\mathbf{B}} &= \sum_{\left\Vert \mathbf{r} \right\Vert =k,\mathbf{r}\in R} (-1)^{k+1}\tilde{\mathbf{B}}_r & \tilde{\mathbf{B}}_r &\equiv \sum^N_{i=1}\sum^N_{j=1}\hat{\mathbf{h}}_i\hat{\mathbf{h}'}_j\mathbb{I}_{\mathbf{r}}(i,j)
    \end{align*}
    with $\mathbf{r}\in R$\footnote{This multiway clustering can be implemented using several one-way clustered bootstraps. Each of the one-way cluster robust matrices is estimated by a pairs cluster bootstrap that resamples with replacement from the appropriate cluster dimension. They are then combined as if they had been estimated analytically \citep{cameron2011robust}.}.
\end{itemize} 

\paragraph*{GMM estimation} Consider an example of over-identified models: linear two stage least squares with more instruments than endogenous regressors, we have 
$$
\hat{\boldsymbol{\theta}} = \arg\min_{\boldsymbol{\theta}} Q(\boldsymbol{\theta}) =  \arg\min_{\boldsymbol{\theta}} \left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)'\mathbf{W}\left(\sum^N_{i=1}\mathbf{h}_i(\boldsymbol{\theta})\right)
$$
where $\mathbf{W}$ is a symmetric positive definite weighting matrix. Under standard regularity conditions, $\hat{\boldsymbol{\theta}}$ is asymptotically normal, with estimated variance matrix 
\begin{equation*}
    \hat{\mathbb{V}}\left[\hat{\boldsymbol{\theta}}\right] = \left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}\hat{\mathbf{A}}'\mathbf{W}\tilde{\mathbf{B}}\mathbf{W}\hat{\mathbf{A}}\left(\hat{\mathbf{A}}'\mathbf{W}\hat{\mathbf{A}}\right)^{-1}
\end{equation*}
again, $\hat{\mathbf{A}} = \sum_i \left. \frac{\partial \mathbf{h}_i}{\partial \boldsymbol{\theta}'}\right\vert _{\hat{\boldsymbol{\theta}}} $, and $\tilde{\mathbf{B}}$ is an estimate of $\mathbb{V}\left[\sum_i \mathbf{h}_i\right]$.

\section{Menzel (2021): Asymptotic Gaussianity}
One key of TWCR inference is the asymptotic Gaussianity, \citet{menzel2021bootstrap} pointed out the potential non-Gaussianity of the limit distribution.
Still, consider a random array ($Y_{it}$) indexed by two dimensions by $i=1,\cdots,N$ and $t=1,\cdots,T$. Clusters are sampled independently at random from an infinite population,
but otherwise \textbf{unrestricted} in dependence within each row $\mathbf{Y}_{i\cdot} \coloneq \left(Y_{i1}\cdots,Y_{iT}\right)$ and within each column $\mathbf{Y}_{\cdot t}\coloneq \left(Y_{1t},\cdots,Y_{Nt}\right)$.

\subsection{Distribution of Sample Average}
First, consider 
$$
\bar{Y}_{NT} \coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y_{it}
$$
and approximate the asymptotic distribution regardless of whether, or what type of, cluster-dependence is present.

\paragraph*{3 scenarios} of the array $(Y_{it})$
\begin{itemize}
    \item \myhl[myblue]{\textbf{no cluster-dependence}}: $(Y_{it})$ are mutually independent, CLT at a rate of $(NT)^{-1/2}$ applies (under regularity conditions)
    \item \myhl[myblue]{\textbf{correlation within clusters}}: the convergence rate of $(Y_{it})$ is determined by the number of relevant clusters 
    \item \myhl[myblue]{\textbf{non-separable models of heterogeneity (dependence with clusters, even uncorrelated)}}\footnote{This is specific to clustering in 2 or more dimensions.}: The asymptotic behavior is non-standard
\end{itemize}
Consider 2 examples:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Additive factor model}}
    $$ Y_{it} = \mu + \alpha_i + \gamma_t + \epsilon_{it} $$
    where $\mu$ is a constant, and $\alpha_i,\gamma_i,\epsilon_{it}$ are zero-mean i.i.d. random variables for $i=1,\cdots,N$ and $t=1,\cdots,T$ with bounded second moments, and $N=T$. Based on a standard central limit theory, we have 
    \begin{itemize}
        \item in the \underline{non-degenerate} case with $\mathrm{Var}(\alpha_i) >0$ or $\mathrm{\gamma_t}>0$, the sample distribution $$ \sqrt{N}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\alpha_i) + \mathrm{Var}(\gamma_t)\right) $$
        \item in the \underline{degenerate} case of \underline{no clustering} with $\mathrm{Var}(\alpha_i) = \mathrm{Var}(\gamma_t) =0$, the sample distribution $$ \sqrt{NT}\left(\bar{Y}_{NT}-\mathbb{E}\left[Y_{it}\right]\right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\epsilon_{it})\right) $$ 
    \end{itemize}
    if marginal distributions of $\alpha_i,\gamma_t,\epsilon_{it}$ are known, we can simulate from the joint distribution of $\left(Y_{it}\right)$ by sampling the individual components at random, a bootstrap procedure would be consistent. If \textbf{unknown}, consider estimators 
    \begin{align*}
        \hat{\alpha}_i & \coloneq \frac{1}{T}\sum^T_{t=1}\left(Y_{it}-\bar{Y}_{NT}\right) = \alpha_i + \frac{1}{T}\sum^T_{t=1} \left(\epsilon_{it}-\bar{\epsilon}_{NT}\right)\\
        \hat{\gamma}_t & \coloneq \frac{1}{N}\sum^N_{i=1}\left(Y_{it}-\bar{Y}_{NT}\right) = \gamma_t + \frac{1}{N}\sum^N_{i=1} \left(\epsilon_{it}-\bar{\epsilon}_{NT}\right)\\
        \hat{\epsilon}_{it} & \coloneq Y_{it} - \bar{Y}_{NT} -\hat{\alpha}_i -\hat{\gamma}_t
    \end{align*}
    then use these empirical distributions for estimation and form a bootstrap sample 
    $$ Y^*_{it} \coloneq \bar{Y}_{NT} + \alpha^*_i + \gamma^*_t + \epsilon^*_{it} $$
    by drawing from these estimators and obtain $\bar{Y}^*_{NT}\coloneq \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}Y^*_{it}$, and verify the conditional variances of the bootstrap distribution given the sample:
    \begin{align*}
        \frac{1}{N}\sum^N_{i=1}\left(\hat{\alpha}_i - \frac{1}{N}\sum^N_{j=1}\hat{\alpha}_j\right)^2 - \left[\mathrm{Var}(\alpha_i) + \frac{\mathrm{Var}(\epsilon_{it})}{T}\right] & \xrightarrow{p} 0\\
        \frac{1}{T}\sum^N_{i=1}\left(\hat{\gamma}_t - \frac{1}{T}\sum^T_{s=1}\hat{\gamma}_t\right)^2 - \left[\mathrm{Var}(\gamma_t) + \frac{\mathrm{Var}(\epsilon_{it})}{N}\right] & \xrightarrow{p} 0
    \end{align*}
    then the bootstrap distribution is
    \begin{itemize}
        \item in the \underline{non-degenerate} case, $$ \sqrt{N}\left( \bar{Y}^*_{NT}-\bar{Y}_{NT} \right) \xrightarrow{d} \mathcal{N}\left(0,\mathrm{Var}(\alpha_i) + \mathrm{Var}(\gamma_t)\right) $$
        the estimation error $\hat{\alpha}_i$ does \textbf{NOT} affect the asymptotic variance.
        \item in the \underline{degenerate} case, $$ \sqrt{NT}\left(\bar{Y}^*_{NT}-\bar{Y}_{NT}\right) \xrightarrow{d} \mathcal{N}\left(0,3\mathrm{Var}(\epsilon_{it})\right) $$
        asymptotically overestimates the variance of the sampling distribution, leading to inconsistency of this naive bootstrapping procedure.
    \end{itemize}
    \item \myhl[myblue]{\textbf{Non-Gaussian limit distribution}} $$ Y_{it} = \alpha_i \gamma_t + \epsilon_{it} $$
    where $\alpha_i,\gamma_t,\epsilon_{it}$ are independently distributed with $\mathbb{E}\left[\epsilon_{it}\right] = 0,\mathrm{Var}(\alpha_i) = \sigma^2_{\alpha},\mathrm{Var}(\gamma_t) = \sigma^2_{\gamma}, \mathrm{Var}(\epsilon)_{it}=\sigma^2_{\epsilon}$. \textbf{If} $\mathbb{E}\left[\alpha_i\right] = \mathbb{E}\left[\gamma_t\right] = 0$, then CLT and Continuous Mapping Theorem (CMT) imply 
    \begin{align*}
        \sqrt{NT} \cdot \bar{Y}_{NT} =& \frac{1}{\sqrt{NT}} \sum^N_{i=1}\sum^T_{t=1} \left(\alpha_i \gamma_t +\epsilon_{it}\right)\\
        =& \left( \frac{1}{\sqrt{N}} \sum^N_{i=1}\alpha_i \right) \left(\frac{1}{\sqrt{T}} \sum^T_{t=1}\gamma_t \right) + \frac{1}{\sqrt{NT}}\sum^N_{i=1}\sum^T_{t=1}\epsilon_{it}\\
        \xrightarrow{d}& \sigma_{\alpha}\sigma_{\gamma}Z_1Z_2 + \sigma_{\epsilon}Z_3
    \end{align*}
\end{itemize}
then even without correlation within clusters, non-separable heterogeneity can still generate dependence in $2^{\text{nd}}$ or higher moments in the limiting distribution\footnote{2 major issues arise:\begin{itemize}
    \item The limiting distribution needs \textbf{not} be Gaussian: plug-in asymptotic inference based on the normal distribution is invalid 
    \item It only comes from two-or-more-dimension cluster dependence, not single-dimension cluster dependence.
\end{itemize}
}.

\subsection{Menzel (2021)'s Bootstrap procedure}
\subsubsection{Notation}
For the array $\left(Y_{it}\right)_{i,t}$, denote 
\begin{itemize}
    \item $\mathbb{P}$: joint distribution of $\left(Y_{it}\right)_{i,t}$
    \item $\mathbb{P}_{NT}$: drifting DGP indexed by $N,T$
    \item $\mathbb{P}^*_{NT}$: bootstrap distribution for $\left(Y^*_{it}\right)$ given the realizations $\left(  Y_{it}:i=1,\cdots,N; t= 1,\cdots,T  \right)$
    \item respective distributions $\mathbb{E},\mathbb{E}_{NT},\mathbb{E}^*_{NT}$
\end{itemize}

\subsubsection{Inference: Sample Mean}
First, consider the assumption of \textit{separate exchangeability}
\begin{assumption}{Separate Exchangeability}{sep_exchange}
    \begin{itemize}
        \item A \textbf{separately exchangeable} array is an infinite array $\left(Y_{it}\right)_{i,t}$ such that for any integers $\tilde{N},\tilde{T}$ and permutations $\pi_1:\left\{1,\cdots,\tilde{N}\right\}\rightarrow \left\{1,\cdots,\tilde{N}\right\}$ and $\pi_2:\left\{1,\cdots,\tilde{T}\right\}\rightarrow \left\{1,\cdots,\tilde{T}\right\}$, we have 
        $$ \left(Y_{\pi_1(i),\pi_2(t)}\right)_{i,t} \overset{d}{=} \left(Y_{it}\right)_{i,t} $$
        such an array is called \textbf{dissociated} if for any $N_0,T_0\geq 1$, $\left(Y_{it}\right)^{i=N_0,t=T_0}_{i=1,t=1}$ is independent of $\left(Y_{it}\right)_{i>N_0,t>T_0}$.
        \item For dyadic data, consider the alternative assumption \textbf{jointly exchangeable} arrays $\left(Y_{ij}\right)_{i,j}$ satisfying 
        $$ \left(Y_{\pi(i),\pi(j)}\right)_{i,j} \overset{d}{=} \left(Y_{ij}\right)_{i,j} $$
        for any permutation $\pi$ on $\left\{1,\cdots,\tilde{N}\right\}$, in addition, $\left(Y_{ij}\right)^{N_0}_{i,j=1}$ is independent of $\left(Y_{ij}\right)_{i,j>N_0}$
    \end{itemize}
\end{assumption}
This assumption can be interpreted as rows (and columns) corresponding to units that are drawn independently from a common population, where we then observe the joint outcome for every row-column pair, consider the requirements in the following applications
\begin{itemize}
    \item \myhl[myblue]{\textbf{DiD/matched data}}: the units corresponding to either dimension of the sample to represent independent draws from a common, infinite population 
    \item \myhl[myblue]{\textbf{non-exhaustively matched data}}: only observe joint outcomes for a posibly self-selected subset of unit pairs, sample selection should be (jointly or separately) exchangeable
    \item \myhl[myblue]{\textbf{U-/V-statistics}}: the kernel $Y_{i_1,\cdots,i_D} \coloneq h\left(X_{i_1},\cdots,X_{i_D}\right) $ evaluated at i.i.d. observations $X_1,\cdots,X_N$ forms a dissociated, jointly exchangeable array
    \item \myhl[myblue]{\textbf{Network}}: unlabeled\footnote{\textit{Unlabeled}: nodel identifiers do not carry any significance for the statistical model.} data implies finite exchangeability, the sampled graph has joint (\textit{infinite}) exchangeability if it is a subgraph of an infinite graph
\end{itemize}
Directly from Assumption \ref{assump:sep_exchange}, any dissociated separately exchangeable array can be represented as 
\begin{equation*}
    Y_{it} = f\left(\alpha_i,\gamma_t,\epsilon_{it}\right)
\end{equation*}
for some function $f(\cdot)$ where $\alpha_1,\cdots,\alpha_N$, $\gamma_1,\cdots,\gamma_T$, $\epsilon_{11},\cdots,\epsilon_{NT}$ are mutually independent, uniformly distributed random variables.

\paragraph*{Projection} now, decompose the array $\left(Y_{it}\right)_{i,t}$ as 
\begin{align*}
    Y_{it} &= b + a_i + g_t + w_{it} & \mathbb{E}\left[w_{it}\mid a_i,g_t\right]&=0
\end{align*}
where $a_i$ and $g_t$ are mean-zero and mutually independent, s.t. the joint distribution of $Y_{it}$ can then be expended as 
\begin{align*}
    Y_{it} =& \mathbb{E}\left[Y_{it}\right] + \left(\mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\right] \right) + \left( \mathbb{E}\left[Y_{it}\mid \gamma_t\right] - \mathbb{E}\left[Y_{it}\right] \right) \\
    &+ \left( \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]\right) + \left( Y_{it}-\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] \right) \\
    \eqcolon & b+ a_i + g_t + v_{it} + e_{it}
\end{align*}
with
\begin{itemize}
    \item $e_{it} = Y_{it} - \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right]$
    \item $a_i = \mathbb{E}\left[Y_{it}\mid\alpha_i \right]-\mathbb{E}\left[Y_{it}\right]$, $g_t= \mathbb{E}\left[Y_{it}\mid \gamma_t\right]-\mathbb{E}\left[Y_{it}\right]$
    \item $v_{it} = \mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t\right] + \mathbb{E}\left[Y_{it}\right]$
    \item $b= \mathbb{E}\left[Y_{it}\right]$
\end{itemize}
here, 
\begin{itemize}
    \item temporal and cross-sectional units were drawn independently: $a_1,\cdots,a_N$ and $g_1,\cdots,g_T$ are independent of each other.
    \item by construction, $\mathbb{E}\left[e_{it}\mid a_i,g_t,v_{it}\right]=0$, $\mathbb{E}\left[v_{it}\mid a_i\right] = \mathbb{E}\left[v_{it}\mid g_t\right]=0$
    \item $e_{it}$, $(a_i,g_t)$ and $v_{it}$ are \textbf{uncorrelated}
\end{itemize}
then, rewrite the sample mean as 
\begin{align*}
    \hat{Y}_{NT} &= b+\bar{a}_N + \bar{g}_T + \bar{v}_{NT} + \bar{e}_{NT}\\
    &\coloneq b + \frac{1}{N}\sum^N_{i=1}a_i + \frac{1}{T}\sum^T_{t=1}g_t + \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}v_{it} + \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}e_{it}
\end{align*}
and the unconditional variances of the projections with 
\begin{align*}
    \sigma^2_a&\coloneq \mathrm{Var}(a_i) & \sigma^2_g&\coloneq \mathrm{Var}(g_t) & \sigma^2_v&\coloneq \mathrm{Var}(v_{it}) & \sigma^2_e&\coloneq \mathrm{Var}(e_{it})   
\end{align*}
let $w_{it}\coloneq v_{it}+ e_{it}$, and denote its variance by $\sigma^2_w = \mathrm{Var}(w_{it})$. Then, assume integrability 
\begin{assumption}{Integrability}{integrability}
    Let $Y_{it} = f(\alpha_i,\gamma_t,\epsilon_{it})$, where $\alpha_i,\gamma_t,\epsilon_{it}$ are random arrays with elements i.i.d. drawn from $[0,1]$ uniform distribution, assume 
    \begin{itemize}
        \item $a_i/\sigma_a$, $g_t/\sigma_g$, $v_{it}/\sigma_v$, $e_{it}/\sigma_e$ are well-defined and have bounded moments up to the order $4+\delta$ for some $\delta>0$, whenever the respective variances $\sigma^2_a,\sigma^2_g,\sigma^2_v,\sigma^2_e$ are non-zero.
        \item $\sigma^2_a + \sigma^2_g >0$, or $\sigma^2_v + \sigma^2_e > 0$
    \end{itemize}
\end{assumption}

\paragraph*{Low-rank approximation}
Consider the row/column projection
\begin{equation*}
    \bar{v}_{NT} \equiv \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}\left(\mathbb{E}\left[Y_{it}\mid \alpha_i,\gamma_t\right] -\mathbb{E}\left[Y_{it}\mid \alpha_i\right] -\mathbb{E}\left[Y_{it}\mid \gamma_t\right] +\mathbb{E}\left[Y_{it}\right] \right) \eqcolon \frac{1}{NT}\sum^T_{t=1}\sum^N_{i=1}v(\alpha_i,\gamma_t)
\end{equation*}
as a generalized U-statistic with a kerel $v(\alpha,\gamma)$ evaluated at the samples $\alpha_1,\cdots,\alpha_N$ and $\gamma_1,\cdots,\gamma_t$. There are 2 major issues w.r.t. characterizing the distribution of $\bar{Y}_{NT}$
\begin{itemize}
    \item the presence of the projection error $e_{it}$
    \item the factors $\alpha_i,\gamma_t$ are not observable
\end{itemize}
Define,
$$
v(\alpha,\gamma) \coloneq \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha,\gamma_t=\gamma\right] - \mathbb{E}\left[Y_{it}\mid \alpha_i=\alpha\right] - \mathbb{E}\left[Y_{it}\mid \gamma_t=\gamma\right] + \mathbb{E}\left[Y_{it}\right]
$$
under Assumption \ref{assump:integrability}, we have compact integral operators
\begin{align*}
    S(u)(g)&=\int v(a,g)u(a)F_{\alpha}(\mathrm{d}a) & S^*(u)(a)&=\int v(a,g)u(g)F_{\gamma}(\mathrm{d}g)
\end{align*}
where $F_{\alpha},F_{\gamma}$ are the marginal distributions corresponding to the joint $F_{\alpha\gamma}$ of $\alpha_i,\gamma_t$. Then the low-rank approximation is
\begin{equation}\label{eq:low-rank_approximation}
    v(\alpha,\gamma) = \sum^{\infty}_{k=1}c_k\phi_k(\alpha)\psi_k(\gamma)
\end{equation}
under the $L_2(F_{\alpha\gamma})$ norm on the space of smooth functions of $(\alpha,\gamma)\in[0,1]^2$. Here 
\begin{itemize}
    \item $(c_k)_{k\geq 1}$: a sequence of singular values, $\lim\left\vert c_k \right\vert \rightarrow 0$
    \item $\left(\phi_k\left(\cdot\right)\right)_{k\geq 1}$ and $\left(\psi_k\left(\cdot\right)\right)_{k\geq 1}$: orthonormal bases for $L_2\left([0,1],F_{\alpha}\right)$ and $L_2\left([0,1],F_{\gamma}\right)$:
    \begin{itemize}
        \item By construction: $$ \mathbb{E}\left[v(a,\gamma_t)\right] = \mathbb{E}\left[v(\alpha_i,g)\right] =0,\forall a,g\in[0,1] \Rightarrow \mathbb{E}\left[\phi_k(\alpha_i)\right] = \mathbb{E}\left[\psi_k(\gamma_t)\right] = 0,\forall k =1,2,\cdots $$
        \item the basis functions are orthonormal and $\alpha_i$ and $\gamma_t$ are independent, then $\forall K<\infty$ $$ \mathrm{Cov}\left[ \left(\phi_1(\alpha_i),\psi_1(\gamma_t), \cdots, \phi_K(\alpha_i),\psi_K(\gamma_t) \right) \right] $$ is the $2K$-dimensional identity matrix
        \item $\left(\phi_1(\alpha_i),\cdots, \phi_K(\alpha_i)\right)$ can be correlated with $a_i$: $\sigma_{ak}\coloneq \mathrm{Cov}\left(a_i,\phi_k(\alpha_i)\right)$
        \item $\left(\psi_1(\gamma_t),\cdots, \psi_K(\gamma_t)\right)$ can be correlated with $g_t$: $\sigma_{gk}\coloneq \mathrm{Cov}\left(g_t,\psi_k(\gamma_t)\right)$
    \end{itemize}
\end{itemize}
with this representation of Eq.(\ref{eq:low-rank_approximation}), we have\footnote{The limiting distribution of this term is not Gaussian, but can be represented as a linear combination of independent chi-squared random variables. This type of distributions is known as Wiener/Gaussian chaos.} 
\begin{equation*}
    \frac{1}{NT}\sum^N_{i=1}\sum^T_{t=1}v(\alpha_i,\gamma_t) = \sum^{\infty}_{k=1}c_k \left(\frac{1}{N}\sum^N_{i=1}\phi_k(\alpha_i)\right) \left(\frac{1}{T}\sum^T_{t=1}\psi_k(\gamma_t)\right)
\end{equation*}
and the second-order projection term can also be represented as a function of \textbf{countably many} sample averages of \textbf{i.i.d. mean-zero} random variables.


\section*{A Theoretical}
\citet{chiang2023using}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}