\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{20}{Random Forest}{}{Sai Zhang}{.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Denote by $m(\mathbf{X})$ the measurable nonparametric regression function with p-dimensional random vector $\mathbf{X}$ taking values in $[0,1]^p$. The Random Forest algorithm aims to learn the 
regression function in a non-parametric way based on the observations $\mathbf{x}_i \in [0,1]^p,y_i\in\mathbb{R},i=1,\cdots,n$, from the model 
$$
y_i = m(\mathbf{x}_i) + \epsilon_i
$$
where $\mathbf{X},\mathbf{x}_i,\epsilon_i,i=1,\cdots,n$ are independent, and $\left\{\mathbf{x}_i\right\}$ and $\left\{\epsilon_i\right\}$ are two sequences of identically distributed random variables. $\mathbf{x}_i$ is distributed identically as $\mathbf{X}$.

\paragraph*{Why Random Forest (RF)?} \textbf{RF} has gained significant popularity due to its
\begin{itemize}
    \item \myhl[myblue]{\textbf{High accuracy}}: \textbf{RF} consistently rank among the top performer, often surpassing more complex models
    \item \myhl[myblue]{\textbf{Robustness}}: \textbf{RF} are less subject to overfitting due to the ensemble nature leveraging multiple decision trees
    \item \myhl[myblue]{\textbf{Interpretability}}: \textbf{RF} provide rankings of feature importance
\end{itemize}

As illustrated in Figure \ref{fig:level2tree}, in a level 2 

\begin{figure}[ht]
    \begin{center}
        \begin{tikzpicture}[scale=1.2]
            % basics
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (a) at (0,2.5) {Level 0 \\ $\mathbf{t}_0:[0,1]^p$ \\ split: $\left(j_i \in \Theta_{1,1},c_1\right)$};
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (b1) at (-2.5,1) {Cell $\mathbf{t}_{1,1}$ \\ split: $\left(j_2 \in \Theta_{2,1},c_2\right)$};
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (b2) at (2.5,1) {Cell $\mathbf{t}_{1,2}$ \\ split: $\left(j_3 \in \Theta_{2,2},c_3\right)$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c1) at (-4,0) {Cell $\mathbf{t}_{2,1}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c2) at (-1.25,0) {Cell $\mathbf{t}_{2,2}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c3) at (1.25,0) {Cell $\mathbf{t}_{2,3}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c4) at (4,0) {Cell $\mathbf{t}_{2,4}$};
            \draw[-] (a) -- (b1);
            \draw[-] (a) -- (b2);
            \draw[-] (b1) -- (c1);
            \draw[-] (b1) -- (c2);
            \draw[-] (b2) -- (c3);
            \draw[-] (b2) -- (c4);
        \end{tikzpicture}
    \end{center}
    \caption{Level-2 Tree Example}\label{fig:level2tree}
\end{figure}


\citet{chi2022asymptotic}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}