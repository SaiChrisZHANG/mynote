\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{20}{Random Forest}{}{Sai Zhang}{.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Denote by $m(\mathbf{X})$ the measurable nonparametric regression function with p-dimensional random vector $\mathbf{X}$ taking values in $[0,1]^p$. The Random Forest algorithm aims to learn the 
regression function in a non-parametric way based on the observations $\mathbf{x}_i \in [0,1]^p,y_i\in\mathbb{R},i=1,\cdots,n$, from the model 
$$
y_i = m(\mathbf{x}_i) + \epsilon_i
$$
where $\mathbf{X},\mathbf{x}_i,\epsilon_i,i=1,\cdots,n$ are independent, and $\left\{\mathbf{x}_i\right\}$ and $\left\{\epsilon_i\right\}$ are two sequences of identically distributed random variables. $\mathbf{x}_i$ is distributed identically as $\mathbf{X}$.

\paragraph*{Why Random Forest (RF)?} \textbf{RF} has gained significant popularity due to its
\begin{itemize}
    \item \myhl[myblue]{\textbf{High accuracy}}: \textbf{RF} consistently rank among the top performer, often surpassing more complex models
    \item \myhl[myblue]{\textbf{Robustness}}: \textbf{RF} are less subject to overfitting due to the ensemble nature leveraging multiple decision trees
    \item \myhl[myblue]{\textbf{Interpretability}}: \textbf{RF} provide rankings of feature importance
\end{itemize}

As illustrated in Figure \ref{fig:level2tree}, in a level-2 tree, each node (cell) defines the point where the current cell split and new cells are produced. The sets of features eligible for splitting cells at level $k-1$ are denoted as $\Theta_k\coloneq \left\{\Theta_{k,1},\cdots,\Theta_{k,2^{k-1}} \right\}$, where $\Theta_{k,s}\subset \left\{ 1,\cdots,p \right\}$.

\begin{figure}[ht]
    \begin{center}
        \begin{tikzpicture}[scale=1.2]
            % basics
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (a) at (0,2.5) {Level 0 \\ $\mathbf{t}_0:[0,1]^p$ \\ split: $\left(j_i \in \Theta_{1,1},c_1\right)$};
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (b1) at (-2.5,1) {Cell $\mathbf{t}_{1,1}$ \\ split: $\left(j_2 \in \Theta_{2,1},c_2\right)$};
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (b2) at (2.5,1) {Cell $\mathbf{t}_{1,2}$ \\ split: $\left(j_3 \in \Theta_{2,2},c_3\right)$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c1) at (-4,0) {Cell $\mathbf{t}_{2,1}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c2) at (-1.25,0) {Cell $\mathbf{t}_{2,2}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c3) at (1.25,0) {Cell $\mathbf{t}_{2,3}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c4) at (4,0) {Cell $\mathbf{t}_{2,4}$};
            \draw[-] (a) -- (b1);
            \draw[-] (a) -- (b2);
            \draw[-] (b1) -- (c1);
            \draw[-] (b1) -- (c2);
            \draw[-] (b2) -- (c3);
            \draw[-] (b2) -- (c4);
        \end{tikzpicture}
    \end{center}
    \caption{Level-2 Tree Example}\label{fig:level2tree}
\end{figure}

Given any $T$ (and the associated splitting criterion) and $\Theta_{1:k}$, the tree estimate denoted as $\hat{m}_{T(\Theta_{1:k})}$ for a test point $\mathbf{c}\in [0,1]^p$ is defined as 
$$
\hat{m}_{T(\Theta_{1:k})}\left(\mathbf{c},\mathcal{X}_n\right) \coloneq \sum_{\left(\mathbf{t}_1,\cdots,\mathbf{t}_k\right)\in T(\Theta_{1:k})} \mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}\left( \frac{\sum_{i\in \left\{i:\mathbf{x}_i\in \mathbf{t}_k\right\}} y_i}{\# \left\{i:\mathbf{x}_i\in\mathbf{t}_k\right\}} \right)
$$
where $\mathcal{X}_n\coloneq \left\{ \mathbf{x}_i,y_i \right\}^n_{i=1}$, the fraction is defined as $0$ when no sample is in the cell $\mathbf{t}_k$, and $\mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}$ is an indicator function $=1$ if $\mathbf{c}\in\mathbf{t}_k$ and $=0$ otherwise.

\section{Chi et al. (2022): High Dimensional RFs}
For a RF model where
\begin{itemize}
    \item a sequence of distinct $\Theta_{1:k}$ results in a distinct tree
    \item every set of available features $\Theta_{l,s}$, $l=1,\cdots,k$; $s=1,\cdots,2^{l-1}$
\end{itemize}

\paragraph*{Column subsampling} Define a \myhl[myblue]{\textbf{column subsampling}} procedure: $\Theta_{l,s},\forall l,s$ has $[\gamma_0p]$ distinct integers among $1,\cdots,p$, with $[\cdot]$ the ceiling function for some $0<\gamma_0\leq 1$. $\gamma_0$ is the predetermined constant parameter of column subsampling.
Introduce the boldface random mappings $\boldsymbol{\Theta}_{1:k}$, which are independent and uniformly distributed over all possible $\Theta_{1:k}$ for all integer $k$. Then random forests estimate for $\mathbf{c}$ with observations $\mathcal{X}_n$ is given by 
\begin{equation*}
    \mathbb{E}\left( \hat{m}_{T(\boldsymbol{\Theta}_{1:k})}\left(\mathbf{c},\mathcal{X}_n\right) \mid \mathcal{X}_n \right) = \sum_{\Theta_{1:k}} \mathbb{P} \left( \bigcap^k_{s=1} \left\{ \boldsymbol{\Theta}_s = \Theta_s \right\} \right) \hat{m}_{T(\Theta_{1:k})} \left(\mathbf{c},\mathcal{X}_n\right)
\end{equation*}
The expectation is taken over sets of available features.

\paragraph*{Observation resampling}
Let $A=\left\{a_1,\cdots, a_B\right\}$ be a set of subsamples with each $a_i$ consisting of $\lceil bn\rceil$ observations (indices) drawn without replacement from $\left\{1,\cdots,n\right\}$ for some positive integer $B$ and $0<b\leq 1$; in addition, each $a_i$ is independent of model training. The default values of $B$ and $b$ are $500$ and $0.632$\footnote{Or, $b=1$ but observations are drawn with replacement.}.

\citet{chi2022asymptotic}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}