\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{20}{Random Forest}{}{Sai Zhang}{.}{The note is built on Prof. \hyperlink{http://faculty.marshall.usc.edu/jinchi-lv/}{Jinchi Lv}'s lectures of the course at USC, DSO 607, High-Dimensional Statistics and Big Data Problems.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Motivation}
Denote by $m(\mathbf{X})$ the measurable nonparametric regression function with p-dimensional random vector $\mathbf{X}$ taking values in $[0,1]^p$. The Random Forest algorithm aims to learn the 
regression function in a non-parametric way based on the observations $\mathbf{x}_i \in [0,1]^p,y_i\in\mathbb{R},i=1,\cdots,n$, from the model 
$$
y_i = m(\mathbf{x}_i) + \epsilon_i
$$
where $\mathbf{X},\mathbf{x}_i,\epsilon_i,i=1,\cdots,n$ are independent, and $\left\{\mathbf{x}_i\right\}$ and $\left\{\epsilon_i\right\}$ are two sequences of identically distributed random variables. $\mathbf{x}_i$ is distributed identically as $\mathbf{X}$.

\paragraph*{Why Random Forest (RF)?} \textbf{RF} has gained significant popularity due to its
\begin{itemize}
    \item \myhl[myblue]{\textbf{High accuracy}}: \textbf{RF} consistently rank among the top performer, often surpassing more complex models
    \item \myhl[myblue]{\textbf{Robustness}}: \textbf{RF} are less subject to overfitting due to the ensemble nature leveraging multiple decision trees
    \item \myhl[myblue]{\textbf{Interpretability}}: \textbf{RF} provide rankings of feature importance
\end{itemize}

As illustrated in Figure \ref{fig:level2tree}, in a level-2 tree, each node (cell) defines the point where the current cell split and new cells are produced. The sets of features eligible for splitting cells at level $k-1$ are denoted as $\Theta_k\coloneq \left\{\Theta_{k,1},\cdots,\Theta_{k,2^{k-1}} \right\}$, where $\Theta_{k,s}\subset \left\{ 1,\cdots,p \right\}$.

\begin{figure}[ht]
    \begin{center}
        \begin{tikzpicture}[scale=1.2]
            % basics
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (a) at (0,2.5) {Level 0 \\ $\mathbf{t}_0:[0,1]^p$ \\ split: $\left(j_i \in \Theta_{1,1},c_1\right)$};
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (b1) at (-2.5,1) {Cell $\mathbf{t}_{1,1}$ \\ split: $\left(j_2 \in \Theta_{2,1},c_2\right)$};
            \node[draw,rectangle,rounded corners,text width=4cm,align=center] (b2) at (2.5,1) {Cell $\mathbf{t}_{1,2}$ \\ split: $\left(j_3 \in \Theta_{2,2},c_3\right)$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c1) at (-4,0) {Cell $\mathbf{t}_{2,1}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c2) at (-1.25,0) {Cell $\mathbf{t}_{2,2}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c3) at (1.25,0) {Cell $\mathbf{t}_{2,3}$};
            \node[draw,rectangle,rounded corners,text width=2cm,align=center] (c4) at (4,0) {Cell $\mathbf{t}_{2,4}$};
            \draw[-] (a) -- (b1);
            \draw[-] (a) -- (b2);
            \draw[-] (b1) -- (c1);
            \draw[-] (b1) -- (c2);
            \draw[-] (b2) -- (c3);
            \draw[-] (b2) -- (c4);
        \end{tikzpicture}
    \end{center}
    \caption{Level-2 Tree Example}\label{fig:level2tree}
\end{figure}

Given any $T$ (and the associated splitting criterion) and $\Theta_{1:k}$, the tree estimate denoted as $\hat{m}_{T(\Theta_{1:k})}$ for a test point $\mathbf{c}\in [0,1]^p$ is defined as 
$$
\hat{m}_{T(\Theta_{1:k})}\left(\mathbf{c},\mathcal{X}_n\right) \coloneq \sum_{\left(\mathbf{t}_1,\cdots,\mathbf{t}_k\right)\in T(\Theta_{1:k})} \mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}\left( \frac{\sum_{i\in \left\{i:\mathbf{x}_i\in \mathbf{t}_k\right\}} y_i}{\# \left\{i:\mathbf{x}_i\in\mathbf{t}_k\right\}} \right)
$$
where $\mathcal{X}_n\coloneq \left\{ \mathbf{x}_i,y_i \right\}^n_{i=1}$, the fraction is defined as $0$ when no sample is in the cell $\mathbf{t}_k$, and $\mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}$ is an indicator function $=1$ if $\mathbf{c}\in\mathbf{t}_k$ and $=0$ otherwise.

\section{Chi et al. (2022): High Dimensional RFs}
Following \citet{chi2022asymptotic}, for a RF model where
\begin{itemize}
    \item a sequence of distinct $\Theta_{1:k}$ results in a distinct tree
    \item every set of available features $\Theta_{l,s}$, $l=1,\cdots,k$; $s=1,\cdots,2^{l-1}$
\end{itemize}

\paragraph*{Column subsampling} Define a \myhl[myblue]{\textbf{column subsampling}} procedure: $\Theta_{l,s},\forall l,s$ has $[\gamma_0p]$ distinct integers among $1,\cdots,p$, with $[\cdot]$ the ceiling function for some $0<\gamma_0\leq 1$. $\gamma_0$ is the predetermined constant parameter of column subsampling.
Introduce the boldface random mappings $\boldsymbol{\Theta}_{1:k}$, which are independent and uniformly distributed over all possible $\Theta_{1:k}$ for all integer $k$. Then random forests estimate for $\mathbf{c}$ with observations $\mathcal{X}_n$ is given by 
\begin{equation*}
    \mathbb{E}\left( \hat{m}_{T(\boldsymbol{\Theta}_{1:k})}\left(\mathbf{c},\mathcal{X}_n\right) \mid \mathcal{X}_n \right) = \sum_{\Theta_{1:k}} \mathbb{P} \left( \bigcap^k_{s=1} \left\{ \boldsymbol{\Theta}_s = \Theta_s \right\} \right) \hat{m}_{T(\Theta_{1:k})} \left(\mathbf{c},\mathcal{X}_n\right)
\end{equation*}
The expectation is taken over sets of available features.

\paragraph*{Observation resampling}
Let $A=\left\{a_1,\cdots, a_B\right\}$ be a set of subsamples with each $a_i$ consisting of $\lceil bn\rceil$ observations (indices) drawn without replacement from $\left\{1,\cdots,n\right\}$ for some positive integer $B$ and $0<b\leq 1$; in addition, each $a_i$ is independent of model training. The default values of $B$ and $b$ are $500$ and $0.632$\footnote{Or, $b=1$ but observations are drawn with replacement.}. Then the tree estimate using subsample $a$ is 
define as 
$$
\hat{m}_{T(\Theta_{1;k}),a}\left(\mathbf{c},\mathcal{X}_n\right) \coloneq \sum_{\left( \mathbf{t}_1,\cdots,\mathbf{t}_k \right)\in T(\Theta_{1:k})}\mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}\left( \frac{\sum_{i\in a\cap \left\{i:\mathbf{x}_i\in\mathbf{t}_k\right\} }y_i}{\#\left(a\cap \left\{i:\mathbf{x}_i\in\mathbf{t}_k\right\}\right) } \right)
$$
the random forests estimate given $A$ is then 
$$
B^{-1}\sum_{a\in A}\mathbb{E}\left[\hat{m}_{T,a}\left(\boldsymbol{\Theta}_{1:k},\mathbf{c},\mathcal{X}_n \right) \mid \mathcal{X}_n\right] \coloneq B^{-1}\sum_{a\in A}\mathbb{E}\left[\hat{m}_{T(\boldsymbol{\Theta_{1:k}}),a}\left(\mathbf{c},\mathcal{X}_n\right)\mid \mathcal{X}_n\right]
$$

\paragraph*{CART-split criterion}
Given a cell $\mathbf{t}$, a subset of observation indices $a$ and a set of available features $\Theta \subset \left\{ 1,\cdots,p \right\}$, the CART-split is defined as 
\begin{equation}\label{eq:cart_split}
    \left(\hat{j},\hat{c}\right) = \arg\min_{j\in\Theta,c\in \left\{x_{ij}:\mathbf{x}_i\in\mathbf{t},i\in a\right\}} \left[ \sum_{i\in a \cap P_L}\left(\bar{y}_L-y_i\right)^2 + \sum_{i\in a\cap P_R}\left(\bar{y}_R-y_i\right)^2 \right]
\end{equation}
where 
\begin{align*}
    P_L & \coloneq \left\{ i: \mathbf{x}_i\in\mathbf{t},x_{ij}<c \right\} & P_R & \coloneq  \left\{ i: \mathbf{x}_i\in\mathbf{t},x_{ij}\geq c \right\}\\
    \bar{y}_L & \coloneq \sum_{i\in a \cap P_L}\frac{y_i}{\# \left(a\cap P_L\right)} & \bar{y}_R & \coloneq \sum_{i\in a \cap P_R}\frac{y_i}{\# \left(a\cap P_R\right)}
\end{align*}
The CART-split criterion conditional on the sample is a deterministic splitting criterion; conditioning on another sample leads to another deterministic splitting criterion. Define $\hat{T}_a$ as the sample tree growing rule that is associated with a splitting criterion following Eq. (\ref{eq:cart_split}), the tree estimates using $\hat{T}_a$ can be similarly defined as 
\begin{equation*}
    \hat{m}_{\hat{T}_a(\Theta_{1:k})} \left(\mathbf{c},\mathcal{X}_n\right) \coloneq \sum_{\left(\mathbf{t}_1,\cdots,\mathbf{t}_k\right)\in\hat{T}_a\left(\Theta_{1:k}\right)} \mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}\left( \frac{\sum_{i\in \left\{i:\mathbf{x}_i\in \mathbf{t}_k\right\}}y_i}{\#\left\{i:\mathbf{x}_i\in\mathbf{t}_k\right\}} \right)
\end{equation*}
the definition is the same for $\hat{m}_{\hat{T}_a,a}$. Then the random forests estimate for a test point $\mathbf{c}\in [0,1]^p$ is given by 
\begin{equation*}
    B^{-1}\sum_{a\in A}\mathbb{E}\left( \hat{m}_{\hat{T}_a,a} \left(\boldsymbol{\Theta}_{1:k},\mathbf{c},\mathcal{X}_n\right) \mid \mathcal{X}_n\right)
\end{equation*}
where the average and conditional expectation correspond to the sample and column subsamplings respectively, and they are interchangeable.

\paragraph*{Bias-variance decomposition}
For a tree growing rule $T$ and $\Theta_{1:k}$, the population version is defined as 
\begin{equation}
    m^*_{T(\Theta_{1:k})}(\mathbf{c}) \coloneq \sum_{\left(\mathbf{t}_1,\cdots,\mathbf{t}_k\right)\in T(\Theta_{1:k})} \mathbf{1}_{\mathbf{c}\in\mathbf{t}_k}\mathbb{E}\left(m(\mathbf{X})\mid \mathbf{X}\in \mathbf{t}_k\right)
\end{equation}
for each test point $\mathbf{c}\in [0,1]^p$. And the $\mathbb{L}^2$ prediction loss for random forests is defined as 
\begin{equation}\label{eq:l2predictloss}
    \mathbb{E}\left[ m(\mathbf{X}) - B^{-1}\sum_{a\in A}\mathbb{E}\left( \hat{m}_{\hat{T}_a,a} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right)\mid \mathbf{X},\mathcal{X}_n \right) \right]^2
\end{equation}
if we use the full sample $a=\left\{1,\cdots,n\right\}$, and denote $\hat{T}_a$ and $\hat{m}_{\hat{T}_a,a}$ as $\hat{T}$ and $\hat{m}_{\hat{T}}$, the sample subsampling and average $B^{-1}\sum_{a\in A}\left(\cdot\right)$ in the random forests estimate are no 
longer needed, then Eq.(\ref{eq:l2predictloss}) can be simplified as 
\begin{equation*}
    \mathbb{E} \left[m(\mathbf{X})-\mathbb{E}\left(\hat{m}_{\hat{T}}\left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right) \mid \mathbf{X},\mathcal{X}_n\right)\right]^2
\end{equation*}
By Jensen's inequality and Cauchy-Schwarz inequality,
\begin{align*}
    &\frac{1}{2}\mathbb{E} \left[m(\mathbf{X}) - \mathbb{E}\left(\hat{m}_{\hat{T}} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right) \mid \mathbf{X},\mathcal{X}_n\right)\right]^2 \\
    \leq & \underbrace{\mathbb{E}\left[m(\mathbf{X}) - m^*_{\hat{T}} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X}\right) \right]^2}_{\text{approximation error (squared bias)}} + \underbrace{\mathbb{E} \left[m^*_{\hat{T}}\left(\boldsymbol{\Theta}_{1:k},\mathbf{X}\right) -\hat{m}_{\hat{T}}\left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right)\right]^2}_{\text{estimation variance}}
\end{align*}

\section*{Consistency of RF Models}
For a cell $\mathbf{t}$ and its two daughter cells $\mathbf{t}'$ and $\mathbf{t}''$, define 
\begin{align*}
    \left(\mathbb{I}\right)_{\mathbf{t},\mathbf{t}'} \coloneq & \mathbb{P}\left(\mathbf{X}\in \mathbf{t}'\mid \mathbf{X} \in \mathbf{t}\right) \mathrm{Var} \left(m(\mathbf{X}) \mid \mathbf{X}\in\mathbf{t}'\right) + \mathbb{P}\left(\mathbf{X}\in \mathbf{t}''\mid \mathbf{X} \in \mathbf{t}\right)\mathrm{Var} \left(m(\mathbf{X}) \mid \mathbf{X}\in\mathbf{t}''\right) \\
    \left(\mathbb{II}\right)_{\mathbf{t},\mathbf{t}'} \coloneq & \mathbb{P}\left(\mathbf{X}\in \mathbf{t}'\mid \mathbf{X} \in \mathbf{t}\right) \left[\mathbb{E}(m(\mathbf{X})\mid \mathbf{X}\in\mathbf{t}') - \mathbb{E}(m(\mathbf{X})\mid \mathbf{X}\in \mathbf{t})\right]^2\\
    &+ \mathbb{P}\left(\mathbf{X}\in \mathbf{t}''\mid \mathbf{X} \in \mathbf{t}\right) \left[\mathbb{E}(m(\mathbf{X})\mid \mathbf{X}\in\mathbf{t}'') - \mathbb{E}(m(\mathbf{X})\mid \mathbf{X}\in \mathbf{t})\right]^2
\end{align*}
and $\left( \mathbb{I} \right)_{\mathbf{t,t}''}$ and $\left(\mathbb{II}\right)_{\mathbf{t,t}''}$ are defined similarly.

in this context, we assume the following regularity conditions:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Absolutely continuous distribution}}: $f$, the density function of $\mathbf{X}$, is bounded away from $0$ and $\infty$
    \item \myhl[myblue]{\textbf{Covariates and model errors}}: assume $p=O(n^{K_0})$ for $K_0>0$, and there is a symmetric distribution around $0$ for $\epsilon_1$, s.t. $\mathbb{E}\left\vert \epsilon_1 \right\vert^q < \infty$ for sufficiently large $q>0$
    \item \myhl[myblue]{\textbf{Bounded regression functions}}: $\sup_{\mathbf{c}\in [0,1]^p}\left\vert m(\mathbf{c}) \right\vert \leq M_0$, for some $M_0>0$
    \item \myhl[myblue]{\textbf{Sufficient impurity decrease}}: $\exists \alpha_1\geq 1$ s.t. $\forall \mathbf{t} = t_1\times \cdots \times t_p$, $$ \mathrm{Var}\left[m(\mathbf{X})\mid \mathbf{X}\in\mathbf{t}\right] \leq \alpha_1 \sup_{j\in \left\{1,\cdots, p \right\}, c\in t_j} \left(\mathbb{II}\right)_{\mathbf{t,t}(j,c)}$$where 
    \begin{itemize}
        \item $\left( \mathbb{II} \right)_{\mathbf{t,t}'}$: conditional bias decrease (or conditional impurity decrease)
        \item $\mathrm{Var}\left[m(\mathbf{X})\mid \mathbf{X}\in\mathbf{t}\right]$: conditional \textbf{\textit{total}} bias, $\mathrm{Var}\left[m(\mathbf{X})\mid \mathbf{X}\in\mathbf{t}\right]= \left( \mathbb{I} \right)_{\mathbf{t,t}'}+\left( \mathbb{II} \right)_{\mathbf{t,t}'}$
        \item \textbf{Intuition:} having large conditional bias decrease on each cell is a desired property for achieving a good control of the squared bias of random forests estimate
    \end{itemize}
\end{itemize}

\paragraph*{Sufficient impurity decrease (SID)}
Define the functional class 
$$
\mathrm{SID}(\alpha) \coloneq \left\{ m(\mathbf{X}): m(\mathbf{X}) \text{ satisfies SID with }\alpha_1 \leq \alpha \right\}
$$
the size of $\mathrm{SID}(\alpha)$ is \textbf{non-decreasing} in $\alpha \geq 1$: if $m(\mathbf{X})\in \mathrm{SID}(\alpha -c)$ for some $\alpha-c\geq 1$ and $c>0$, then $m(\mathbf{X})\in \mathrm{SID}(\alpha)$\footnote{Many popular regression functions belong to this functional class.}.

Under the regularity conditions mentioned above, we have the following theorem
\begin{theorem}{Consistency}{main_consistency}
    Let $0<b\leq 1,0<\gamma_0\leq 1,\alpha_2 >1,0<\eta<1/8,0<c<1/4$ and $\delta>0$, $2\eta < \delta <\frac{1}{4}$. Let $A=\left\{ a_1,\cdots,a_B \right\}$ with $\# a_i = \lceil bn \rceil $ for $i=1,\cdots,B$ and $a\in A$. Then $\exists C>0$ s.t. for all large $n$ and each $1\leq k \leq c \log_2\lceil bn \rceil$, 
    \begin{align*}
        &\mathbb{E}\left[ m(\mathbf{X}) - \mathbb{E}\left(\hat{m}_{\hat{T}_a,a} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right)\mid \mathbf{X},\mathcal{X}_n\right) \right]^2\\
        \leq & C\left[ \alpha_1 \left(\lceil bn \rceil\right)^{-\eta} + \left(1-\gamma_0 \left(\alpha_1\alpha_2\right)^{-1}\right)^k + \left(\lceil bn \rceil\right)^{-\delta+c} \right]
    \end{align*} 
    In addition, when aggregate over row subsamples (over $a\in A$), get 
    \begin{align*}
        &\mathbb{E}\left[ m(\mathbf{X}) - \frac{1}{B}\mathbb{E}\left(\hat{m}_{\hat{T}_a,a} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right)\mid \mathbf{X},\mathcal{X}_n\right) \right]^2\\
        \leq & C\left[ \alpha_1 \left(\lceil bn \rceil\right)^{-\eta} + \left(1-\gamma_0 \left(\alpha_1\alpha_2\right)^{-1}\right)^k + \left(\lceil bn \rceil\right)^{-\delta+c} \right]
    \end{align*}
\end{theorem}
Here, the feature dimensionality $p$ and tree height $k$ decide the number of all possible cells when growing trees. 

\paragraph*{Bias-variance decomposition}
Under Theorem (\ref{thm:main_consistency}), both bias and variance depend implicitly on $p$ through $n$ in the upper bounds.
\begin{proposition}{Bias-Variance Decomposition}{bias_var_decomp}
    Under Thm (\ref{thm:main_consistency}), for all large $n$ and $1\leq k\leq c\log _2 n$, it holds that
    \begin{align*}
        \text{Squared bias} & \coloneq \underbrace{\mathbb{E}\left[m(\mathbf{X}) - m^*_{\hat{T}} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X}\right) \right]^2}_{\text{approximation error}} \\
        & \leq O\left( n^{-\eta} + \underbrace{\left(1-\gamma_0(\alpha_1\alpha_2)^{-1}\right)^k}_{\text{Main term of bias}} \right) + \underbrace{O\left(n^{-\delta+c}\right)}_{\text{Uninteresting error}}
    \end{align*} 
    and 
    \begin{align*}
        \text{Squared bias} & \coloneq \underbrace{\mathbb{E}\left[m(\mathbf{X}) - m^*_{\hat{T}} \left(\boldsymbol{\Theta}_{1:k},\mathbf{X}\right) \right]^2}_{\text{approximation error (squared bias)}} + \underbrace{\mathbb{E} \left[m^*_{\hat{T}}\left(\boldsymbol{\Theta}_{1:k},\mathbf{X}\right) -\hat{m}_{\hat{T}}\left(\boldsymbol{\Theta}_{1:k},\mathbf{X},\mathcal{X}_n\right)\right]^2}_{\text{estimation variance}}
    \end{align*} 
\end{proposition}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}