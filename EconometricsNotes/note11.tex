\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{11}{Lasso}{}{Sai Zhang}{}{}

Lasso (Least absolute Shrinkage and Selection Operator), proposed by \citet{tibshirani1996regression}, aims to minimize the \myhl[blue!45!black]{\textbf{SSR \textit{(sum of residual squares)}}} subject to the \myhl[red!55!black]{\textbf{$L1-$norm \textit{(sum of the absolute value)}}} of the coefficients being less than a constant.

\section{Set up}
For data $\left(\mathbf{x}_i,y_i\right)^n_{i=1}$, where
\begin{itemize}
    \item[-] $y_i$ is the outcome for individual $i$
    \item[-] $\mathbf{x}_i = \left(x_{i1},\cdots,x_{ip}\right)'$ is the $p\times 1$ vector of predictors
\end{itemize}
Then the Lasso estimator $\left(\hat{\alpha},\hat{\boldsymbol{\beta}}\right)$ is defined as 
\begin{align*}
    \left(\hat{\alpha},\hat{\boldsymbol{\beta}}\right) &= \arg\min_{\alpha,\boldsymbol{\beta}}\left\{ \sum^n_{i=1}\left(y_i-\alpha-\sum^p_{j=1}\beta_jx_{ij}\right)^2 \right\} & \text{s.t. }\sum^p_{j=1}\lvert \beta_j \rvert\leq t
\end{align*}


for the $n\times 1$ response vector $\mathbf{y}=\left( y_1,\cdots,y_n \right)'$, the $n\times p$ design matrix $\mathbf{X}=\left(\mathbf{x}_1,\cdots,\mathbf{x}_n\right)'$ where $\mathbf{x}_i= \left( x_{i1},cdots,x_{ip} \right)' $ is a $p\times 1$ vector.

\section{Penalized Least Square Estimation}
Lasso is one special class of Penalized Least Square (PLS) Estimation. For the linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, if $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\sigma^2 \mathbf{I}_n)$, we have PLS as
$$
\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 + \sum^p_{j=1}p_{\lambda}\left( \lvert \beta_j \rvert \right) \right\}
$$



\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}