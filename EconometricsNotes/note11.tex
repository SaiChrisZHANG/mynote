\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{11}{Lasso And Beyond: Convex Learning}{}{Sai Zhang}{}{}

\section{Lasso}
Lasso (Least absolute Shrinkage and Selection Operator), proposed by \citet{tibshirani1996regression}, aims to minimize the \myhl[blue!45!black]{\textbf{SSR \textit{(sum of residual squares)}}} subject to the \myhl[red!55!black]{\textbf{$L1-$norm \textit{(sum of the absolute value)}}} of the coefficients being less than a constant.

\subsection{Set up}
For data $\left(\mathbf{x}_i,y_i\right)^n_{i=1}$, where
\begin{itemize}
    \item[-] $y_i$ is the outcome for individual $i$
    \item[-] $\mathbf{x}_i = \left(x_{i1},\cdots,x_{ip}\right)'$ is the $p\times 1$ vector of predictors
\end{itemize}
Then the Lasso estimator $\left(\hat{\alpha},\hat{\boldsymbol{\beta}}\right)$ is defined as 
\begin{align*}
    \left(\hat{\alpha},\hat{\boldsymbol{\beta}}\right) &= \arg\min_{\alpha,\boldsymbol{\beta}}\left\{ \sum^n_{i=1}\left(y_i-\alpha-\sum^p_{j=1}\beta_jx_{ij}\right)^2 \right\} & \text{s.t. }\sum^p_{j=1}\lvert \beta_j \rvert\leq t
\end{align*}

for the $n\times 1$ response vector $\mathbf{y}=\left( y_1,\cdots,y_n \right)'$, the $n\times p$ design matrix $\mathbf{X}=\left(\mathbf{x}_1,\cdots,\mathbf{x}_n\right)'$ where $\mathbf{x}_i= \left( x_{i1},\cdots,x_{ip} \right)' $ is a $p\times 1$ vector. Here $\hat{\alpha}=\bar{y}$, w.l.o.g., let $\bar{y}=0$ and omit $\alpha$ for simplicity.

In matrix form, we have
\begin{itemize}
    \item constrained form:
    \begin{align*}
        \hat{\boldsymbol{\beta}} &= \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 \right\} & \text{s.t.\ } & \lVert \boldsymbol{\beta} \rVert_1 \leq t
    \end{align*}
    \item unconstrained form:
    \begin{align*}
        \hat{\boldsymbol{\beta}}(\lambda) &= \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 +\lambda \lVert \boldsymbol{\beta} \rVert_1 \right\}
    \end{align*}
\end{itemize}
where the regularization parameter $\lambda \geq 0$:
\begin{itemize}
    \item[-] $\lambda \rightarrow \infty$: $\hat{\boldsymbol{\beta}}_{lasso}=\mathbf{0}$
    \item[-] $\lambda = 0$: $\hat{\boldsymbol{\beta}}_{lasso} \rightarrow\hat{\boldsymbol{\beta}}_{OLS}$
\end{itemize}

\subsection{Solving Lasso}
Lasso is essentially a quadratic optimization problem. Hence, the solution is given by taking the derivative (of the unconstrainted question) and set it equal to 0
\begin{align*}
   && \frac{\mathrm{d}}{\mathrm{d}\boldsymbol{\beta}} \left( \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 +\lambda \lVert \boldsymbol{\beta} \rVert_1 \right) = 0 \\
   \Rightarrow && \frac{1}{n} \underbrace{\mathbf{X}'}_{p\times n} \underbrace{ \left(\mathbf{y}-\mathbf{X}\boldsymbol{\beta} \right)}_{=\boldsymbol{\epsilon}, n \times 1} &= \lambda \begin{cases}
    \mathrm{sign}\left(\beta_j\right), & \beta_j\neq 0\\
    \left[ -1,1 \right], & \beta_j =0
   \end{cases}
\end{align*}
this result follows the fact the L-1 norm $\lVert \boldsymbol{\beta} \rVert$ is piecewise linear:

\begin{minipage}{0.45\textwidth}
    \centering
        \begin{tikzpicture}[scale=1.4]
            % basics
            \draw [-stealth,color=black,very thick] (2,-1) -- (2,2);
            \draw [-stealth,color=black,very thick] (0,0) -- (4,0);
            \draw [name path = neg_up, color = myblue, very thick] (0,2) -- (2,0);
            \draw [name path = pos_up, color = myred, very thick] (2,0) -- (4,2);
            \draw [name path = pos_down, color = myblue, very thick, densely dashed] (2.8,-0.8) -- (2,0);
            \draw [name path = neg_down, color = myred, very thick, densely dashed] (2,0) -- (1.2,-0.8);
            \filldraw [color=black] (2,0) circle (2pt);
            \draw [color = black, very thick, densely dashed] (0.5,-0.25) -- (3.5,0.25);
            \draw [color = black, very thick, densely dashed] (0.5,0.75) -- (3.5,-0.75);
            \tikzfillbetween[of=neg_up and neg_down, on layer=bg]{black!20!white};
            \tikzfillbetween[of=pos_up and pos_down, on layer=bg]{black!20!white};
        \end{tikzpicture}

        L1-norm (1-dimension)
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
    For each component of the vector of the L-1 norm $f(\beta_j) = \lvert \beta_j \rvert$, we have:
    \begin{itemize}
        \item[-] \textcolor{myred}{$\beta_j>0$: $f'(\beta_j) = 1$}
        \item[-] \textcolor{myblue}{$\beta_j<0$: $f'(\beta_j) = -1$}
        \item[-] $\beta_j=0$: $\mathrm{d}f \in [-1,1]$ (shaded area)
    \end{itemize}
    which gives the results stated above.
\end{minipage}

Take another look at this result 
\begin{proposition}{Lasso Parameter Selection Rule}{lasso-math}
    $$
    \frac{1}{n} \mathbf{X}'  \left(\mathbf{y}-\mathbf{X}\boldsymbol{\beta} \right) =\frac{1}{n} \mathbf{X}'\boldsymbol{\epsilon}  = \lambda \begin{cases}
        \mathrm{sign}\left(\beta_j\right), & \beta_j\neq 0\\
        \left[ -1,1 \right], & \beta_j =0
    \end{cases}
    $$
    which gives a parameter selection criterion: for $\beta_j \neq 0$, \myhl[myred]{$\mathrm{sign}(\beta_j)$ \textbf{must agree} with $\mathrm{Corr}(\mathbf{x}_j,\boldsymbol{\epsilon})$}, the correlation between the $j$-th variable $\mathbf{x}_j$ and (full-model) residuals $\boldsymbol{\epsilon}=\mathbf{y}-\mathbf{X}\boldsymbol{\beta}$.
\end{proposition}

\subsection{Algorithm: LARS}
Mathematically, Lasso is quite intuitive, but computationally, it can be quite consuming. \citet{efron2004least} propose an algorithm that takes steps from a all-0 model to the biggest model (OLS), that is, \textbf{Least Angle Regression (LARS)}. 

\subsubsection*{Intuition}
The basic intuition of LARS is quite straight-forward: covariates are considered from the \myhl[myred]{\textbf{\textit{highest}}} corerlation with $\mathbf{y}$ (\textit{smallest} angle from $\mathbf{y}$) to the \myhl[myblue]{\textbf{\textit{least}}} correlated one (\textit{largest} angle from $\mathbf{y}$) (illustrated below).

\begin{figure}[ht]
    \begin{minipage}[b]{0.45\textwidth}
    \centering
      \begin{tikzpicture}[scale=1.4]
          % basics
          \draw [-stealth,color=black,thin] (-0.5,0) -- (4,0) node[right] {$x_1$};
          \draw [-stealth,color=black,thin] (0,-0.5) -- (0,4) node[above] {$x_2$};
          \draw [color=black, ultra thick] (0,0) -- (3.5,1.5) node[above] {OLS};
          \draw [color=myred, ultra thick] (0,0) -- (2,0) node[below] {Step 1};
          \draw [color=myblue, ultra thick] (2,0) -- (3.5,1.5) node[below,xshift=15pt] {Step 2};
          
          \coordinate (X) at (3.5,1.5);
            \coordinate (A) at (2,0);
            \coordinate (Y) at (3,0);

            % Draw Angle
            \draw[thin] (Y) -- (A) -- (X)
            pic ["$45^{\circ}$", draw, thin, angle eccentricity=1.8] {angle = Y--A--X};
      \end{tikzpicture}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \tdplotsetmaincoords{60}{45}
        \begin{tikzpicture}
            [scale=5,
                tdplot_main_coords,
                axis/.style={-stealth,black,thin},
                vector/.style={black,very thick},
                LARS/.style={ultra thick}]

            %standard tikz coordinate definition using x, y, z coords
            \coordinate (O) at (0,0,0);
            
            \coordinate (P) at (0.8, 0.6, 0.2);

            \coordinate (P12) at (0.8, 0.6, 0);
            \coordinate (P23) at (0, 0.6, 0.2);
            \coordinate (P13) at (0.8, 0, 0.2);
            \coordinate (P1) at (0.8, 0, 0);
            \coordinate (P2) at (0, 0.6, 0);
            \coordinate (P3) at (0, 0, 0.2);
            
            %draw axes
            \draw[axis] (0,0,0) -- (1,0,0) node[anchor=north east]{$x_1$};
            \draw[axis] (0,0,0) -- (0,1,0) node[anchor=north west]{$x_2$};
            \draw[axis] (0,0,0) -- (0,0,1) node[anchor=south]{$x_3$};

            % projection dash line
            \draw[thin, black, densely dashed] (P12)--(P1);
            \draw[thin, black, densely dashed] (P12)--(P2);
            \draw[thin, black, densely dashed] (P)--(P13);
            \draw[thin, black, densely dashed] (P)--(P23);
            \draw[thin, black, densely dashed] (P23)--(P3);
            \draw[thin, black, densely dashed] (P13)--(P3);
            \draw[thin, black, densely dashed] (P23)--(P2);
            \draw[thin, black, densely dashed] (P13)--(P1);
            \draw[thin, black, densely dashed] (P)--(P12);
            
            %draw a vector from O to P
            \draw[vector] (O) -- (P);

            % draw LARS steps
            \draw[LARS, myred] (O) -- (0.2,0,0) node[left,xshift=-10pt] {Step 1};
            \draw[LARS, myblue] (0.2,0,0) -- (0.6,0.4,0) node[below] {Step 2};
            \draw[LARS, teal] (0.6,0.4,0) -- (P) node[right] {Step 3};

            % approaching, but not exactly reach OLS
            %\draw[thin, teal, densely dashed] (P13)--(0.8,0.35,0.2);
            %\draw[thin, teal, densely dashed] (0.8,0.35,0.2)--(0,0.35,0.2);
        \end{tikzpicture}
    \end{minipage}
\end{figure}

And the steps of the LARS algorithm are
\begin{itemize}
    \item[1] start with the null model $\hat{\boldsymbol{\beta}}=\mathbf{0}$: $\hat{\boldsymbol{\mu}} = \mathbf{X}'\mathbf{0}=\mathbf{0}$ 
    \item[2] calculate residual vector $\mathbf{r} = \mathbf{y}-\hat{\boldsymbol{\mu}}$
    \item[3] determine the correlation vector between $\mathbf{r}$ and each parameter $\mathbf{x}_j,\forall j=1,\cdots,p$: $\mathbf{X}'\mathbf{r}$
    \item[4] pick the largest correlation $\mathbf{x}^*_{\text{step}1,1}$, increase its $\hat{{\beta}}$ to the point where its correlation with $\mathbf{r}$ will be \myhl[red!55!black]{\textbf{equal}} with that of another parameter $\mathbf{x}^*_{\text{step1},2}$ 
    \item[5] next, increase the $\hat{\beta}$ for both $\mathbf{x}^*_{\text{step1},1},\mathbf{x}^*_{\text{step1},2}$ in an \myhl[red!55!black]{\textbf{equiangular}} direction between these two, until a third parameter becomes equally important
\end{itemize}
And keep looping this way, until all the predictors enter the model and eventually $\mathbf{X}'\mathbf{r}=\mathbf{0}$

\subsubsection*{Properties of LARS}
LARS has several properties:
\begin{itemize}
    \item geometrically travels in the direction of \textbf{equal} angle to all active covariates
    \item assume all covariates are independent
    \item computationally quick: only take $m$ steps, where $m$ is the number of parameters being considered
\end{itemize}
And it is in between 2 classic model-selection methods: \textbf{Forward Selection} and \textbf{Stagewise Selection}:
\begin{itemize}
    \item \myhl[red!55!black]{\textbf{Forward Selection}}
    \begin{itemize}
        \item for $\mathbf{y}$, select the most correlated $\mathbf{x}_{j_1}$
        \item regress $\mathbf{x}_{j_1}$ on $\mathbf{y}$, get the residuals
        \item select the most correlated $\mathbf{x}_{j_2}$ with the redisual of $\mathbf{y}$ net of $\mathbf{x}_{j_1}$
    \end{itemize}
    looping this, for a $k-$parameter linear model, it takes $k$ steps. Forward Selection is an aggressive fitting technique, can be overly greedy (some important predictors may be eliminated due to correlation with already selected variables).
    \item \myhl[red!55!black]{\textbf{Forward Stagewise}}
    \begin{itemize}
        \item also begin with $\hat{\boldsymbol{\mu}}=0$
        \item for a current Stagewise estimate $\hat{\boldsymbol{\mu}}$, the current residual vector is then $\mathbf{y}-\hat{\boldsymbol{\mu}}$, its correlation with $\mathbf{X}$ is then  $\mathbf{X'}(\mathbf{y}-\hat{\boldsymbol{\mu}})\equiv \hat{\mathbf{c}}$
        \item next, heavily computational, go in the direction of the greatest current correlation, but by only a \myhl[red!55!black]{\textbf{small}} step $$ \hat{j}= \arg\max\lvert \hat{c}_j \rvert, \ \hat{\boldsymbol{\mu}}\rightarrow \hat{\boldsymbol{\mu}}+\epsilon\cdot \mathrm{sign}(\hat{c}_{\hat{j}})\cdot \mathbf{x}_{\hat{j}} $$
        here, $\epsilon$ is a \textbf{small} constant, hence avoiding the greediness of Forward Selection, at a cost of computational efficiency\footnote{Forward Selection is essentially choosing $\epsilon=\lvert \hat{c}_{\hat{j}} \rvert$}.
    \end{itemize}
\end{itemize}
LARS avoids the over-greediness of Forward Selection and computational heaviness of Forward Stagewise.

\subsection{From LARS to Lasso}
The Lasso algorithm is built upon LARS, with the constraint from the mathematical condition of Proposition \ref{prop:lasso-math}: \myhl[myblue]{$\mathrm{sign}(\beta_j)$ \textbf{must agree} with $\mathrm{Corr}(\mathbf{x}_j,\boldsymbol{\epsilon})$}. 

\begin{theorem}{Lasso Modification Condition}{lasso-modif}
    If $\tilde{\gamma}< \hat{\gamma}$, stop the onging LARS step at $\gamma = \tilde{\gamma}$ and remove $j$ from the calculation of the next equiangular direction, where
    \begin{itemize}
        \item the path at any LARS step is $$\boldsymbol{\beta}(\gamma),\ \beta_j(\gamma) = \hat{\beta}_j + \gamma\hat{d}_j$$ $\hat{d}_j$ specifies the \myhl[blue!45!black]{\textbf{direction}} to take the $j$-th component, $\gamma$ is \myhl[blue!45!black]{\textbf{how far}} to travel in the direction of $\hat{d}_j$ before adding in a new covariate
        \item $\hat{\gamma}$ represents the smallest \myhl[red!55!black]{\textbf{positive}} value of $\gamma$ s.t. some new covariate joins the active set (the set of covariates used on path)
        \item $\tilde{\gamma}$ represents the first time $\beta_j(\gamma)$ \myhl[red!55!black]{\textbf{changes signs}}.
    \end{itemize}
\end{theorem}
The key point of \ref{thm:lasso-modif} is that Lasso does \textbf{NOT} allow the $\hat{\beta}_j$ to change signs, if it changes sign, it will be substracted from the active set. Now, from this point of view, we can compare the 3 algorithms:

\begin{center}
    \begin{tabular}{ r|l } 
     \hline
     LARS & no sign restrictions  \\ 
     Lasso &  $\hat{\beta}_j$ agrees in sign with $\hat{c}_j$ \\
     Stagewise & successive differences of $\hat{\beta}_j $ agree in sign with the current correlation $\hat{c}=\mathbf{x}_j'(\mathbf{y}-\hat{\boldsymbol{\mu}})$\\
     \hline
    \end{tabular}
\end{center}

Again, LARS requires the least steps but is most greedy, Stagewise is computationally consuming but robust. Lasso is in between.

\section{Consistency of Lasso}
Next, we want to establish the consistency of Lasso, by showing that Lasso selects exactly the relevant covariates asymptotically. We do this in 2 steps:
\begin{itemize}
    \item show that Lasso at least captures all the relevant covariates
    \item asymptotically, under some conditions, Lasso selects exactly all the relevant covariates, not more
\end{itemize}

\subsection{Overestimation}
First, Lasso tends to select a superset of the relevant covariates.

Define the true relevant set Lasso selection estimation $\hat{S}_0$ aim to select as
    $$ S_0 = \left\{ j:\beta^0_j \neq 0,j \neq 1,\cdots,p \right\} $$
    and for some $C>0$, define the relevant set w.r.t. $C$ as $$ S_0^{\text{relevant}(C)} = \left\{ j: \lvert \beta^0_j \rvert \geq C, j=1,\cdots,p \right\} $$ then we have
\begin{theorem}{Lasso Overestimation Condition}{lasso-overest}
    $\forall 0<C<\infty$ $$ \mathbb{P}\left[ \hat{S}_0(\lambda) \supset S_0^{\text{relevant}(C)} \right] \xrightarrow{n\rightarrow \infty}1 $$
\end{theorem}

\subsection*{Consistency}
The consistency of Lasso is established by \citet{meinshausen2006variable} as
\begin{theorem}{Consistency of Lasso}{lasso_consistency}
    For a suitable $\lambda = \lambda_n \gg \sqrt{s_0\log(p)/n}$, Lasso is consistent, i.e. $$ \mathbb{P}\left[ \hat{S}(\lambda)=S_0 \right] \xrightarrow{n\rightarrow\infty}1 $$
    if and only if it satisfies the 2 properties:
    \begin{itemize}
        \item $\beta-$min condition (unselected coefficients non-trivial): $\inf_{j\in S^c_0}\lvert \beta^0_j \rvert \gg \sqrt{s_0 \log(p)/n}$
        \item \myhl[blue!45!black]{\textbf{irrepresentable} condition}: $\mathbf{X}$ should \textbf{NOT} exhibit too strong a degree of linear dependence w.r.t. the selected covariates
    \end{itemize}
\end{theorem}

\paragraph*{discussion on the irrepresentable condition} denote $\hat{\Sigma}=n^{-1}\mathbf{XX}'$, and let the active set $S_0=\left\{ j:\beta^0_j\neq 0 \right\}=\left\{ 1,\cdots, s_0 \right\}$ consists of the first $s_0$ variables, let 
$$ \hat{\Sigma} = \left( \begin{matrix}
    \hat{\Sigma}_{1,1} & \hat{\Sigma}_{1,2} \\
    \hat{\Sigma}_{2,1} & \hat{\Sigma}_{2,2}
\end{matrix} \right) $$
where $\hat{\Sigma}_{1,1}$ is a $s_0\times s_0$ var-cov matrix of the active variables, $\hat{\Sigma}_{2,2}$ is a $(p-s_0)\times (p-s_0)$ cov-var matrix of the other variables

\subsection{Oracle}
Next, we want show Lasso has the oracle procedure, which gives the consistency.
\begin{definition}{Oracle Property}{oracle}
    For a fitting procedure $\delta$, and the estimation $\hat{\beta}(\delta)$, then if $\delta$ is an oracle procedure if $\hat{\beta}(\delta)$ asymptotically has the following properties 
    \begin{itemize}
        \item \myhl[red!55!black]{\textbf{consistency}} (identifying right subset model): $\left\{ j:\hat{\beta}_j \neq 0 \right\} = S_0$
        \item \myhl[red!55!black]{\textbf{optimal estimation rate}} (asymptotically normal): $\sqrt{n}\left( \beta(\delta)_{S_0} - \beta^0_{S_0} \right) \xrightarrow{d} \mathcal{N}(0,\Sigma_0)$, where $\Sigma_0$ is the \textit{true} subset covariance matrix
    \end{itemize}
\end{definition}



\section{Variants of Lasso}

\subsection{Other Variants}
There are also some other useful variants of Lasso
\begin{itemize}
    \item \textbf{\underline{Positive Lasso}}: Constrains the $\hat{\beta}_j$ to enter the prediction equation in their \textbf{defined} directions, non-negative here 
    \begin{align*}
        \hat{\boldsymbol{\beta}} &= \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 \right\} & \text{s.t.\ } & \lVert \boldsymbol{\beta} \rVert _1 \leq t \textcolor{myred}{\text{ and } \beta_j>0, \forall j}
    \end{align*}
    \item \textbf{\underline{LARS-OLS hybrid}}: Use the covariates selected by LARS, but use $\hat{\boldsymbol{\beta}}$ from the OLS model 
    \item \textbf{\underline{Main effects first}}:
    \begin{itemize}
        \item Step 1: run LARS for a model, considering \textbf{only} main effects 
        \item Step 2: run LARS again, with the chosen main effects, and \textbf{all possible interactions} between them
    \end{itemize}
    \item \textbf{\underline{Backward Lasso}}: start from the \textbf{full} OLS model, and eliminate covariates \textbf{backwards} (by the order of correlation going 0 the earliest)
\end{itemize}

\section{Penalized Least Square Estimation}
Lasso is one special class of Penalized Least Square (PLS) Estimation. For the linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, if $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\sigma^2 \mathbf{I}_n)$, we have PLS as
$$
\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 + \sum^p_{j=1}p_{\lambda}\left( \lvert \beta_j \rvert \right) \right\}
$$
where $p_{\lambda}(\cdot)$ is a penalty function indexed by the regularization parameter $\lambda \geq 0$. \citet{antoniadis2001regularization} showed that the PLS estimator $\hat{\boldsymbol{\beta}}$ has the following properties:
\begin{itemize}
    \item \textbf{\underline{sparsity}}: if $\min_{t\geq 0}\left\{ t+p'_{\lambda}(t) \right\}>0$
    \item \textbf{\underline{approximate unbiasedness}}: if $p'_{\lambda}(t)=0$ for $t$ large enough
    \item \textbf{\underline{continuity}}: iff $\arg \min_{t\geq 0}\left\{ t+p'_{\lambda}(t) \right\}=0$
\end{itemize}
In general
\begin{itemize}
    \item[-] the \myhl[red!55!black]{\textbf{sigularity}} of penalty function at the origin, $p'_{\lambda}(0_+)>0$ is needed for generating \textbf{sparsity} in variable selection
    \item[-] the \myhl[red!55!black]{\textbf{concavity}} is needed to reduce the bias
\end{itemize}


\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
    \centering
      \begin{tikzpicture}[scale=1.6]
          % basics
          \draw [-stealth,color=black,thick] (0,2) -- (4,2);
          \draw [-stealth,color=black,thick] (2,0) node[below] {Lasso} -- (2,4);
          \draw [name path=lup,color=black,thick] (1,2) -- (2,3) -- (3,2);
          \draw [name path=ldown,color=black,thick]  (3,2) -- (2,1) -- (1,2);
          \draw [name path=lmid, color=black, thick] (1,2) -- (3,2);
          \tikzfillbetween[of=lup and ldown, on layer=main]{myblue};
          \draw [rotate=45, thick, color=myred] (5.05,0.7) ellipse (1.5cm and 0.5cm);
          \draw [rotate=45, thick, color=myred] (5.05,0.7) ellipse (1.2cm and 0.4cm);
          \draw [rotate=45, thick, color=myred] (5.05,0.7) ellipse (0.75cm and 0.25cm);
          \filldraw [color=myred] (3.05,4.05) circle (1pt) node[right] {$\hat{\boldsymbol{\beta}}$};
          \filldraw [color=myred] (2,3) circle (2pt);
      \end{tikzpicture}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \begin{tikzpicture}[scale=1.6]
            % basics
            \draw [-stealth,color=black,thick] (0,2) -- (4,2);
            \draw [-stealth,color=black,thick] (2,0) node[below] {Ridge} -- (2,4);
            \draw [color=black,thick] (2,2) -- (2,1);
            %\draw [name path=ldown,color=black,thick]  (3,2) -- (2,1) node[right,xshift=2pt, yshift=-2pt] {Lasso} -- (1,2);
            %\draw [name path=lmid, color=black, thick] (1,2) -- (3,2);
            \node[circle,draw,minimum size=3.2cm, fill =myblue] (c) at (2,2){};
            \draw [rotate=45, thick, color=myred] (5.1,0.72) ellipse (1.5cm and 0.5cm);
            \draw [rotate=45, thick, color=myred] (5.1,0.72) ellipse (1.2cm and 0.4cm);
            \draw [rotate=45, thick, color=myred] (5.1,0.72) ellipse (0.75cm and 0.25cm);
            \filldraw [color=myred] (3.05,4.05) circle (1pt) node[right] {$\hat{\boldsymbol{\beta}}$};
            \filldraw [color=myred] (2.14,2.97) circle (2pt);
        \end{tikzpicture}
    \end{minipage}
\end{figure}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}