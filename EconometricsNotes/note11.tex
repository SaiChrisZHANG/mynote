\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{11}{Lasso And Beyond: Convex Learning}{}{Sai Zhang}{}{}

\section{Lasso}
Lasso (Least absolute Shrinkage and Selection Operator), proposed by \citet{tibshirani1996regression}, aims to minimize the \myhl[blue!45!black]{\textbf{SSR \textit{(sum of residual squares)}}} subject to the \myhl[red!55!black]{\textbf{$L1-$norm \textit{(sum of the absolute value)}}} of the coefficients being less than a constant.

\subsection{Set up}
For data $\left(\mathbf{x}_i,y_i\right)^n_{i=1}$, where
\begin{itemize}
    \item[-] $y_i$ is the outcome for individual $i$
    \item[-] $\mathbf{x}_i = \left(x_{i1},\cdots,x_{ip}\right)'$ is the $p\times 1$ vector of predictors
\end{itemize}
Then the Lasso estimator $\left(\hat{\alpha},\hat{\boldsymbol{\beta}}\right)$ is defined as 
\begin{align*}
    \left(\hat{\alpha},\hat{\boldsymbol{\beta}}\right) &= \arg\min_{\alpha,\boldsymbol{\beta}}\left\{ \sum^n_{i=1}\left(y_i-\alpha-\sum^p_{j=1}\beta_jx_{ij}\right)^2 \right\} & \text{s.t. }\sum^p_{j=1}\lvert \beta_j \rvert\leq \lambda
\end{align*}

for the $n\times 1$ response vector $\mathbf{y}=\left( y_1,\cdots,y_n \right)'$, the $n\times p$ design matrix $\mathbf{X}=\left(\mathbf{x}_1,\cdots,\mathbf{x}_n\right)'$ where $\mathbf{x}_i= \left( x_{i1},\cdots,x_{ip} \right)' $ is a $p\times 1$ vector. Here $\hat{\alpha}=\bar{y}$, w.l.o.g., let $\bar{y}=0$ and omit $\alpha$ for simplicity.

In matrix form, we have
\begin{itemize}
    \item constrained form:
    \begin{align*}
        \hat{\boldsymbol{\beta}} &= \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 \right\} & \text{s.t.\ } & \lVert \boldsymbol{\beta} \rVert_1 \leq \lambda
    \end{align*}
    \item unconstrained form:
    \begin{align*}
        \hat{\boldsymbol{\beta}}(\lambda) &= \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 +\lambda \lVert \boldsymbol{\beta} \rVert_1 \right\}
    \end{align*}
\end{itemize}
where the regularization parameter $\lambda \geq 0$:
\begin{itemize}
    \item[-] $\lambda \rightarrow \infty$: $\hat{\boldsymbol{\beta}}_{lasso}\rightarrow\hat{\boldsymbol{\beta}}_{OLS}$
    \item[-] $\lambda = 0$: $\hat{\boldsymbol{\beta}}_{lasso}=\mathbf{0}$
\end{itemize}

\subsection{Solving Lasso}
Lasso is essentially a quadratic optimization problem. Hence, the solution is given by taking the derivative (of the unconstrainted question) and set it equal to 0
\begin{align*}
   && \frac{\mathrm{d}}{\mathrm{d}\boldsymbol{\beta}} \left( \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 +\lambda \lVert \boldsymbol{\beta} \rVert_1 \right) = 0 \\
   \Rightarrow && \frac{1}{n} \underbrace{\mathbf{X}'}_{p\times n} \underbrace{ \left(\mathbf{y}-\mathbf{X}\boldsymbol{\beta} \right)}_{=\boldsymbol{\epsilon}, n \times 1} &= \lambda \begin{cases}
    \mathrm{sign}\left(\beta_j\right), & \beta_j\neq 0\\
    \left[ -1,1 \right], & \beta_j =0
   \end{cases}
\end{align*}
this result follows the fact the L-1 norm $\lVert \boldsymbol{\beta} \rVert$ is piecewise linear:

\begin{minipage}{0.45\textwidth}
    \centering
        \begin{tikzpicture}[scale=1.4]
            % basics
            \draw [-stealth,color=black,very thick] (2,-1) -- (2,2);
            \draw [-stealth,color=black,very thick] (0,0) -- (4,0);
            \draw [name path = neg_up, color = myblue, very thick] (0,2) -- (2,0);
            \draw [name path = pos_up, color = myred, very thick] (2,0) -- (4,2);
            \draw [name path = pos_down, color = myblue, very thick, densely dashed] (2.8,-0.8) -- (2,0);
            \draw [name path = neg_down, color = myred, very thick, densely dashed] (2,0) -- (1.2,-0.8);
            \filldraw [color=black] (2,0) circle (2pt);
            \draw [color = black, very thick, densely dashed] (0.5,-0.25) -- (3.5,0.25);
            \draw [color = black, very thick, densely dashed] (0.5,0.75) -- (3.5,-0.75);
            \tikzfillbetween[of=neg_up and neg_down, on layer=bg]{black!20!white};
            \tikzfillbetween[of=pos_up and pos_down, on layer=bg]{black!20!white};
        \end{tikzpicture}

        L1-norm (1-dimension)
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
    For each component of the vector of the L-1 norm $f(\beta_j) = \lvert \beta_j \rvert$, we have:
    \begin{itemize}
        \item[-] \textcolor{myred}{$\beta_j>0$: $f'(\beta_j) = 1$}
        \item[-] \textcolor{myblue}{$\beta_j<0$: $f'(\beta_j) = -1$}
        \item[-] $\beta_j=0$: $\mathrm{d}f \in [-1,1]$ (shaded area)
    \end{itemize}
    which gives the results stated above.
\end{minipage}

Take another look at this result 
$$
\frac{1}{n} \mathbf{X}'  \left(\mathbf{y}-\mathbf{X}\boldsymbol{\beta} \right) =\frac{1}{n} \mathbf{X}'\boldsymbol{\epsilon}  = \lambda \begin{cases}
    \mathrm{sign}\left(\beta_j\right), & \beta_j\neq 0\\
    \left[ -1,1 \right], & \beta_j =0
   \end{cases}
$$
which gives the parameter selection criterion: for $\beta_j \neq 0$, $\mathrm{sign}(\beta_j)$ \myhl[myred]{\textbf{must agree}} with, $\mathrm{Corr}(X_j,\boldsymbol{\epsilon})$ , the correlation between the $j$-th variable $\mathbf{X}_j$ and (full-model) residuals $\boldsymbol{\epsilon}=\mathbf{y}-\mathbf{X}\boldsymbol{\beta}$.


\section{Penalized Least Square Estimation}
Lasso is one special class of Penalized Least Square (PLS) Estimation. For the linear regression model $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$, if $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\sigma^2 \mathbf{I}_n)$, we have PLS as
$$
\min_{\boldsymbol{\beta}\in \mathbb{R}^p}\left\{ \frac{1}{2n} \lVert \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \rVert^2_2 + \sum^p_{j=1}p_{\lambda}\left( \lvert \beta_j \rvert \right) \right\}
$$
where $p_{\lambda}(\cdot)$ is a penalty function indexed by the regularization parameter $\lambda \geq 0$. \citet{antoniadis2001regularization} showed that the PLS estimator $\hat{\boldsymbol{\beta}}$ has the following properties:
\begin{itemize}
    \item \textbf{\underline{sparsity}}: if $\min_{t\geq 0}\left\{ t+p'_{\lambda}(t) \right\}>0$
    \item \textbf{\underline{approximate unbiasedness}}: if $p'_{\lambda}(t)=0$ for $t$ large enough
    \item \textbf{\underline{continuity}}: iff $\arg \min_{t\geq 0}\left\{ t+p'_{\lambda}(t) \right\}=0$
\end{itemize}


\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
      \begin{tikzpicture}[scale=1.6]
          % basics
          \draw [-stealth,color=black,thick] (0,2) -- (4,2);
          \draw [-stealth,color=black,thick] (2,0) node[below] {Lasso} -- (2,4);
          \draw [name path=lup,color=black,thick] (1,2) -- (2,3) -- (3,2);
          \draw [name path=ldown,color=black,thick]  (3,2) -- (2,1) -- (1,2);
          \draw [name path=lmid, color=black, thick] (1,2) -- (3,2);
          \tikzfillbetween[of=lup and ldown, on layer=main]{myblue};
          \draw [rotate=45, thick, color=myred] (5.05,0.7) ellipse (1.5cm and 0.5cm);
          \draw [rotate=45, thick, color=myred] (5.05,0.7) ellipse (1.2cm and 0.4cm);
          \draw [rotate=45, thick, color=myred] (5.05,0.7) ellipse (0.75cm and 0.25cm);
          \filldraw [color=myred] (3.05,4.05) circle (1pt) node[right] {$\hat{\boldsymbol{\beta}}$};
          \filldraw [color=myred] (2,3) circle (2pt);
      \end{tikzpicture}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \begin{tikzpicture}[scale=1.6]
            % basics
            \draw [-stealth,color=black,thick] (0,2) -- (4,2);
            \draw [-stealth,color=black,thick] (2,0) node[below] {Ridge} -- (2,4);
            \draw [color=black,thick] (2,2) -- (2,1);
            %\draw [name path=ldown,color=black,thick]  (3,2) -- (2,1) node[right,xshift=2pt, yshift=-2pt] {Lasso} -- (1,2);
            %\draw [name path=lmid, color=black, thick] (1,2) -- (3,2);
            \node[circle,draw,minimum size=3.2cm, fill =myblue] (c) at (2,2){};
            \draw [rotate=45, thick, color=myred] (5.1,0.72) ellipse (1.5cm and 0.5cm);
            \draw [rotate=45, thick, color=myred] (5.1,0.72) ellipse (1.2cm and 0.4cm);
            \draw [rotate=45, thick, color=myred] (5.1,0.72) ellipse (0.75cm and 0.25cm);
            \filldraw [color=myred] (3.05,4.05) circle (1pt) node[right] {$\hat{\boldsymbol{\beta}}$};
            \filldraw [color=myred] (2.14,2.97) circle (2pt);
        \end{tikzpicture}
    \end{minipage}
  \end{figure}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}