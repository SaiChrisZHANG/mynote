\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{6}{DID and TWFE}{}{Sai Zhang}{This note is on the causal panel data, building upon \citet{arkhangelsky2023causal}.}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Panel Data Configurations}

\subsection{Data Types}
\subsubsection{Panel Data}
For observations on $N$ units, indexed by $i=1,\cdots,N$, over $T$ periods, indexed by $t=1,\cdots,T$, the outcome of interest is denoted by $Y_{it}$, the treatment $W_{it}$.
These observations may themselves consist of averages over more basic units:
\begin{align*}
    \mathbf{Y} &= \begin{pmatrix}
        Y_{11} & \cdots & Y_{1T}\\
        \vdots & \ddots & \vdots \\
        Y_{N1} & \cdots & Y_{NT}
    \end{pmatrix} &
    \mathbf{W} = \begin{pmatrix}
        W_{11} & \cdots & W_{1T}\\
        \vdots & \ddots & \vdots \\
        W_{N1} & \cdots & W_{NT}
    \end{pmatrix}
\end{align*}
we may also observe exogenous variables $X_{it}$ or $X_i$. Typically, we focus on a balanced panel where for all units $i=1,\cdots,N$ we observe outcomes for all $t=1,\cdots,T$.

\subsubsection{Grouped Repeated Cross-Section Data}
In a GRCS data, we have observations on $N$ units, each observed only once in period $T_i$ for unit $i$. Different units may be observed at diffrent points in time, $T_i$ typically takes on only a few values, with many units sharing the same value for $T_i$. The outcome $Y_i$ and treatment $W_i$ are indexed by the unit index $i$.
The set of units is \textbf{partitioned} into 2 or more groups, with the group that unit $i$ belongs to denoted by $G_i\in \mathcal{G}=\left\{1,2,\cdots,G\right\}$.

Define the average outcomes for each group-time-period pair:
\begin{equation*}
    \bar{Y}_{gt} \equiv \frac{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}Y_i}{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}}
\end{equation*}
for treatment 
\begin{equation*}
    \bar{W}_{gt} \equiv \frac{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}W_i}{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}}
\end{equation*}
then treat the $G\times T$ group averages $\bar{Y}_{gt}$ and $\bar{W}_{gt}$ as the unit of observation, then the grouped data is just a panel.
The major issue in practice is that the number of groups is very small comparing to proper panel data.

\subsubsection{Row and Column Exchangeable Data}
The data are doubly indexed by $i=1,\cdots,N$ and $j=1,\cdots,J$, with outcomes $Y_{ij}$. They are different from panel data in that there is \textbf{no time ordering} for the second index. Many methods developed for panel data are also applicable here.

\subsection{Shapes of Data Frames}
Panel data can also be loosely classified by the shape:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Thin Frames} $(N\gg T)$}, where the number of cross-section units is large relative to the number of time periods:
    \begin{itemize}
        \item unit-specific parameters (individual FEs) \textbf{can not be estimated consistently} due to the short time series
        \item REs might be more suitable since they place a stocahstic structure on the individual components
    \end{itemize}
    \item \myhl[myblue]{\textbf{Fat Frames} $(N\ll T)$}, where the number of cross-section units is large relative to the number of time periods.
    \item \myhl[myblue]{\textbf{Square} $N\simeq T$}, where the number of units and time periods is comparable.
\end{itemize}

\subsection{Assignment Mechanisms}
\subsubsection{The General Case}
In the most general case, the treatment may vary both across units and over time, with units \textbf{switching} in and out of the treatment group:
\begin{equation*}
    \mathbf{W}^{\text{general}} = \begin{pmatrix}
        1&1&0&0&\cdots &1\\
        0&0&1&0&\cdots &0\\
        1&0&1&1&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        1&0&1&0&\cdots &0
    \end{pmatrix}
\end{equation*}
This is more relevant for the RCED configurations, and for panel data of products and promotions as treatments.
The assumption on the absence/presence of \textbf{dynamic treatment} effects is very important.

\subsubsection{Single Treated Period}
One special case arises when a substantial number of units is treated, but these units are only treated \textbf{in the last period}
\begin{equation*}
    \mathbf{W}^{\text{last}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&0&\cdots &1
    \end{pmatrix}
\end{equation*}
If $T$ is relatively small, this case is often analyzed as a cross-section problem, the lagged outcomes are used as exogenous covariates or pre-treatment variables to be adjusted.
Here, dynamic effects are not testable, nor do they matter since the shortness of the panel.
\begin{equation*}
    \mathbf{W}^{\text{last}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &0
    \end{pmatrix}
\end{equation*}
this setting is prominent in the original applications of the synthetic control literature, here $T$ is usually small.

\subsubsection{Single Treated Unit and Single Treated Period}
An extreme case is where only a single unit is treated, and it is only treated in a single period (typically the last). 
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&0&\cdots &1
    \end{pmatrix}
\end{equation*}
Normally, we focus on the effect for the single treated/time-period pair and construct prediction intervals.

\subsubsection{Block Assignment}
The case of block assignment is where a subset of units is treated every period after a common starting date:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&1&1&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &1
    \end{pmatrix}
\end{equation*}
There is typically a sufficient number of treated unit/time-period pairs to allow for reasonable approximations. The presence of dynamic effects change the interpretation of the average effect of the treated: the average effect for the treated now is an average over short \textbf{and} medium term effects during different periods.

\subsubsection*{Staggered Adoption (a.k.a. absorbing treatment setting)}
The staggered adoption is the case where units adopt the treatment at various period, and remain in the treatment group once they adopt the treatment:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &1\\
        0&0&0&1&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &1
    \end{pmatrix}
\end{equation*}
Here, with some assumptions, we can separate dynamic effects from heterogeneity across calendar time.

\subsubsection{Event Study Designs}
In the event-study design, units are exposed to the treatment in at most one period:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        1&0&0&0&\cdots &0\\
        0&1&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&1&\cdots &0
    \end{pmatrix}
\end{equation*}
There are often dynamic effects of the treatment past hte time of initial treatment, however, the effects might be changing over time.

\subsubsection{Clustered Assignment}
In many applications, units are grouped together in clusters. Units within the same clusters are always assigned to the treatment:
\begin{equation*}
    \mathbf{W}^{\text{cluster}} = \begin{pmatrix}
        &&&&&&&\text{cluster}\\
        0&0&0&0&\cdots &0&0&1\\
        0&0&0&0&\cdots &0&0&1\\
        0&0&0&0&\cdots &0&0&2\\
        0&0&0&0&\cdots &0&0&2\\
        0&0&0&0&\cdots &1&1&3\\
        \vdots&\vdots&\vdots&\vdots&\ddots &\vdots&\vdots&\vdots\\
        0&0&0&1&\cdots &1&1&C\\
        0&0&0&1&\cdots &1&1&C
    \end{pmatrix}
\end{equation*}
Clustering creates complications for inference.

\subsection{Outcomes, Assumptions and Estimands}
For a treatment assignment matrix $\mathbf{W}$, denote:
\begin{itemize}
    \item the full $T-$component column vector of treatment assignments as $$ \underline{\mathbf{w}} \equiv \left(w_1,\cdots,w_T\right)' $$
    \item the $t$-component column vector of treatment assignments \myhl[myblue]{\textbf{up to} time $t$} as $$ \underline{\mathbf{w}}^{t} \equiv \left(w_1,\cdots,w_t\right)' $$ hence $\underline{\mathbf{w}}^T=\underline{\mathbf{w}}$
    \item the row vector of treatment values for unit $i$ as $\underline{\mathbf{W}}_i$
\end{itemize}
Then in general, we can index the potential outcomes for unit $i$ in period $t$ by the full $T-$component vector of assignments $\underline{\mathbf{w}}$
$$Y_{it}\left(\underline{\mathbf{w}}\right)$$
A key underlying assumption is the \myhl[myblue]{\textbf{Stable Unit Treatment Value Assumption (SUTVA)}}, which requires that there is no interference or spillovers between units\footnote{SUTVA can hold on a cluster/group level, where the spillover effects are within clusters/groups.}.

In this setup, there are $2^T$ potential outcomes for each unit and each time period, as a function of multi-valued treatment $\underline{\mathbf{w}}$. Then, define for each $t$-unit treatment effects for each pair of assignment vectors $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$: $$ \tau^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}_{it}\equiv Y_{it}\left(\underline{\mathbf{w}}'\right) - Y_{it}\left(\underline{\mathbf{w}}\right) $$
and the corresponding population average effect $$ \tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}\equiv \mathbb{E}\left[Y_{it}\left(\underline{\mathbf{w}}'\right)-Y_{it}\left(\underline{\mathbf{w}}\right) \right] $$
where the expectation is implicitly assumed to be taken over a \textbf{large} population.

Under completely random assignment, all $\tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}$ are identified, and are \textbf{just-identified}, given sufficient variation in the treatment paths. Dynamic treatment effects can also be identified\footnote{For example, consider that in the 2-period case $$ \tau_2^{(1,1),(0,1)} $$ is the average effect in the second period of being exposed to $(1,1)$, \textit{treated in both period}, rather than $(0,1)$, \textit{treated only in the second period}.}.
However, we have \myhl[myblue]{$2^{T-1}\times \left(2^T-1\right)$} distinct average effects of the form $\tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}$,
in practice, we often need to focus on summary measures of these causal effects, which requires some addition assumptions:

\begin{assumption}{No Anticipation}{no_anticipation}
    The potential outcomes satisfy $$ Y_{it}\left(\underline{\mathbf{w}}\right) = Y_{it}\left(\underline{\mathbf{w}}'\right)$$ for all $i$, and for all combinations of $t$, $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$ such that $\underline{\mathbf{w}}t=\underline{\mathbf{w}}'^{t}$. 
\end{assumption}
This is a testable assumption with experimental data and sufficient variation in treatment paths, by comparing units that have the same treatment path up to and including $t$ and diverge after $t$.
\begin{itemize}
    \item \underline{Units \textbf{are not} active decision-makers}: the assumption can be guaranteed by design (random treatment assignment each period, or staggered adoption with randomly assigned adoption date)
    \item \underline{\textbf{Limited} antici}p\underline{ation}: assuming the treatment can be anticipated for a \textbf{fixed number} of periods, which shifts $\underline{\mathbf{w}}$ by that number of periods.
    \item \underline{Units \textbf{are} active decision-makers}: potential outcomes are functions of $\underline{\mathbf{w}}$ and the distribution of $\underline{\mathbf{w}}$ (experimental design itself):
    \begin{itemize}
        \item one can define potential outcomes for a given randomized experimental design: the beliefs about the future treatment paths are incorporated in the definition of the potential outcomes, the actual values are by construction unknown. This does change the interpretation of the casual effects\footnote{Think about the differences between a surprise deviation from a given policy rule versus the effect of a permanent chagne int he policy rule itself.}.
        \item In obserational studies, one cannot directly control the information about the future treatment paths. In this case, different units need to be gauranteed to face the \textbf{same information environment} for Assumption \ref{assump:no_anticipation} to hold.
    \end{itemize}
\end{itemize}
Under Assumption \ref{assump:no_anticipation}, the total number of potential treatment effects is reduced from $2^{T-1}\times \left(2^T-1\right)$ to $\left(\sum^T_{t=1}2^{t-1}\right)\left(\sum^T_{t=1}2^t-1\right)$. The unit-period specific treatment effects are now of the type 
\begin{equation*}
    \tau^{\underline{\mathbf{w}}^t,\underline{\mathbf{w}}^{t'}}_{it} \equiv Y_{it}\left(\underline{\mathbf{w}}^{t'}\right) - Y_{it}\left(\underline{\mathbf{w}}^t\right)
\end{equation*}
with the potential outcomes for period $t$ indexed by treatments up to period $t$ only. Here, one can still distinguish
\begin{itemize}
    \item static treatment effects: $\tau_{it}^{\left(\underline{\mathbf{w}}^{t-1},0\right),\left(\underline{\mathbf{w}}^{t-1},1\right)}$, which measures the response of current outcome to the current treatment, holding the past ones fixed.
    \item dynamic treatment effects: $\tau_{it}^{\left(\underline{\mathbf{w}}^{t-1},w^t\right),\left(\underline{\mathbf{w}}^{t-1},w^t\right)}$, which does the opposite.
\end{itemize}

\begin{assumption}{No Dynamic/Carry-Over Effects}{no_dynamic}
    The potential outcomes satisfy $$ Y_{it}\left(\underline{\mathbf{w}}\right) = Y_{it}\left(\underline{\mathbf{w}}'\right) $$
    for all $i$ and for all combinations of $t$, $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$ such that $w_{it}=w'_{it}$.
\end{assumption}
This assumption is \textbf{not} guaranteed by randomization. It restricts the treatment effects and the potential outcomes for the \textbf{post}-treatment periods.
It has testable restrictions given the random assignment of the treatment and sufficient variation in the treatment paths. It does \textbf{not} restrict the time path of the potential outcomes in the absence of any treatment $Y_{it}\left(\mathbf{0}\right)$.

This assumption greatly reduce the total number of treatment effects for each unit to $T$:
$$ \tau_{it}\equiv Y_{it}(1) - Y_{it}(0)$$
where $\tau_{it}$ has no superscripts because there are only 2 possible arguments of the potential outcomes $w\in \left\{0,1\right\}$.

\begin{assumption}{Staggered Adoption}{stag_adopt}
    In staggered adoption, $$W_{it}\leq W_{it-1},\ \forall t=2,\cdots,T$$
    define the adoption date $A_i$ as the date of the first treatment, $A_i\equiv T+1-\sum^T_{t=1}W_{it}$ for treated units, and $A_i\equiv \infty$ for never-treated units.
\end{assumption}
Under Assumption \ref{assump:stag_adopt}, the potential outcomes can be written in terms of the adoption date as $Y_{it}(a)$, for $a=1,\cdots,T,\infty$, and the realized outcome as $Y_{it}=Y_{it}\left(A_i\right)$. There are 2 broad classes of settings that are viewed as staggered adoption designs:
\begin{itemize}
    \item interventions adopted and remain in place 
    \item one-time interventions with a long-term, or even permanent, impact (where the post-intervention period effects are dynamic effects)
\end{itemize}
Under Assumption \ref{assump:stag_adopt}, but \textbf{not} Assumption \ref{assump:no_anticipation} and \ref{assump:no_dynamic}, we can write 
$$ \tau_{it}^{a,a'}\equiv Y_{it}\left(a'\right)-Y_{it}(a) $$
with the corresponding population average
$$ \tau_t^{a,a'}\equiv \mathbb{E}\left[ \equiv Y_{it}\left(a'\right)-Y_{it}(a) \right] $$
we can also denote the average for subpopulations conditional on the adoption dates as 
$$ \tau_{t\mid a''}^{a,a'}\equiv \mathbb{E}\left[\equiv Y_{it}\left(a'\right)-Y_{it}(a)\mid A_i=a''\right] $$
which explicitly depends on the details of the assignment process. This estimand is conceptually similar to the average effect on the treated in cross-sectional settings, but with selection operating over both unit and period dimensions.

\subsection{Conventional TWFE and DiD}
\subsubsection{TWFE Characterization}
First, consider a panel setting with no anticipation, no dynamics, and constant treatment effects:
\begin{assumption}{The TWFE Model}{twfe_model}
    The control outcome $Y_{it}(0)$ satisfies $$ Y_{it}(0) = \alpha_i + \beta_t + \epsilon_{it} $$
    The unobserved component $\epsilon_{it}$ is (mean-)independent of the treatment assignment $W_{it}$
\end{assumption}
And 
\begin{assumption}{Constant Static Treatment Effects}{const_stat_treat}
    The potential outcomes satisfy $$ Y_{it}(1) = Y_{it}(0) + \tau \ \ \forall (i,t) $$
\end{assumption}
Under Assumption \ref{assump:twfe_model} and \ref{assump:const_stat_treat}, for the realized $Y_{it}\equiv W_{it}Y_{it}(1) + \left(1-W_{it}\right)Y_{it}(0)$ we have a model 
$$ Y_{it} = \alpha_i + \beta_t +\tau W_{it}+\epsilon_{it} $$
then we can estimate the parameters of this model by least squares
\begin{equation*}
    \left(\hat{\tau}^{TWFE},\hat{\alpha},\hat{\beta}\right) = \arg\min_{\tau,\alpha,\beta} \sum^N_{i=1}\sum^T_{t=1} \left(Y_{it}-\alpha_i-\beta_t -\tau W_{it}\right)^2
\end{equation*}
one restriction on the $\alpha_i$ or $\beta_t$ needs to be imposed to avoid perfect collinearity, but this normalization does not affect the estimation of $\tau$.

Under a block assignment structure, we have $W_{it}=1$ only for a subset of the units\footnote{The \textit{treatment group} with $i\in \mathcal{J}$, where the cardinality for the set $\mathcal{J}$ is $N^{\mathrm{tr}}$ and $N^{\mathrm{co}}\equiv N-N^{\mathrm{tr}}$.}, and those units are treated only during periods $t$ with $t>T_0$.
Define the averages in the four groups as 
\begin{align*}
    \bar{Y}^{\mathrm{tr,post}} &\equiv \frac{\sum_{i\in\mathcal{J}}\sum_{t>T_0}Y_{it}}{N^{\mathrm{tr}}\left(T-T_0\right)} & \bar{Y}^{\mathrm{tr,pre}} &\equiv \frac{\sum_{i\in\mathcal{J}}\sum_{t\leq T_0}Y_{it}}{N^{\mathrm{tr}}T_0} \\
    \bar{Y}^{\mathrm{co,post}} &\equiv \frac{\sum_{i\not\in\mathcal{J}}\sum_{t>T_0}Y_{it}}{N^{\mathrm{co}}\left(T-T_0\right)} & \bar{Y}^{\mathrm{co,pre}} &\equiv \frac{\sum_{i\not\in\mathcal{J}}\sum_{t\leq T_0}Y_{it}}{N^{\mathrm{co}}T_0}
\end{align*}
and then write the estimator for the treatment effect as 
\begin{equation*}
    \hat{\tau}^{TWFE} = \left(\bar{Y}^{\mathrm{tr,post}}-\bar{Y}^{\mathrm{tr,pre}}\right) - \left(\bar{Y}^{\mathrm{co,post}}-\bar{Y}^{\mathrm{co,pre}}\right)
\end{equation*}

\subsubsection{DiD Estimator in the Grouped Repeated Cross-Section Setting}
In GRCS setting, we observe each physical unit only once. With blocked assignment, the notation only has a single index for the unit $i=1,\cdots,N$.
Let $G_i\in\mathcal{G}=\left\{1,\cdots,G\right\}$ denote the cluster or group unit $i$ belongs to, and $T_i\in \left\{1,\cdots,N\right\}$ the time period unit $i$ is observed in.

The set of clusters $\mathcal{G}$ is partitioned into two groups: control group $\mathcal{G}_C$ and treatment group $\mathcal{G}_T$, with cardinality $G_C$ and $G_T$. Only units with $G_i\in \mathcal{G}_T$, indicated by $D_i=\mathbf{1}_{G_i\in\mathcal{G}_T}$, are exposed to the treatment if they are observed after the treatment date $T_0$: $W_i=\mathbf{1}_{G_i\in\mathcal{G}_T,T_i>T_0}$

Assuming that the treatment within group and time period pairs is constant, the cluster/time-period average treatment $\bar{W}_{gt}$ is binary if the original treatment is. Then the DiD estimator is 
\begin{align*}
    \hat{\tau}^{DiD} =& \frac{1}{G_T\left(T-T_0\right)} \sum_{g\in\mathcal{G}_T,t>T_0} \bar{Y}_{gt} - \frac{1}{G_C\left(T-T_0\right)} \sum_{g\in\mathcal{G}_C,t>T_0}\bar{Y}_{gt}\\
    &- \frac{1}{G_TT_0}\sum_{g\in\mathcal{G}_T,t\leq T_0} \bar{Y}_{gt} + \frac{1}{G_CT_0}\sum_{g\in\mathcal{G}_C,t\leq T_0}\bar{Y}_{gt}
\end{align*}
and at the group level, we have a proper panel setup:
\begin{align*}
    \bar{Y}_{gt}(0) &= \alpha_g+\beta_t+\epsilon_{gt} & \bar{Y}_{gt}(1) &\bar{Y}_{gt}(0)+\tau
\end{align*}
and the potential outcomes $\bar{Y}_{gt}(0)$ and $\bar{Y}_{gt}(1)$ should be interpreted as the average of the potential outcomes if all units in a group/time-period pair are exposed to the control treatment.

\subsubsection{Inference}
There are two ways to conduct inference about $\hat{\tau}^{\mathrm{DiD}}$ and $\hat{\tau}^{\mathrm{TWFE}}$:
\begin{itemize}
    \item the assignment process is known: \myhl[myblue]{\textbf{design-based}} or \myhl[myblue]{\textbf{randomization-based}} inference
    \item otherwise: \myhl[myblue]{\textbf{sampling-based}} inference
\end{itemize}

\paragraph*{Design-Based Inference}


\paragraph*{Sampling-Based Inference}
\begin{itemize}
    \item \myhl[myblue]{\textbf{proper panel setting}}: it is often assumed that all units are randomly sampled from a large population and thus \textbf{exchangeable}. Inference about $\hat{\tau}^{\mathrm{TWFE}}$ reduces to joint inference about four means with i.i.d. observations.
    \item \myhl[myblue]{\textbf{GRCS setting}}: one can allow for non-vanishing errors at the group level, but it cannot be done in the two-group case.
\end{itemize}

\subparagraph*{Standard errors}
Regardless of the level of aggregation, inference for TWFE and DiD estimators typically takes into account the correlation in outcomes over time within units in applications with more than two periods. So it is \textbf{NOT} appropriate to use the robust Eicker-Huber-White standard errors. 
Instead, one should use clustered standard errors based on clustering observations by units. It can also be approximated by bootstrapping all observations for each unit.

\subsubsection{The Parallel Trend Assumption}

The \textbf{parallel trend assumption} is the fundamental justification for the DiD estimator. It states that the units who are treated would have followed a path that is parallel to the path followed by the control units on average, in the absence of the treatment.

\paragraph*{Proper panel settings} the assumption is that the expected difference in control outcomes in any period for units who later are exposed to the treatment and units who are always in the control group is \textbf{contstant}:
\begin{assumption}{Parallel Trend Assumption: Proper Panel}{parallel_trend_panel}
    For all $t,t'$,
    \begin{equation*}
        \mathbb{E}\left[Y_{it}\left(0\right)\mid D_i=1\right] - \mathbb{E}\left[Y_{it}\left(0\right)\mid D_i=0\right] = \mathbb{E}\left[Y_{it'}\left(0\right)\mid D_i=1\right] - \mathbb{E}\left[Y_{it'}\left(0\right)\mid D_i=0\right]
    \end{equation*}
    equivalently, we can formulate it in terms of changes over time\footnote{the expected change in control outcomes is the same for those who will eventually be exposed to the treatment and those who will not}:
    \begin{equation*}
        \mathbb{E}\left[Y_{it}(0)-Y_{it'}(0)\mid D_i=1\right] = \mathbb{E}\left[Y_{it}(0)-Y_{it'}(0)\mid D_i=0\right]
    \end{equation*}
    alternatively, postulate a TWFE model for the control outcomes, additionally assuming that the treatment assignment $D_i$ is independent of the vector of residuals $\epsilon_{it},t=1,\cdots,T$, conditional on FEs: 
    \begin{equation*}
        D_i \perp \left(\epsilon_{i1},\cdots,\epsilon_{iT}\right) \mid \alpha_i 
    \end{equation*}
\end{assumption}
From the point of view of the modern casual inference literature, the parallel trend assumption is non-standard in the sense that it combbines restrictions on the potential outcomes with restrictions on the assignment mechanism.

\paragraph*{GRCS settings} Suppose in the population, all groups are (infinitely) large in each period, and we have random samples from these populations for each period. Then the expectations are well defined as population averages.
The parallel trends assumption can the nbe formulated as requiring that the difference in expected control outcomes between two groups remains constant over time: 
\begin{assumption}{Parallel Trend Assumption: Grouped Repeated Cross-Section}{parallel_trend_grcs}
    For all pairs of groups $g,g'$ and for all pairs of time periods $t,t'$, the average difference between the groups remains the same over time, irrespective of their treatment status: 
    \begin{equation*}
        \mathbb{E}\left[Y_{gt}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{g't}(0)\mid D_i=0\right] = \mathbb{E}\left[Y_{gt'}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{g't'}(0)\mid D_i=0\right]
    \end{equation*}
    an alternative formulation is that expected change between periods $t'$ and $t$ is the same for all groups:
    \begin{equation*}
        \mathbb{E}\left[Y_{gt}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{gt'}(0)\mid D_i=1\right] = \mathbb{E}\left[Y_{g't}(0)\mid D_i=0\right] - \mathbb{E}\left[Y_{g't'}(0)\mid D_i=0\right]
    \end{equation*}
\end{assumption}
If $Y_{gt}(0)$ for all $g$ and $t$ are observed, the presence of the two groups and two time periods would be sufficient for the assumption to have testable implications. However, in the 2-group/2-period case, at least one of the four cells is exposed to the treatment, there are no testable restrictions implied by this assumption\footnote{If there are more than 2 periods, or more than 2 groups, there are testable restrictions by the parallel trend assumption.}.

\subsubsection{Pre-treatment Variables}

Time-invariant characteristics of the units in addition to the time path of the outcome are observed, these variables are colinear with the individual fixed effects $\alpha_i$ hence cannot be incorporated simply by adding them to the TWFE specification.
A reason one might want to include these pre-treatment variables is that the parallel trend and constant treatment effect assumptions hold only within subpopulations defined by them.

\paragraph*{Semi-parametric DiD}
\citet{abadie2005semiparametric} proposed a solution based on \textbf{re-weighting} the differences in outcomes by the propensity score for balance. TO estimate the average treatment effect on the treated (ATT):
\begin{equation*}
    ATT\equiv \mathbb{E}\left(\mathbf{y}_{1t}-\mathbf{y}_{0t}\mid \mathbf{d}=1\right)
\end{equation*}
where the 2 potential outcomes $\mathbf{y}_{1t}$ is the value of $\mathbf{y}$ if the participant received the treatment by $t$, $\mathbf{y}_{0t}$ is the value of $\mathbf{y}$ if the participant had not received the treatment by time $t$. $\mathbf{d}$ is an indicator of treatment.

ATT cannot be directly estimated since $\mathbf{y}_{0t}$, the counterfactual, is never observed. For a set of pretreatment characteristics $\mathbf{x}_{b}$, define the probability to be in the treatment group conditional on $\mathbf{x}_b$ as $$ \pi\left(\mathbf{x}_b\right)\equiv \mathbb{P}\left(\mathbf{d}=1\mid \mathbf{x}_b\right) $$
define the change of $\mathbf{y}$ from baseline $b$ to $t$ as $$\Delta \mathbf{y}_t \equiv \mathbf{y}_t - \mathbf{y}_b $$
then
\begin{equation}\label{eq:abadie_att_estimate}
    \mathbb{E}\left\{ \frac{\Delta \mathbf{y}_t}{\mathbb{P}\left(\mathbf{d}=1\right)} \times \frac{\mathbf{d}-\pi\left(\mathbf{x}_b\right)}{1-\pi\left(\mathbf{x}_b\right)} \right\}
\end{equation}
gives an unbiased estimate of the ATT if 
\begin{align*}
    \mathbb{E}\left(\mathbf{y}_{0t}-\mathbf{y}_{0b}\mid \mathbf{d}=1,\mathbf{x}_b\right) &= \mathbb{E}\left(\mathbf{y}_{0t}-\mathbf{y}_{0b}\mid \mathbf{d}=0,\mathbf{x}_b\right)\\
    \mathbb{P}\left(\mathbf{d}=1\right) &>0\\
    \pi\left(\mathbf{x}_b\right) &<1
\end{align*}
This estimator is a weighted average of the difference of trend, $\Delta \mathbf{y}_t$, across treatment groups: it reweights the trend of the untreated based on the propensity score $\pi\left(\mathbf{x}_b\right)$\footnote{$\frac{\pi\left(\mathbf{x}_b\right)}{1-\pi\left(\mathbf{x}_b\right)}$ is an increasing function of $\pi\left(\mathbf{x}_b\right)$, hence untreated participants with a higher propensity score are given a higher weight.}.

\citet{abadie2005semiparametric} suggests to approximate the propensity score $\pi\left(\mathbf{x}_b\right)$ semiparametrically using a polynomial series of the predictors and plug the predicted values into the sample analogue of the ATT estimates \ref{eq:abadie_att_estimate}.
There are two main ways to do the approximation:
\begin{itemize}
    \item \textbf{linear probability model (LPM)}: higher order improves the approximation, but less precise
    \item \textbf{series logit estimator (SLE)}: using a logit specification to constrain the estimated propensity score to vary between 0 and 1
\end{itemize}

consider $\hat{\pi}\left(\mathbf{x}_b\right)$, the approximated propensity score, and $k$, the order of the polynomial function for approximation. Then the \textbf{LPM} approximation is 
\begin{align*}
    \hat{\pi}\left(\mathbf{x}_b\right) = \hat{\gamma}_0 + \hat{\gamma}_1 \times \mathbf{x}_1 + \sum^k_{i=1}\hat{\gamma}_{2i}\times \mathbf{x}^i_2
\end{align*}
where $\mathbf{x}_1$ is a binary variable, $\mathbf{x}_2^i = \prod^i_{j=1}\mathbf{x}_2$, with $\mathbf{x}_2$ being a continuous variable. Then the coefficients $\hat{\gamma}_0,\hat{\gamma}_1,\hat{\gamma}_{21},\cdots,\hat{\gamma}_{2i},\cdots,\hat{\gamma}_{2k}$ are estimated using OLS estimators.

The \textbf{SLE} approximation is 
\begin{equation*}
    \hat{\pi}\left(\mathbf{x}_b\right) = \Lambda\left(\hat{\gamma}_0 + \hat{\gamma}_1\times \mathbf{x}_1 + \sum^K_{k=1}\hat{\gamma}_{2k}\times \mathbf{x}^k_2\right)
\end{equation*}
where $\Lambda(x) = \frac{\exp(x)}{1+\exp(x)}$ is the logistic function. Higher order binary variables are not considered here since $\mathbf{x}_1^k=\mathbf{x}_1$ for any value $k>1$.

\paragraph*{Doubly robust DiD} \citet{sant2020doubly} adjust for time-invariant covariates in a doubly robust way, by combining inverse-propensity score weighting with outcome modeling. 

\paragraph*{Timing varying covariates}
With finite $T$, strictly exogenous time-varying covariates $X_{it}$ can be converted to time invariant $X_i\equiv \left(X_{i1},\cdots,X_{iT}\right)$, in practice, applied researchers only rely on linear specifications with contemporaneous covariates instead.

\citet{sant2020doubly} also assume that covariates and treatment status are stationary as \citet{abadie2005semiparametric}. Let $T_i$ be a dummy variable that takes value one if the observation $i$ is only observed in the post-treatment period, and 0 if only observed in the pre-treatment period. Define $Y_i = T_iY_{i1}+\left(1-T_i\right)Y_{i0}$. Let $n_1$ and $n_0$ be the sample size of the post- and pre-treatment periods such that $n=n_1+n_0$, and let $\lambda = \mathbb{P}\left(T=1\right)\in \left(0,1\right)$:
\begin{assumption}{Main assumptions of \citet{sant2020doubly}}{santanna_zhao}
    Assume that
    \begin{itemize}
        \item[1] the data $\left\{Y_{i0},Y_{i1},D_i,X_i\right\}^n_{i=1}$ are \textbf{i.i.d.}, or the pooled repeated cross-section data $\left\{Y_i,D_i,X_i,T_i\right\}^n_{i=1}$ consisting of i.i.d. draws from the mixture distribution
        \begin{align*}
            \mathbb{P}\left(Y\leq y, D=d, X\leq x, T=t\right) =& t\cdot \lambda \cdot \mathbb{P}\left(Y_1\leq y, D=d, X\leq x \mid T=1\right)\\
            &+ \left(1-t\right)\cdot \left(1-\lambda\right) \mathbb{P}\left(Y_0\leq y, D=d, X\leq x \mid T=0\right)
        \end{align*}
        where $\left(y,d,x,t\right)\in \mathbb{R}\times \left\{0,1\right\}\times \mathbb{R}^k\times \left\{0,1\right\}$, with the joint distribution of $\left(D,X\right)$ invariant to $T$.
        \item[2] \textbf{Conditional Parallel Trend Assumption (PTA)}\footnote{It allows for covariate-specific time trends but not unit specific trends.}: $$ \mathbb{E}\left[Y_1(0)-Y_0(0)\mid D=1,X\right] \overset{a.s.}{=} \mathbb{E}\left[Y_1(0)-Y_0(0)\mid D=1,X\right] $$
        \item[3] $\exists \epsilon>0$, $\mathbb{P}\left(D=1\right)>\epsilon$ and $\mathbb{P}\left(D=1\mid X\right)\leq 1-\epsilon$ a.s.\footnote{This overlapping condition states that at least a small fraction of the population is treated and that for every value of $X$, at least a small probability that the unit is not treated.}
    \end{itemize}
\end{assumption}
Under Assumption \ref{assump:santanna_zhao}, there are 2 main flexible estimation procedures to estimate the ATT:
\begin{itemize}
    \item[1] outcome regression (\textbf{OR}) approach
    \begin{equation*}
        \hat{\tau}^{\mathrm{reg}} = \bar{Y}_{1,1}-\left[ \bar{Y}_{1,0} + n^{-1}_{\mathrm{treat}} \sum_{i\mid D_i=1}\left(\hat{\mu}_{0,1}\left(X_i\right) - \hat{\mu}_{0,0}\left(X_i\right) \right) \right]
    \end{equation*}
    where $\bar{Y}_{d,t}= \sum_{i\mid D_i=d,T_i=t}Y_{it}/n_{d,t}$ is the sample average outcome among units in treatment group $d$ and time $t$, $\hat{\mu}_{d,t}(x)$ is an estimator of the unknown $m_{d,t}(x)\equiv \mathbb{E}\left[Y_t\mid D=d,X=x\right]$
    \item[2] inverse propensity weighting (\textbf{IPW}) approach, as in \citet{abadie2005semiparametric}.
\end{itemize}

\citet{sant2020doubly} proposed to combine both the \textbf{OR} and \textbf{IPW} approaches to form the doubly robust (\textbf{DR}) moments/estimands for the ATT.

\subparagraph*{Notation} Let $\pi(X)$ be an arbitrary model for the true, unknown propensity score.
\begin{itemize}
    \item with proper panel data, let $\Delta Y= Y_1-Y_0$ and define $\mu^p_{d,\Delta}(X)\equiv \mu^p_{d,1}(X)-\mu^p_{d,0}(X)$ being a model for the true, unknown outcome regression $m^p_{d,t}(x)\equiv \mathbb{E}\left[Y_t\mid D=d,X=x\right]$, $d,t=0,1$. 
    \item with repeated cross-section data, let $\mu_{d,t}^{rc}(x)$ be an arbitrary model for the true, unknown regression $m^{rc}_{d,t}(x)\equiv \mathbb{E}\left[Y\mid D=d,T=t,X=x\right], d,t=0,1$, $\mu_{d,Y}^{rc}(T,X)\equiv T\cdot \mu^{rc}_{d,1}(X)+(1-T)\cdot \mu^{rc}_{d,0}(X)$, and $\mu^{rc}_{d,\Delta}(X)\equiv \mu^{rc}_{d,1}(X)-\mu^{rc}_{d,0}(X)$.
\end{itemize}

\paragraph*{Estimands} consider
\begin{itemize}
    \item for proper panel data: 
    \begin{equation*}
        \tau^{dr,p} = \mathbb{E}\left[ \left(w^p_1(D)-w^p_0\left(D,X;\pi\right)\right) \left(\Delta Y-\mu^p_{0,\Delta}(X)\right) \right]
    \end{equation*}
    where, for a generic $g$,
    \begin{align*}
        w^p_1(D) &= \frac{D}{\mathbb{E}\left[D\right]} & w^p_0\left(D,X;g\right)&= \frac{g(X)(1-D)}{1-g(X)}\cdot \left(\mathbb{E}\left[\frac{g(X)(1-D)}{1-g(X)}\right]\right)^{-1}
    \end{align*}
    \item for repeated cross-section data, consider 2 different estimands
    \begin{align*}
        \tau_1^{dr,rc} =& \mathbb{E}\left[\left(w^{rc}_1(D,T)-w_0^{rc}\left(D,T,X;\pi\right)\right) \cdot \left(Y-\mu^{rc}_{0,Y}(T,X)\right)\right] \\
        \tau^{dr,rc}_2 =& \tau^{dr,rc}_1 + \left(\mathbb{E}\left[\mu^{rc}_{1,1}(X)-\mu^{rc}_{0,1}(X)\mid D=1\right] -\mathbb{E}\left[\mu^{rc}_{1,1}(X)-\mu^{rc}_{0,1}(X)\mid D=1,T=1\right] \right)\\
        &- \left( \mathbb{E}\left[\mu^{rc}_{1,0}(X)-\mu^{rc}_{0,0}(X)\mid D=1\right] - \mathbb{E}\left[\mu^{rc}_{1,0}(X)-\mu^{rc}_{0,0}(X)\mid D=1,T=0\right] \right)
    \end{align*}
    where for a generic $g$,
    \begin{align*}
        w^{rc}_1(D,T)&=w^{rc}_{1,1}(D,T)-w^{rc}_{1,0}(D,T) & w^{rc}_0\left(D,T,X;g\right) &=w^{rc}_{0,1}\left(D,T,X;g\right) - w^{rc}_{0,0}\left(D,T,X;g\right) 
    \end{align*}
    and for $t=0,1$
    \begin{align*}
        w^{rc}_{1,t}\left(D,T\right) &= \frac{D\cdot 1\left\{T=t\right\}}{\mathbb{E} \left[D\cdot 1\left\{T=t\right\}\right]}\\
        w^{rc}_{0,t}\left(D,T,X;g\right) &= \frac{g(X)\left(1-D\right)\cdot 1\left\{T=t\right\}}{1-g(X)} \left(\mathbb{E}\left[\frac{g(X)\left(1-D\right)\cdot 1\left\{T=t\right\}}{1-g(X)}\right]\right)^{-1}
    \end{align*}
\end{itemize}
Then if at least 
\begin{itemize}
    \item for \textbf{panel} data, either $\pi(X)\overset{a.s.}{=} p(X)$ or $\mu^p_{\Delta}(X) \overset{a.s.}{=} m^p_{0,1}(X)-m^p_{0,0}(X)$
    \item for \textbf{repeated cross-section} data, either $\pi(X)\overset{a.s.}{=} p(X)$ or $\mu^p_{0,\Delta}(X) \overset{a.s.}{=} m^{rc}_{0,1}(X)-m^{rc}_{0,0}(X)$\footnote{For repeated cross-section data, $\tau^{dr,rc}_1$ does not reply on OR models for the treated group but $\tau_2^{dr,rc}$ does, however, $\tau^{dr,rc}_1$ is not more robust against model misspecification than $\tau_2^{dr,rc}$ since they identify the ATT under the same conditions. Given that $\mathbb{E}\left[g(X)\mid D=1\right] = \mathbb{E}\left[g(X)\mid D=1,T=t\right],t=0,1$ holds for any $g(\cdot)$, it must hold for $\mu^{rc}_{1,t}(\cdot)-\mu^{rc}_{0,t}(\cdot),t=0,1$, even when $\mu^{rc}_{d,t}(\cdot)$ are misspecified.} 
\end{itemize}
that is, at least one of the working nuisance models is correctly specified, the ATT can be estimated. This is less demanding than both OR and IPW approach.

\paragraph*{Semiparametric efficiency bound} 
Let $m^p_{0,\Delta} \equiv m^p_{0,1}(x)-m^p_{0,0}(x)$ and $m^{rc}_{d,\Delta}(X)\equiv m^{rc}_{0,1}(X)-m^{rc}_{0,0}(X)$ for $d=0,1$. Then 
\begin{itemize}
    \item for \textbf{panel} data, the \textbf{efficient influence function} for the ATT is
    \begin{align*}
        \eta^{e,p}\left(Y_1,Y_0,D,X\right) =& w^p_1(D)\left(m^p_{1,\Delta}(X) - m^p_{0,\Delta}(X) -\tau \right) \\
        &+ w_1^p(D)\left(\Delta Y-m^p_{1,\Delta}(X)\right) - w^p_0(D,X;p)\left(\Delta Y-m^p_{0,\Delta}(X)\right)
    \end{align*}
    and the \textbf{semiparametric efficiency bound} for all regular ATT estimators is
    \begin{align*}
        \mathbb{E}\left[\eta^{e,p}\left(Y_1,Y_0,D,X\right)\right]^2 =& \frac{1}{\mathbb{E}\left[D\right]^2} \left[ D\left(m^p_{1,\Delta}(X)-m^p_{0,\Delta}(X)-\tau\right)^2 \right. \\
        & \left. + D\left(\Delta-m^p_{1,\Delta}(X)\right)^2 + \frac{(1-D)p(X)^2}{\left(1-p(X)\right)^2} \left(\Delta Y-m^p_{0,\Delta}(X)\right)^2 \right]
    \end{align*}
    \item for \textbf{repeated cross-section} data, the \textbf{efficient influence function} for the ATT is
    \begin{align*}
        \eta^{e,rc}\left(Y,D,T,X\right) =& \frac{D}{\mathbb{E}[D]}\left(m^{rc}_{1,\Delta}(X)-m^{rc}_{0,\Delta}(X)-\tau\right) \\
        &+\left( w^{rc}_{1,1}(D,T)\left(Y-m^{rc}_{1,1}(X)\right)-w^{rc}_{1,0}(D,T)\left(Y-m^{rc}_{1,0}(X)\right) \right)\\
        &- \left(w^{rc}_{0,1}(D,T,X;p)\left(Y-m^{rc}_{0,1}(X)\right) - w^{rc}_{0,0}(D,T,X;p)\left(Y-m^{rc}_{0,0}(X)\right) \right)
    \end{align*}
    and the \textbf{semiparametric efficiency bound} for all regular ATT estimators is
    \begin{align*}
        \mathbb{E}\left[\eta^{e,rc}\left(Y,D,T,X\right)^2\right] = \frac{1}{\mathbb{E}[D]^2} \mathbb{E}& \left[ D \left(m^{rc}_{1,\Delta}(X)-m^{rc}_{0,\Delta}(X)-\tau\right)^2 \right. \\
        & + \frac{DT}{\lambda^2}\left(Y-m^{rc}_{1,1}(X)\right)^2 + \frac{D(1-T)}{(1-\lambda)^2}\left(Y-m^{rc}_{1,0}(X)\right)^2 \\
        &+ \left. \frac{(1-D)p(X)^2T}{\left(1-p(X)\right)^2\lambda^2}\left(Y-m^{rc}_{0,1}(X)\right)^2 + \frac{(1-D)p(X)^2 (1-T)}{\left(1-p(X)\right)^2(1-\lambda)^2}\left(Y-m^{rc}_{0,0}(X)\right)^2 \right]
    \end{align*}
\end{itemize}

Both $\eta^{e,p}$ and $\eta^{e,rc}$ depends on the true, unknown, outcome regression functions for the treated group, $m_{1,1}(\cdot)$ and $m_{1,0}(\cdot)$ in an asymmetric manner.

The key difference between the two estimators is that for panel data, 
\begin{align*}
    \eta^{e,p}\left(Y_1,Y_0,D,X\right) =& w^p_1(D)\left(m^p_{1,\Delta}(X) - m^p_{0,\Delta}(X) -\tau \right) \\
    &+ w_1^p(D)\left(\Delta Y-m^p_{1,\Delta}(X)\right) - w^p_0(D,X;p)\left(\Delta Y-m^p_{0,\Delta}(X)\right) \\
    =& \left[w^p_1(D) - w^p_0(D,X;p)\right] Y \left[\Delta Y - m_{0,\Delta}(X)\right] - w_1^p(D)\cdot\tau 
\end{align*}
which ends up \textbf{not} depending are $m_{1,1}(\cdot)$ or $m_{1,0}(\cdot)$. 

Comparing the efficiency bound of the two cases, we have, if $T$ is independent of $\left(Y_1,Y_0,D,X\right)$,
\begin{align*}
    &\mathbb{E}\left[\eta^{e,rc}\left(Y,D,T,X\right)^2\right] - \mathbb{E}\left[\eta^{e,p}\left(Y_1,Y_0,D,X\right)^2\right]\\
    =& \frac{1}{\mathbb{E}\left[D\right]^2} \left[ D\left(\sqrt{\frac{1-\lambda}{\lambda}}\left(Y_1-m_{1,1}(X)\right) + \sqrt{\frac{\lambda}{1-\lambda}}\left(Y_0-m_{1,0}(X)\right)\right)^2 \right.\\
    & \left. + \frac{(1-D)P(X)^2}{\left(1-p(X)\right)^2} \left(\sqrt{\frac{1-\lambda}{\lambda}}\left(Y_1-m_{0,1}(X)\right)+\sqrt{\frac{\lambda}{1-\lambda}}\left(Y_0-m_{0,0}(X)\right)  \right)^2 \right] \geq 0
\end{align*}
which gives that under the DiD framework, it is possible to form more efficient estimators for the ATT when the panel data are available.

This result also gives an efficiency-loss-minimizing $\lambda$:
\begin{align*}
    \lambda &= \frac{\tilde{\sigma}_1}{\tilde{\sigma}_0+ \tilde{\sigma}_1} & \text{where }\tilde{\sigma}^2_t &=\mathbb{E}\left[D\left(Y_t-m_{1,t}(X)\right)^2 + \frac{(1-D)p(X)^2}{\left(1-p(X)\right)^2}\left(Y_t-m_{0,t}(X)\right)^2 \right], t=0,1
\end{align*}
hence, in principle, one may benefit from \textit{oversampling} from either the pre- or post-treamtent period. But it's generally infeasible to do so during the design stage since $\tilde{\sigma}_1^2$ depends on post-treatment data. \citet{sant2020doubly} recommand $\lambda=0.5$ for DiD with repeated cross-section units as a reasonable choice.

\paragraph*{Estimation and inference} 
\citet{sant2020doubly} proposed a two-step procedure for estimation:
\begin{itemize}
    \item first, estimate the true, unknown $p(\cdot)$ with $\pi(\cdot)$, the true unknown $m^p_{d,t}(\cdot)$ and $m^{rc}_{d,t}(\cdot)$ with $\mu^p_{d,t}(\cdot)$ and $\mu^{rc}_{d,t}(\cdot)$, $d,t=0,1$
    \item second, plug the fitted values of the estimated propensity score and regression models into the sample analogue of $\tau^{dr,p}$, $\tau^{dr,rc}_1$, $\tau^{dr,rc}_2$
\end{itemize}
instead of using semi-parametric estimators as \citet{abadie2005semiparametric}, \citet{sant2020doubly} use generic parametric estimators for the first step, assuming:
\begin{itemize}
    \item $\pi\left(x;\gamma^*\right)$ is a parametric model for $p(x)$ s.t. $\pi(\cdot)$ is known up to the \textbf{finite} dimensional pseudo-true $\gamma^*$
    \item for $d,t=0,1$, $\mu^p_{d,t}\left(x;\beta^{*,p}_{d,t}\right)$ and $\mu^{rc}_{d,t}\left(x;\beta^{*,rc}_{d,t}\right)$ s.t. they are known up to the finite dimensional pseudo-true parameter $\beta^{*,p}_{d,t}$ and $\beta^{*,rc}_{d,t}$
\end{itemize}
this approach is most suitable when the sample size is moderate and the dimension of covariates is high. The estimations are 
\begin{itemize}
    \item for \textbf{panel data}
    \begin{equation*}
        \hat{\tau}^{dr,p} = \mathbb{E}_n\left[\left(\hat{w}^p_1(D)-\hat{w}^p_0\left(D,X;\hat{\gamma}\right)\right) \left(\Delta Y- \mu^p_{0,\Delta}\left(X;\hat{\beta}^p_{0,0},\hat{\beta}^p_{0,1}\right)\right) \right]
    \end{equation*}
    where 
    \begin{align*}
        \hat{w}^p_1(D)&=\frac{D}{\mathbb{E}_n[D]} & \hat{w}^p_0\left(D,X;\gamma\right)&= \frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)} \left(\mathbb{E}_n\left[\frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\right]\right)^{-1}
    \end{align*}
    \item for \textbf{repeated cross-section data},
    \begin{align*}
        \hat{\tau}^{dr,rc}_1 =& \mathbb{E}_n \left[ \left(\hat{w}^{rc}_1(D,T) - \hat{w}_0^{rc}(D,T,X;\hat{\gamma}) \right) \left(Y-\mu^{rc}_{0,Y}\left(T,X;\hat{\beta}^{rc}_{0,0},\hat{\beta}^{rc}_{0,1}\right)\right) \right] \\
        \hat{\tau}^{dr,rc}_2 =& \hat{\tau}_1^{dr,rc} + \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,1}(D,T) \right) \left(\mu^{rc}_{1,1}\left(X;\hat{\beta}^{rc}_{1,1}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{rc}_{0,1}\right)\right) \right]\right) \\
        & - \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,0}(D,T) \right) \left(\mu^{rc}_{1,0}\left(X;\hat{\beta}^{rc}_{1,0}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{rc}_{0,0}\right)\right) \right]\right)
    \end{align*}
    where
    \begin{align*}
        \mu^{rc}_{0,Y}\left(Y,X;\beta^{rc}_{0,0},\beta^{rc}_{0,1}\right) &= T\cdot \mu^{rc}_{0,1}\left(\cdot;\beta^{rc}_{0,1}\right)+(1-T)\mu^{rc}_{0,0}\cdot \left(\cdot; \beta^{rc}_{0,0}\right) \\
        \mu^{rc}_{d,\Delta}\left(\cdot;\beta^{rc}_{d,1},\beta^{rc}_{d,0}\right) &= \mu^{rc}_{d,1}\left(\cdot;\beta^{rc}_{d,1}\right) - \mu^{rc}_{d,0}\left(\cdot;\beta^{rc}_{d,0}\right)
    \end{align*}
\end{itemize}
These estimators can be improved to achieve not only \textbf{consistency} doubly robustness, but also \textbf{inference} doubly robustness\footnote{This way, there is no estimation effect from first-step estimators, the asymptotic variance of the results DR DiD estimator for the ATT is invariant to which working modesl for the nuisance functions are correctly specified. This in practice usually translates to simpler and more stable inference procedures.}. 
For the improvement, \citet{sant2020doubly} assume, in addition, 
\begin{itemize}
    \item linear regression working models, for the outcome of interest
    \item a logistic working model, for the propensity score
    \item covariates $X$ entering all nuisance models in a symemtric manner 
\end{itemize}
which are more stringent than the generic DR DiD estimators, but weaker than TWFE estimators. Under such assumptions, we have the improved DR DiD estimators 
\begin{itemize}
    \item for \textbf{panel data}, the 3-step estimator is given as 
    \begin{equation*}
        \hat{\tau}^{dr,p}_{imp} = \mathbb{E}_n\left[ \left(\hat{w}^p_1(D)-\hat{w}^p_0 \left(D,X;\hat{\gamma}^{ipt}\right)\right) \left( \Delta Y-\mu^{lin,p}_{0,\Delta} \left(X;\hat{\beta}^{wls,p}_{0,\Delta}\right) \right) \right]
    \end{equation*}
    the first two steps compute 
    \begin{align*}
        \hat{\gamma}^{ipt} &= \arg\max_{\gamma\in \Gamma}\mathbb{E}_n\left[DX'\gamma -(1-D)\exp(X'\gamma)\right], & \hat{\beta}^{wls,p}_{0,\Delta} & \arg\min_{b\in\Theta}\mathbb{E}_n \left[ \frac{\Lambda\left(X'\hat{\gamma}^{ipt}\right)}{1-\Lambda\left(X'\hat{\gamma}^{ipt}\right)} \left(\Delta Y-X'b\right)^2 \mid D=0 \right]
    \end{align*}
    where $\hat{\gamma}^{ipt}$ is the inverse probability tilting estimator, $\hat{\beta}^{wls,p}_{0,\Delta}$ is the weighted least squares estimator for $\beta^{*,p}_{0,\Delta}$.
    In the last step, plug the fitted value of the working models for the nuisance functions 
    \begin{align*}
        \pi(X,\gamma) &= \Lambda\left(X'\gamma\right) \equiv \frac{\exp(X'\gamma)}{1+\exp (X'\gamma)} & \mu^p_{0,\Delta}\left(X;\beta^p_{0,1},\beta^p_{0,1}\right)&= \mu^{lin,p}_{0,\Delta}\left(X;\beta^p_{0,\Delta}\right)\equiv X'\beta^p_{0,\Delta}
    \end{align*}
    into the sample analogue of $\tau^{dr,p}$. \citet{sant2020doubly} show that if 
    \begin{align*}
        \mathbb{E}\left[ \left(\frac{D}{\mathbb{E}\left[D\right]} - \frac{\exp(X'\gamma^*)(1-D)}{\mathbb{E}\left[\exp(X'\gamma^*)(1-D)\right]}\right) X \right] &= 0 & \mathbb{E}\left[\exp (X'\gamma^*) \left(\Delta Y-\mu^{lin,p}_{0,\Delta}\left(X;\beta^*_{0,\Delta}\right)\right)X \mid D=0 \right] &=0
    \end{align*}
    there will be no estimation effect from the first stage, with the linear outcome models and logistic propensity score models assumed. 
    
    As $n\rightarrow \infty$, these 2 moment conditions follow from the first-order conditions of the optimization problems associated with $\hat{\gamma}^{ipt}$ and $\hat{\beta}^{wls,p}_{0,\Delta}$, even when the working models are misspecified. Hence, replacing the pseudo-true parameters $\gamma^{*,ipt}$ and $\beta^{*,wls,p}_{0,\Delta}$ with their estimators $\hat{\gamma}^{ipt}$ and $\hat{\beta}^{wls,p}_{0,\Delta}$ guarantee that $\hat{\tau}^{dr,p}_{imp}$ is doubly robust:
    $$
    \hat{\tau}^{dr,p}_{imp} \xrightarrow{p} \tau,\ \  \text{ if \textbf{either} $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{or} $X'\beta^{*,wls,p}_{0,\Delta}\overset{a.s.}{=}m^p_{0,\Delta}(X)$}
    $$
    As for inference, $\hat{\tau}^{dr,p}_{imp}$ is $\sqrt{n}-$consistent and asymptotically normal
    \begin{align*}
        \sqrt{n}\left(\hat{\tau}^{dr,p}_{imp}-\tau^{dr,p}_{imp}\right) &= \frac{1}{\sqrt{n}} \sum^n_{i=1}\eta^{dr,p}_{imp} \left(W; \gamma^{*,ipt},\beta^{*,wls,p}_{0,\Delta},\tau^{dr,p}_{imp}\right) + o_p(1) \xrightarrow{d} \mathcal{N}\left(0,V^p_{imp}\right)
    \end{align*}
    and if \textbf{both}  $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{and} $X'\beta^{*,wls,p}_{0,\Delta}\overset{a.s.}{=}m^p_{0,\Delta}(X)$, $V^p_{imp}$ equals to the semiparametrically efficiency bound, and it can be estimated as 
    \begin{equation*}
        \hat{V}^p_{imp} = \mathbb{E}_n\left[\eta^{dr,p}_{imp}\left(W;\hat{\gamma}^{ipt},\hat{\beta}^{wls,p}_{0,\Delta},\hat{\tau}^{dr,p}_{imp}\right)^2\right]
    \end{equation*}
    \item for \textbf{repeated cross-section data}, again assume the working models
    \begin{align*}
        \pi(X,\gamma) &= \Lambda\left(X'\gamma\right) \equiv \frac{\exp(X'\gamma)}{1+\exp (X'\gamma)} & \mu^{rc}_{d,t}\left(X;\beta^{rc}_{d,t}\right)&= \mu^{lin,rc}_{d,t}\left(X;\beta^{rc}_{d,t}\right)\equiv X'\beta^{rc}_{d,t}
    \end{align*}
    then the two improved estimators are given as 
    \begin{align*}
        \hat{\tau}^{dr,rc}_{1,imp} =& \mathbb{E}_n \left[ \left(\hat{w}^{rc}_1(D,T) - \hat{w}_0^{rc}\left(D,T,X;\hat{\gamma}^{ipt}\right) \right) \left(Y-\mu^{lin,rc}_{0,Y}\left(X;\hat{\beta}^{wls,rc}_{0,0},\hat{\beta}^{wls,rc}_{0,1}\right)\right) \right] \\
        \hat{\tau}^{dr,rc}_{2,imp} =& \hat{\tau}_{1,imp}^{dr,rc} + \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,1}(D,T) \right) \left(\mu^{rc}_{1,1}\left(X;\hat{\beta}^{ols,rc}_{1,1}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{wls,rc}_{0,1}\right)\right) \right]\right) \\
        & - \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,0}(D,T) \right) \left(\mu^{rc}_{1,0}\left(X;\hat{\beta}^{ols,rc}_{1,0}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{wls,rc}_{0,0}\right)\right) \right]\right)
    \end{align*}
    where 
    \begin{align*}
        \hat{\gamma} &= \arg\max_{\gamma\in\Gamma} \mathbb{E}_n \left[DX'\gamma - (1-D)\exp(X'\gamma)\right]\\
        \hat{\beta}^{wls,rc}_{0,t} &= \arg\min_{b\in\Theta} \mathbb{E}_n \left[ \frac{\Lambda \left(X'\hat{\gamma}^{ipt}\right)}{1-\Lambda \left(X'\hat{\gamma}^{ipt}\right)} (Y-X'b)^2 \mid D=0,T=t \right]\\
        \hat{\beta}^{ols,rc}_{1,t} &= \arg\min_{b\in\Theta} \mathbb{E}_n \left[ (Y-X'b)^2 \mid D=1, T=t \right]
    \end{align*}
    OLS is adopted to estimate $\beta^{*,rc}_{1,t},t=0,1$ as there is no estimation effect. 
    
    Let 
    \begin{equation*}
        \tau^{dr,rc}_{imp} = \mathbb{E}\left[ \left(w^{rc}_1(D,T) - w^{rc}_0 \left(D,T,X;\gamma^{*,ipt}\right)\right) \left(Y-\mu^{lin,rc}_{0,Y}\left(T,X;\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right)\right) \right]
    \end{equation*}
    and for $\beta^{*,rc}_{imp}=\left(\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0},\beta^{*,ols,rc}_{1,1},\beta^{*,ols,rc}_{1,0}\right)$, define 
    \begin{align*}
        \eta^{dr,rc}_{1,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) &= \eta^{rc,1}_1 \left(W;\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right) -  \eta^{rc,1}_0 \left(W;\gamma^{*,ipt},\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right) \\
        \eta^{dr,rc}_{2,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) &= \eta^{rc,2}_1 \left(W;\beta^{*,rc}_{imp}\right) - \eta^{rc,1}_1 \eta^{rc,2}_0\left(W;\gamma^{*,ipt},\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right)
    \end{align*}
    
    Let $n=n_1+n_0$, where $n_1$ and $n_0$ are the sample sizes of the post- and pre-treatment periods respectively. If $n_1/n\xrightarrow{p}\lambda\in(0,1)$ as $n_0,n_1\rightarrow\infty$, then
    $$
    \hat{\tau}^{dr,rc}_{j,imp} \xrightarrow{p} \tau,\ \  \text{ if \textbf{either} $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{or} $X'\beta^{*,wls,rc}_{0,1}-X'\beta^{*,wls,rc}_{0,0}\overset{a.s.}{=}m^{rc}_{0,\Delta}(X)$}
    $$
    As for inference, $\hat{\tau}^{dr,p}_{imp}$ is $\sqrt{n}-$consistent and asymptotically normal
    \begin{align*}
        \sqrt{n}\left(\hat{\tau}^{dr,rc}_{j,imp}-\tau\right) &= \frac{1}{\sqrt{n}} \sum^n_{i=1}\eta^{dr,rc}_{j,imp} \left(W; \gamma^{*,ipt},\beta^{*,rc}_{imp}\right) + o_p(1) \xrightarrow{d} \mathcal{N}\left(0,V^{rc}_{j,imp}\right)
    \end{align*}
    and if \textbf{both}  $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{and} $X'\beta^{*,wls,rc}_{d,t}\overset{a.s.}{=}m^{rc}_{d,t}(X)$, $\eta^{dr,rc}_{2,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) \overset{a.s.}{=} \eta^{e,rc}\left(Y,D,T,X\right)$, $V^{rc}_{2,imp}$ equals to the semiparametrically efficiency bound, $V^{rc}_{1,imp}$ does \textbf{not}, where
    \begin{equation*}
        {V}^{rc}_{j,imp} = \mathbb{E}_n\left[\eta^{dr,rc}_{j,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right)^2\right]
    \end{equation*}
    and the efficiency loss of using $\hat{\tau}^{dr,rc}_{1,imp}$ instaed of $\hat{\tau}^{dr,rc}_{2,imp}$ is 
    \begin{align*}
        V^{rc}_{1,imp} - V^{rc}_{2,imp} = \mathbb{E} [D]^{-1} \cdot \mathrm{Var} \left[ \sqrt{\frac{1-\lambda}{\lambda}} \left(m^{rc}_{1,1}(X)-m^{rc}_{0,1}(X)\right) + \sqrt{\frac{\lambda}{1-\lambda}} \left(m^{rc}_{1,0}(X)-m^{rc}_{0,0}(X)\right) \mid D=1\right]\geq 0
    \end{align*}
\end{itemize}

\subsubsection{Unconfoundedness}

Viewing the pre-treatment outcomes as covariates, then one can assume \myhl[myblue]{\textbf{unconfoundedness}}:
\begin{equation*}
    D_i \bot \left(Y_{iT}(0),Y_{iT}(1)\right) \mid Y_{i1},\cdots,Y_{iT-1}
\end{equation*}

under this assumption, one can apply the large literature of treatment effect estimation under unconfoundedness \citep{imbens2004nonparametric} or modern approaches \citep{bang2005doubly,chernozhukov2017double,athey2018approximate}.

\citet{imbens2004nonparametric} pointed out 3 arguments for the assumption of unconfoundedness
\begin{itemize}
    \item statistical motivation: the unconfoundedness assumption is logically nature in program evaluation 
    \item purpose: the unconfoundedness assumption asserts that all variables that need to be adjusted for are observed by the researcher
    \item even when agents choose their treatment optimally, two agents with the same values for observed characteristics may differ in their treatment choices without invalidating the unconfoundedness assumption if the difference in their choices is driven by differences in unobserved characteristics that are themselves unrelated to the outcomes of interest.
\end{itemize}

\citet{imbens2004nonparametric} proposes that is is sufficient to assume a weaker version of unconfoundedness, \textbf{mean independence} $$ \mathbb{E}\left[Y_{iT}(0),Y_{iT}(1) \mid D_i, Y_{i1},\cdots,Y_{iT-1}\right] = \mathbb{E}\left[ Y_{iT}(0),Y_{iT}(1) \mid Y_{i1},\cdots,Y_{iT-1} \right] $$
for population ATE. 

Denote 
$$\mu_d = \mathbb{E}_d(Y_{i1},\cdots,Y_{iT-1}) = \mathbb{E}\left[Y\mid D_i=d,Y_{i1},\cdots,Y_{iT-1} \right]$$
for $d=0,1$, \citet{imbens2004nonparametric} reviews 5 groups of estimation for ATEs under unconfoundedness, 

\paragraph*{A. Regression for population/sample/conditional ATE}
we have the estimand
\begin{equation*}
    \hat{\tau}_{reg} = \frac{1}{N} \sum^N_{i=1}\left(\hat{\mu}_1-\hat{\mu}_0\right) = \frac{1}{N}\sum^N_{i=1}D_i\cdot\left(Y_i - \hat{\mu}_0\right) + (1-D_i)\cdot\left(\hat{\mu}_1-Y_i\right)
\end{equation*}
\begin{itemize}
    \item \textbf{early estimators} for $\mu_d$ included parametric regression functions including least-square estimators with the regression function $\mu_d = \beta x+\tau\cdot d$, then one can use the regression $$ Y_i =\alpha + \beta'\left(Y_{i1},\cdots,Y_{iT-1}\right) + \tau W_i +\epsilon_i $$
    more generally, one can specify separate regressions for the two regimes $\mu_d = \beta_d x$, and estimate the two regressions separately.

    \underline{\textit{cons}}: the regression estimators may rely heavily on extrapolation, hence sensitive to changes in the specification of the models.
    \item \textbf{non-parametric estimators}
    \begin{itemize}
        \item \citet{hahn1998role} proposes to estimate first the 3 conditional expectations
        \begin{align*}
            g_1(x) &= \mathbb{E}\left[DY_t\mid Y_{1},\cdots,Y_{T-1}\right] & g_0(x) &= \mathbb{E}\left[(1-D)Y_t\mid Y_{1},\cdots,Y_{T-1}\right] & e(x)&= \mathbb{E}\left[D\mid Y_{1},\cdots,Y_{T-1}\right]
        \end{align*}
        nonparametrically using series methods, then estimate\footnote{For simplicity, let $X_i \equiv \left(Y_{i1},\cdots,Y_{iT-1}\right)$.}
        \begin{align*}
            \hat{\mu}_1(x) &= \frac{\hat{g}_1(x)}{\hat{e}(x)} & \hat{\mu}_0(x) &= \frac{\hat{g}_0(x)}{1-\hat{e}(x)}
        \end{align*}
        and show that the estimators for population ATE and ATT achieve the semiparametric efficiency bounds. Alternatively, one can use this series approach to directly estimate $\mu_d$.
        \item \citet{heckman1998characterizing,heckman1998matching} propose (local-linear) kernel methods, one simple form is 
        \begin{equation*}
            \hat{\mu}_d = \sum_{i:D_i=d} Y_{iT}\cdot K\left(\frac{X_i-x}{h}\right)/\sum_{i:D_i=d}K\left(\frac{X_i-x}{h}\right)
        \end{equation*}
        with kernel $K(\cdot)$ and bandwidth $h$. In the local linear kernel regression, $\mu_d$ is estimated as the intercept $\beta_0$ in the minimization problem 
        \begin{equation*}
            \min_{\beta_0,\beta_1} \sum_{i:D_i=d} \left[Y_i-\beta_0-\beta_1'(X_i-x)\right]^2 \cdot K\left(\frac{X_i-x}{h}\right)
        \end{equation*}
        for bias control, the order of the kernel should be at least as large as the dimension of the covariates: $\int_z z^{r}K(z)\mathrm{d}z=0$, for $r\leq \mathrm{dim}(X)$.
    \end{itemize}
\end{itemize}
for the population ATT, it is important to note that with the propensity score known, the average $\sum D_iY_i/N_T$ is not efficient for the population expectation $\mathbb{E}\left[Y(1)\mid D=1\right]$. The efficient estimator can instead be obtained by weighting all the estimated treatment effects $\hat{\mu}_1-\hat{\mu}_0$ by the probability of receiving treatment:
\begin{equation*}
    \tilde{\tau}_{reg,T} = \frac{\sum^N_{i=1}e(X_i)\cdot \left[\hat{\mu}_1 - \hat{\mu}_0\right]}{\sum^N_{i=1}e(X_i)}
\end{equation*}
this allows one to exploit the control observations to adjust for imbalances in the sampling of the covariates.

\paragraph*{B. Matching}
similar to nonparametric kernel regression methods, matching estimators also impute the missing potential outcomes, but using \textbf{only} the outcomes of \textbf{nearest neighbors} of the opposite treatment group\footnote{What makes matching estimators more attractive is that the researcher only has to choose the \textbf{number of matches}, instead of smoothing parameters.}. Matching estimators often apply in settings where 
\begin{itemize}
    \item the interest is in the ATT
    \item there is a large reservoir of potential controls
\end{itemize}
the estimator is essentially the difference between 2 sample means, the variance is calculated using standard methods for differences in means or paired randomized experiments. The biggest challenge mostly comes from computation.

\citet{abadie2002simple} propose that: again, for a sample $\left\{ \left(Y_i,X_i,D_i\right) \right\}^N_{i=1}$, let $l_m(i)$ be the index that satisfies $D_l\neq D_i$, and 
    \begin{equation*}
        \sum_{j\mid D_j\neq D_i} \mathbf{1} \left\{ \left\Vert X_j-X_i \right\Vert \leq \left\Vert X_l-X_i \right\Vert \right\} = m
    \end{equation*}
    then $l_m(i)$ is the index of the unit in the opposite treatment group that is the $m^{th}$ closest to unit $i$ based on norm $\left\Vert \cdot \right\Vert$ distance, and $l_1(i)$ is the nearest match. Let the set of indices for the first $M$ matches for unit $i$ be $\mathcal{L}_M(i) = \left\{l_1(i),\cdots,l_M(i)\right\}$, then define the imputed potential outcomes as 
    \begin{align*}
        \hat{Y}_i(0) &= \begin{cases}
            Y_i, & D_i=0\\
            \frac{1}{M} \sum_{j\in\mathcal{L}_M(i)} Y_j, & D_i=1
        \end{cases}
        & \hat{Y}_i(1) &= \begin{cases}
            \frac{1}{M} \sum_{j\in\mathcal{L}_M(i)} Y_j, & D_i=0 \\
            Y_i, & D_i=1
        \end{cases}
    \end{align*}
    the simple matching estimator is 
    \begin{equation*}
        \hat{\tau}^{sm}_M = \frac{1}{N} \sum^N_{i=1} \left[hat{Y}_i(1) - \hat{Y}_i(0)\right]
    \end{equation*}
    the bias of this estimator is $O\left(N^{-1/k}\right)$, where $k$ is the dimension of the covariates. \citet{imbens2004nonparametric} listed 3 caveats to \cite{abadie2002simple}'s result:
    \begin{itemize}
        \item[\textbf{1}] only continuous covariates should be counted in $k$: matching with discrete covariates will be exact in large samples
        \item[\textbf{2}] if only matching the treated and the number of potential controls is much larger, the bias can be ignored asymptotically 
        \item[\textbf{3}] the order of the bias may be high, but the actual bias may be small if the coefficients in the leading term are small\footnote{One such case is that biases for different units are at least partially offsetting.}
    \end{itemize}
    and these matching estimators are generally not efficient.

One key aspect of matching is the choice of distance metrics, one can choose 
\begin{itemize}
    \item standard Euclidean metric: $d_E\left(x,z\right)=\left(x-z\right)'\left(x-z\right)$
    \item the diagonal matrix of the inverse of the covariate variances $d_{AI}\left(x,z\right) = \left(x-z\right)'\mathrm{diag}\left(\Sigma^{-1}_X\right)\left(x-z\right)$
    \item Mahalanobis metric: $d_M(x,z) = \left(x-z\right)' \Sigma^{-1}_X \left(x-z\right)$, which reduces differences in covariates within matched pairs in all directions
    \item metrics depending on the correlation between covariates, treatment assignemnt and outcomes:
    \begin{itemize}
        \item weighting absolute differences by the coefficients in the propensity score $$d_{Z1}\left(x,z\right) = \sum^K_{k=1}\left\vert x_k-z_k \right\vert \cdot \left\vert \gamma_k \right\vert$$ where the propensity score has a logistic form $e(x) = \frac{\exp(x'\gamma)}{1+\exp(x'\gamma)}$
        \item weighting absolute differences by the coefficients in the regression function $$ d_{Z2}\left(x,z\right) = \sum^K_{k=1} \left\vert x_k-z_k \right\vert \left\vert \beta_k \right\vert $$ where the regression functions are linear $\mu_{d}(x) = \alpha_d  +x'\beta $
    \end{itemize}
    \citet{imbens2004nonparametric} comments that when the regression function is misspecified, matching with the particular metrics may lead to inconsistency.
\end{itemize}

\paragraph*{C. Propensity scores}
There are 3 main ways to use propensity scores in estimation:
\begin{itemize}
    \item \myhl[myblue]{\textbf{weighting}}: the units by the reciprocal of the probability of receiving the treatment can undo the imbalance of the covariate distributions conditional on the treatment assignment. Formally,
    \begin{align*}
        \mathbb{E} \left[\frac{DY}{e(X)}\right] = \mathbb{E} \left[\frac{DY(1)}{e(X)}\right] = \underbrace{\mathbb{E}\left[\mathbb{E}\left[ \frac{DY(1)}{e(X)} \mid X \right]\right] = \mathbb{E} \left[ \frac{e(X)\cdot \mathbb{E}\left[Y(1)\mid X\right]}{e(X)} \right]}_{\text{unconfoundedness}} = \mathbb{E}\left[Y(1)\right]
    \end{align*}
    and similarly $\mathbb{E}\left[\frac{(1-D)Y}{1-e(X)}\right]=\mathbb{E}\left[Y(0)\right]$, which imply 
    \begin{equation*}
        \tau^p = \mathbb{E} \left[\frac{D\cdot Y}{e(X)} - \frac{\left(1-D\right)\cdot Y}{1-e(X)}\right]
    \end{equation*} 
    it can be directly estimated as 
    \begin{equation*}
        \tilde{\tau} = \frac{1}{N} \sum^N_{i=1} \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i)\cdot Y_i}{1-e(X_i)} \right]
    \end{equation*}
    here, the weights do not necessarily add to 1\footnote{For the treated units, the weights added up to $\frac{1}{N}\sum^N_{i=1}W_i/e(X_i)$, which equals to 1 in expectation, but not so in any given sample.}. One can normalize the weights within subpopulations, which in the limits leads to the estimator proposed by \citet{hirano2003efficient}
    \begin{equation*}
        \hat{\tau}_{weight} = \frac{\sum^N_{i=1} \frac{D_i\cdot Y_i}{\hat{e}(X_i)}}{\sum^N_{i=1}\frac{D_i}{\hat{e}(X_i)}} - \frac{\sum^N_{i=1} \frac{(1-D_i)\cdot Y_i}{1-\hat{e}(X_i)} }{\sum^N_{i=1} \frac{1-D_i}{1-\hat{e}(X_i)}}
    \end{equation*}
    where they specify a sequence of functions of the covariates, such as power series $h_l(x),l=1,\cdots,\infty$, and choose a number of terms $L(N)$ as a function of the sample size, then esitmate the $L-$dimensional vector $\gamma_L$ in 
    \begin{equation*}
        \Pr \left(D=1\mid X=x\right) = \frac{\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \gamma_L \right]}{1+\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \gamma_L \right]}
    \end{equation*}
    by maximizing the associated likelihood function, and calculate the estimated propensity score as 
    \begin{equation*}
        \hat{e}(x) = \frac{\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \hat{\gamma}_L \right]}{1+\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \hat{\gamma}_L \right]}
    \end{equation*}
    a nonparametric estimator for $e(x)$ is efficient, ignoring the 2 regression functions\footnote{The finite-sample properties of the two approaches (nonparametrically estimating propensity scores versus regression functions) may be different, except for when there are only discrete covariates.}.
    
    For the treatment effects of the treated, weight the contribution for unit $i$ by the propensity score $e(x_i)$,
    \begin{align*}
        \hat{\tau}_{weight,tr} &= \frac{\sum^N_{i=1} D_i\cdot Y_i\cdot \frac{e(X_i)}{\hat{e}(X_i)} }{ \sum^N_{i=1} D_i \frac{e(X_i)}{\hat{e}(X_i)} } - \frac{\sum^N_{i=1} (1-D_i)\cdot Y_i\cdot \frac{e(X_i)}{\left(1-\hat{e}(X_i)\right)} }{ \sum^N_{i=1} (1-D_i) \frac{e(X_i)}{ \left(1-\hat{e}(X_i)\right) } } & \text{propensity scores known} \\
        \hat{\tau}_{weight,tr} &= \left[\frac{1}{N_1} \sum_{D_i=1}Y_i \right] - \left[ \frac{\sum_{i:D_i=0} Y_i \cdot \frac{\hat{e}(X_i)}{\left(1-\hat{e}(X_i)\right)} }{ \sum_{i:D_i=0} \frac{\hat{e}(X_i)}{\left(1-\hat{e}(X_i)\right)} }  \right] & \text{propensity scores unknown}
    \end{align*}

    \underline{\textbf{CAUTIOUS}}: the problem of choosing the smoothing parameters is relevant here too\footnote{\citet*{hirano2003efficient}'s series estimators require choosing the \textbf{number of terms} in the series, a kernel-version alternative requires choosing a bandwidth.}, but here one want to use nonparametric regression methods even if the propensity scores are known.
    
    \item \myhl[myblue]{\textbf{blocking}}: \citet{rosenbaum1983central} suggest using the (estimated) propsensity score divide the sample into $M$ blocks of units of approximately equal probability of treatment, letting $J_{im}$ be an indicator for unit $i$ being in block $m$. One way of implementing this is by dividing the unit interval into $M$ blocks with boundary values equal to $m/M$ for $m=1,\cdots,M-1$, s.t. 
    \begin{equation*}
        J_{im} = \mathbf{1} \left\{ \frac{m-1}{M} < e(X_i) < \leq \frac{m}{M} \right\},\ m=1,\cdots,M 
    \end{equation*}
    within each block there are $N_{dm}$ observations with treatment equal to $d$, $N_{dm}=\sum_i \mathbf{1}\left\{D_i=d, J_{im}=1\right\}$. Within each block, estimate the average treatment effect as if random assignment held:
    \begin{equation*}
        \hat{\tau}_m = \frac{1}{N_{1m}} \sum^N_{i=1} J_{im}W_iY_i - \frac{1}{N_{0m}}\sum^N_{i=1} J_{im}\left(1-D_i\right)Y_i
    \end{equation*}
    then estimate the overall average treatment effect as 
    \begin{equation*}
        \hat{\tau}_{block} = \sum^M_{m=1}\hat{\tau}_m \cdot \frac{N_{1m}+N_{0m}}{N}
    \end{equation*}
    for the treatment effect of the treated, one can weight the within-block average treatment effects by the number of treated units: 
    \begin{equation*}
        \hat{\tau}_{T,block} = \sum^M_{m=1} \hat{\tau}_m \cdot \frac{N_{1m}}{N_T}
    \end{equation*}
    \underline{\textbf{CAUTIOUS}}: The asymptotic properties of such estimators require establishing the relative relationship between the number of blocks and the sample size, so choosing the number of blocks becomes essential 
    \begin{itemize}
        \item \textit{starting point}: a single covariate, assuming normality, \textbf{5 blocks} removes $\geq 95\%$ of the bias
        \item \textit{balance check}: covariates whould be balanced within blocks 
        \item \textit{unbalanced blocks}: if the distributions of the covariates among treated and controled are different, one can 
        \begin{itemize}
            \item split the blocks into a number of subblocks if the propensity score itself is unbalanced.
            \item generalize the specification of the propensity score, if the score is balanced but covariates not
        \end{itemize}
        \item \textit{weighting with modified propensity score estimators}: discretize $\hat{e}(x)$ to 
        \begin{equation*}
            \tilde{e}(x) = \frac{1}{M} \sum^M_{m=1} \sum^M_{m=1} \mathbf{1} \left\{ \frac{m}{M} \leq \hat{e}(x) \right\}
        \end{equation*}
        then use $\tilde{e}(x)$ as the propensity score in the weighting estimator leads to an estimator for the ATE \textbf{identical} to that obtained by using the blocking estimator with $\hat{e}(x)$ as the propensity score and $M$ blocks\footnote{With sufficiently large $M$, the blocking estimator is sufficiently close to the oiriginal weighting estimator, sharing first-order asymptotic properties. Hence a large number of blocks does little harm, with regard to asymptotic properties.}. 
    \end{itemize}
    \item \myhl[myblue]{\textbf{propensity scores as regressors}}: estimate the conditional expectation of $Y$ given $D$ and $e(X)$, define $$ v_d(e) = \mathbb{E}\left[ Y(d) \mid e(X)=e \right] \overset{\text{unconfoundedness}}{=} \mathbb{E}\left[Y \mid D=d, e(X)=e\right] $$
    given an estimator $\hat{v}_w(e)$, one can estimate the ATE as 
    \begin{equation*}
        \hat{\tau}_{regprop} = \frac{1}{N} \sum^N_{i=1} \left[ \hat{v}_1\left(e(X_i)\right) - \hat{v}_0 \left(e(X_i)\right) \right]
    \end{equation*}
\end{itemize}

There are 2 cases in practice when considering propensity score approaches
\begin{itemize}
    \item \textbf{propensity scores known}: all 3 methods are efficient, do not rely on high-dimensional nonparametric regressions, have attractive finite-sample properties
    \item \textbf{propensity scores unknown}: require high-dimensional nonparametric regression of the treatment indicator on the covariates. The relative merits of the 3 approaches will depend on whether the propensity score is more or less smooth than the regression functions, and whether additional information is available about either the propensity score or the regression functions.
\end{itemize}

\paragraph*{D. Mixed} Neither matching nor the propensity score methods directly address the correlation between the covariates and the outcome, incorporating the regression method may eliminate remaining bias and improve precision.

\begin{itemize}
    \item \myhl[myblue]{\textbf{Weighting and Regression}}: estimating $$ Y_i = \alpha + \tau\cdot D_i + \epsilon_i $$ with weights $$ \lambda_i = \sqrt{ \frac{D_i}{e(X_i)} + \frac{1-D_i}{1-e(X_i)} } $$
    the covariates are uncorrelated with the treatment indicator, making the weighted estimator consistent. To improve precision, add covariates 
    $$ Y_i = \alpha + \beta'X_i + \tau\cdot D_i + \epsilon_i $$
    which is doubly robust: consistent if either the regression model or the propensity score are specified correctly.
    \item \myhl[myblue]{\textbf{Blocking and Regression}}: least square estimator in block $m$ as $$ Y_i = \alpha_m + \tau_m\cdot D_i + \epsilon_i $$ using only units in block $m$. One can again add covariates and estimate $Y_i = \alpha_m + \beta_m' X_i + \tau_m\cdot D_i + \epsilon_i$.
    \item \myhl[myblue]{\textbf{Matching and Regression}}: the bias of the simple matching estimator can dominate the variance if the dimension of the covariates is too large, regression can help in this situation.
    
    Let $\hat{Y}_i(0)$ and $\hat{Y}_i(1)$ be the observed or imputed potential outcomes for unit $i$, the \textit{estimated potential} outcomes equal the \textit{observed} outcomes for some unit $i$ for its match $l(i)$, the bias $$ \mathbb{E}\left[\hat{Y}_i(1) - \hat{Y}_i(0)\right] - \left[Y_i(1)-Y_i(0)\right] $$
    arises from the fact that the covariates $X_i$ and $X_[l(i)]$ for units $i$ and $l(i)$ are not equal, although they are close because of the matching process. Focusing on the single-match case, define for unit $i$ 
    \begin{align*}
        \hat{X_i}(0) &= \begin{cases}
            X_i &D_i=0\\
            X_{l_1(i)}&D_1=1
        \end{cases} &
        \hat{X_i}(1) &= \begin{cases}
            X_{l_1(i)} &D_i=0\\
            X_i &D_1=1
        \end{cases}
    \end{align*}
    if the matching is exact, $\hat{X}_i(0)=\hat{X}_i(1)$ for each unit $i$. Suppose $D_1=1$, then $\hat{Y}_i(1)=Y_i(1)$, and $\hat{Y}_i(0)$ is an imputed value for $Y_i(0)$. This value is unbiased for $\mu_0\left(X_{l_1(i)}\right)$, but not necessarily for $\mu_0\left(X_i\right)$. Hence, $\hat{Y}_i(0)$ should be adjusted by an estimate of $\mu_0\left(X_i\right) - \mu_0\left(X_{l_1(i)}\right)$. Typically, these corrections are taken be to linear in the difference in the covariates for unit $i$ and its match, of the form $$\beta_0'\left[\hat{X}_i(1)-\hat{X}_i(0)\right] = \beta'_0\left(X_i - X_{l_1(i)}\right)$$
    then \citet{rubin1973use} proposed 3 modifications
    \begin{itemize}
        \item[1] using least squares, estimate $$ \hat{Y}_i\left(1\right) -\hat{Y}_(0) = \tau+\left[\hat{X}_i(1)-\hat{X}_i(0)\right]'\beta + \epsilon_i $$
        \item[2] estimate $\mu_0(x)$ directly by taking all control units and use lease squares to estimate $$ Y_i = \alpha_0 + \beta_0'X_i + \epsilon_i $$
        if unit $i$ is a control unit, or $Y_i=\alpha_1+\beta_1'X_i+\epsilon_i$ for the treated units\footnote{If the correction is done nonparametrically, the resulting matching estimator is consistent and asymptotically normal, with its bias dominated by the variance.}.
        \item[3] estimate the same regression function for the controls, but using \textbf{only those that are used as matches} for the treated units with weights corresponding to the number of times a control observation is used as a match.
        
        \textbf{\underline{CAUTIOUS}}: This approach can be less efficient due to dropping some control observations, but can likely avoid including outliners. 
    \end{itemize}
\end{itemize}

\paragraph*{E. Bayesian}
Bayesian approaches have not been applied in estimating ATE under unconfoundedness. They can be useful for a number of reasons,
\begin{itemize}
    \item \textbf{high-dimensionality}: Bayesian methods would allow researchers to include covariates with more or less informative prior distributions.
    \item \textbf{missing-at-random (MAR)}: multiple imputation methods often rely on a Bayesian approach for missing data.
\end{itemize}

\paragraph*{Estimating variances}
\begin{itemize}
    \item for population ATE, the variance of efficient estamators is $$ V^P = \mathbb{E} \left[ \frac{\sigma^2_1(X)}{e(X)} + \frac{\sigma^2_0(X)}{1-e(X)} + \left(\mu_1(X)-\mu_0(X)-\tau\right)^2 \right] $$
    which can be estimated by
    \begin{itemize}
        \item[1] brute force: estimate all 5 components $\sigma^2_0(x)$, $\sigma^2_1(x)$, $\mu_0(x)$, $\mu_1(x)$ and $e(x)$ using kernel methods or series.
        \item[2] either estimate regression functions $\mu_0(x),\mu_1(x)$ or the propensity score $e(x)$ using series/sieves. This can be interpreted as parametric estimators
        \item[3] bootstrapping: given the asymptotic linearity of the estimators, bootstrapping will lead to valid standard errors and CIs at least for the regression and propensity score methods. For matching, it's more complicated since bootstrapping introduces discreteness in the distribution, which will lead to ties.
    \end{itemize}
    \item for sample ATE, the appropriate (conservative) variance is $$ V^S = \mathbb{E}\left[\frac{\sigma^2_1(X)}{e(X)} + \frac{\sigma^2_0(X)}{1-e(X)}\right] $$
    which can be estimated by 
    \begin{itemize}
        \item[1] estimating the conditional moments of the outcome distributions, with the accompanying inherent difficulties.
        \item[2] matching variance estimator\footnote{The idea is that one need not actually estimate this variance consistently at all values of the covariates. One needs only the average of this variance over the distribution, weighted by the inverse of either $e(x)$ ot its complement $1-e(x)$.}: the key is to obtain a close-to-unbiased estimator for $\sigma^2_w(x)$. Suppose units $i$ and $j$ have $X=x$, then an unbiased estimator for $\sigma^2_1(x)$ is $$ \hat{\sigma}^2_1(x) = \frac{1}{2}(Y_i-Y_j)^2 $$
        this matching doesn't need to be exact, one can use the closest match within the set of units with the same treatment indicator. Let $v_m(i)$ be the $m$-th closest unit to $i$ with the same treatment indicator $W_{v_m(i)}=W_i$ and 
        $$
        \sum_{l\mid W_l=l,l\neq i} \mathbf{1} \left\{ \left\Vert X_l-x \right\Vert \leq \left\Vert X_{v_m(i)}-x \right\Vert \right\} = m
        $$
        which gives $M$ units with the same treatment indicator and approximately the same values for the covariates. The sample variance of the outcome variable for these $M$ units can then be used to estimate $\sigma^2_1(x)$, and similarly for the control variance function $\sigma^2_0(x)$. 
        \item[3] estimate the variance of the ATE as
        \begin{equation*}
            \hat{V}^S = \frac{1}{N} \sum^N_{i=1}\left( \frac{\hat{\sigma}^2_1(X_i)}{\hat{e}(X_i)} + \frac{\hat{\sigma^2_{0}(X_i)}}{1-\hat{e}(X_i)} \right)
        \end{equation*}
        for matching estimators, even estimation of the propensity score can be avoided:
        \begin{equation*}
            \hat{V}^E = \frac{1}{N}\sum^N_{i=1} \left(1+\frac{K_M(i)}{M}\right)\hat{\sigma}^2_{W_i}(X_i)
        \end{equation*}
        where $M$ is the number of matches and $K_M(i)$ is the number of times unit $i$ is used as a match.
    \end{itemize}
\end{itemize}

\paragraph*{Assessing the assumptions}
There assumption of unconfoundedness is not directly testable, since the distribution of $Y(0)$ for those who received the treatment and of $Y(1)$ for those who received the control are never in the data. But there are still 2 groups of ways to indirectly test the assumption:
\begin{itemize}
    \item estimating the casual effect of a treatment that is known \textbf{not to have} an effect: postulate a three-valued indicator $T_i\in\left\{-1,0,1\right\}$ for the groups of ineligibles, eligible non-participants and participants, and treatment indicator $W_i=\mathbf{1}\left\{T_i=1\right\}$. The extended unconfoundedness assumption is 
    $$ Y_i(0),Y_i(1) \bot T_i\mid X_i $$
    and a testable implication is 
    $$Y_i \bot \mathbf{1}\left\{T_i=0 \right\} \mid X_i,T_i\leq 0 $$
    \item estimating the casual effect of the treatment on a variable known to be \textbf{unaffected} by it, typically a pre-determined variable, which could eitehr me time-invariant, or more interestingly, a lagged outcome. One can test 
    $$ Y_{i,-1}\bot W_i\mid Y_{i,-2},\cdots,Y_{i,-T},Z_i $$
    which combines 2 assumptions\footnote{The test depends on the link between the 2 assumptions and the original unconfoundedness assumption. With a sufficient number of lags, unconfoundedness given all lags but one appears plausible, conditional on unconfoundedness given all lags, so the relevance of the test depends largely on the plausiblity of the stationarity and exchangeability assumption.}
    \begin{align*}
        Y_i(1),Y_i(0)\bot W_i &Y_{i,-1},\cdots,Y_{i,-(T-1)},Z_i &\text{unconfoundedness given only $T-1$ lags}\\
        f_{Y_{i,s(0)\mid Y_{i,s-1}(0)},\cdots,Y_{i,s-(T-1)}(0),Z_i,W_i}&\left(y_s\mid y_{s-1},\cdots, y_{s-(T-1)},z,w\right) &\text{stationarity and exchangeability}
    \end{align*}
\end{itemize}

next, the important question is how to \textbf{choose the covariates}, some concerns are 
\begin{itemize}
    \item some variables should not be adjusted for: the unconfoundedness assumption may apply with one set of covariates, but \textbf{not} with an expanded set. A covariate measured before the treatment was chosen should be safe to include in theory, but in practice, the covariates are often recorded as the same time as the outcomes
    \item the expected mean squared error may be reduced by ignoring those covariates that have only weak correlation with the treatment indicator and the outcomes: including a covariate in teh adjustment procedure will not lower the asymptotically precision of ATE but could reduce precision \textbf{in finite samples} if the covariates are not or only weakly correlated with outcomes and treatment indicators.
\end{itemize}

the second key assumption is the \textbf{overlapping}: the propensity score, i.e., the probability of receiving the active treatment, should be strictly between 0 and 1. In practice, there are 2 main issues
\begin{itemize}
    \item[\textbf{1}] \myhl[myblue]{\textbf{detect} the lack of overlapping}: there are several methods popularly used
    \begin{itemize}
        \item plot distributions of covariates by treatment groups: can be very difficult in \textbf{high-dimensional} cases
        \item non-parametrically estimate the distribution of the propensity score: one may wish to \textbf{undersmooth} the estimation by choosing a bandwidth smaller than optimal or including higher-order terms
        \item inspect the \textbf{worst} matches: for each component $k$ of the covariate vector, check $$\max_i\left\vert x_{i,k} - x_{l_1(i),k} \right\vert$$
        the maximum over all observations of the matching discrepancy. If it's large relative to the \textbf{sample standard deviation} of the $k$-th component of the coviarates, there would be a lack of overlapping.
    \end{itemize}
    \item[\textbf{2}] \myhl[myblue]{\textbf{address} the lack of overlapping}: given a lack of overlap, one can 
    \begin{itemize}
        \item conclude the ATE \textbf{cannot} be estimated with sufficient precision 
        \item focus on an average treatment effect that is estimable with greater accuracy, by \textit{\underline{dropping observations} with a \textbf{cutoff} of propensity scores}: one principle is to evaluate the weight of each unit as
        \begin{align*}
            \frac{1}{N\cdot \left[1-e(X_i)\right]}, &\text{ for treated units} & \frac{1}{N\cdot e(X_i)}, &\text{ for control units}
        \end{align*}
        and make sure the weights of all units are \textbf{upper-bounded} by a reasonable number\footnote{For example, set the cutoff as 0.05, s.t. no unit will have a weight of more than 5\% in the average. In a sample with 1000 units, with such a cutoff, only units with a propensity score outside $[0.02,0.98]$ will be ignored.}.
    \end{itemize}
    \item[\textbf{3}] \myhl[myblue]{\textbf{comparing} the 3 approaches}: in general, variance estimates \textbf{increase} when adding treated observations to a sufficiently-overlapping data set, approximately \textbf{unchanged} when adding control observations.
    \begin{itemize}
        \item \textbf{regression}: in general, adding observations with outlier values of the regressors leads to more precise estimates. If the added observations are \textbf{treated} units, the precision of the estimated control regression at thes outlier values will be lower; if in \textbf{control} units, such precision will increase.
        \item \textbf{matching}: adding \textbf{control} observations with outlier covariate values will likely not change the results, such they won't be useed as matches; but adding \textbf{treated} will bias the estimates, while the standard error would largely unaffected.
        \item \textbf{propensity-score}: if the propensity score of a \textbf{control} unit is \textbf{close to 0}, adding it would not cause much trouble; but adding a \textbf{control} unit with a propensity score \textbf{close to 1} would increase variance of an ATE estimator
    \end{itemize}
    over all, \textbf{matching} and \textbf{propensity-score}, as well as \textbf{kernel-based} regression methods, are better in coping with limited overlap than (semi-)parametric regression models. In practice, one should \textit{always} inspect histograms of the estimated propensity scores in both groups to assess whether limited overlap is an issue.
\end{itemize}

\citet{chernozhukov2017double} propose to a machine-learning based approach. They consider the case where treatment effects are fully heterogeneous, and the treatment variable $D\in \left\{0,1\right\}$, and model random vector $\left(Y,D,Z\right)$ as 
\begin{align*}
    Y&= g_0\left(D,Z\right)+\zeta & \mathbb{E}\left[\zeta \mid Z,D\right]&=0 & \text{outcome variable}\\
    D&= m_0\left(Z\right) + \nu & \mathbb{E}\left[\nu\mid Z\right] &=0 & \text{treatment assignment}
\end{align*}
which allows for general heterogeneity in treatment effect. The confounding factors $Z$ affect the treatment variable $D$ via propensity score $m_0(Z)\coloneq \mathbb{E}\left[D\mid Z\right]$, and the outcome variable via function $g_0(D,Z)$, both of these functions will be estimated via ML methods.

Then the ATE or ATT are 
\begin{align*}
    \theta_0 &= \mathbb{E}\left[g_0\left(1,Z\right),g_0\left(0,Z\right)\right] & \text{ATE}\\
    \theta_0 &= \mathbb{E}\left[g_0\left(1-Z\right)-g_0\left(0,Z\right)\mid D=1\right] & \text{ATT}
\end{align*}
conisder a score $\psi(W;\theta,\eta)$ that satisfies
\begin{align*}
    \mathbb{E}\psi \left(W;\theta_0,\eta_0\right) &=0 & \text{identification condition}\\
    \left.\partial_n\mathbb{E}\psi \left(W;\theta_0,\eta\right)\right\vert_{\eta=\eta_0} &=0 & \text{Neyman orthogonality condition}
\end{align*}
\citet{chernozhukov2017double} suggest employing 
\begin{itemize}
    \item for \myhl[myblue]{\textbf{ATE}}
    \begin{align*}
        \psi\left(W;\theta,\eta\right)&= \frac{D\left(Y-g(0,Z)\right)}{m} - \frac{m(Z)(1-D)\left(Y-g(0,Z)\right)}{\left(1-m(Z)\right)m} - \theta\frac{D}{m} \\
        \eta(Z) &\coloneq \left(g(0,Z),g(1,Z),m(Z)\right)\\
        \eta_0(Z) &\coloneq \left(g_0(0,Z),g_0(1,Z),m_0(Z)\right)
    \end{align*}
    \item for \myhl[myblue]{\textbf{ATT}}
    \begin{align*}
        \psi\left(W;\theta,\eta\right) &= \frac{D\left(Y-g(0,Z)\right)}{m} - \frac{m(Z)(1-D)\left(Y-g(0,Z)\right)}{\left(1-m(Z)\right)m} - \theta\frac{D}{m} \\
        \eta(Z) &\coloneq \left(g(0,Z),g(1,Z),m(Z),m\right)\\
        \eta_0(Z) &\coloneq \left(g_0(0,Z),g_0(1,Z),m_0(Z),\mathbb{E}[D]\right)
    \end{align*}
\end{itemize}
where $\eta(Z)$ is the nuisance parameter with true value of $\eta_0(Z)$ consisting of $P-$square integrable functions, mapping the support of $Z$ to $\mathbb{R}\times \mathbb{R} \times\left(\epsilon,1-\epsilon \right)$ for ATE and to $\mathbb{R}\times \mathbb{R} \times\left(\epsilon,1-\epsilon \right) \times\left(\epsilon,1-\epsilon\right)$ for ATT\footnote{All semiparametrically efficient scores satisfy Neyman orthogonality, but not vice versa. In some problems, one may consider inefficient orthogonal scores for robustness.}. 

\citet{chernozhukov2017double} propose to use cross-fitting to avoid overfitting and the Neyman orthogonality to reduce regularization and modeling biases of ML estimators $\hat{\eta}_0$, hence, \textbf{double debiasing}
\begin{algorithm}{Estimation with Orthogonal Scores by $K-$fold Cross-Fitting}{chernozhukov_doublyrobust_alg}
    \begin{itemize}
        \item[\textbf{S1}] Let $K$ be a fixed integer. Form a $K-$fold random partition of $\left\{1,\cdots,N\right\}$ by dividing it into equal parts $\left(I_k\right)^K_{k=1}$, each of size $n\coloneq N/K$. For each $I_k$, let $I^C_k$ denote all indices that are \textbf{not} in $I_k$
        \item[\textbf{S2}] Construct $K$ estimators
        $\check{\theta}_0\left(I_k,I^C_k\right)$, $k=1,\cdots,K$
        that employ the machine learning estimators 
        $$
        \hat{\eta}_0\left(I^C_k\right) = \left( \hat{g}_0\left(0,Z;I^C_k\right),\hat{g}_0\left(1,Z;I^C_k\right),\hat{m}_0\left(Z;I^C_k\right),\frac{1}{N-n}\sum_{i=\in I^C_k}D_i \right)'
        $$
        of the nuisance parameters $$ \eta_0(Z) = \left(g_0(0,Z),g_0(1,Z),m_0(Z),\mathbb{E}\left[D\right]\right)' $$
        and where each estimator $\check{\theta}_0\left(I_k,I^C_k\right)$ is defined as the root $\theta$ of $$ \frac{1}{n}\sum_{i\in I_k}\psi \left(W;\theta,\hat{\eta}_0\left(I^C_k\right)\right) =0 $$
        for the score $\psi$ for ATE and ATT respectively.
        \item{\textbf{S3}} Average the $K$ estimators to obtain the final estimator $$ \tilde{\theta}_0 = \frac{1}{K}\sum^K_{k=1}\check{\theta}_0\left(I_k,I^C_k\right) $$
        and an approximate standard error for this estimator is $\hat{\sigma}/\sqrt{N}$, where
        $\hat{\sigma}^2 = \frac{1}{N}\sum^N_{i=1}\hat{\psi}^2_i$, $\hat{\psi}\coloneq \psi\left(W_i;\tilde{\theta}_0,\hat{\eta}_0\left(I^C_{k(i)}\right)\right)$, $k(i) \coloneq \left\{k\in \left\{1,\cdots,K\right\}:i\in I_k\right\}$
        an approximate $\left(1-\alpha\right)\times 100\%$ Confidence interval is $$ \mathrm{CI}_n \coloneq \left[\tilde{\theta}_0 \pm \Phi^{-1}\left(1-\frac{\alpha}{2}\right)\frac{\hat{\sigma}}{\sqrt{N}} \right] $$
    \end{itemize}
    The \textbf{key assumptions} on the rate of estimating are 
    \begin{itemize}
        \item $\forall d\in\left\{0,1\right\}$, $\left\Vert \zeta \right\Vert _{P,2}\geq c$, $\left\Vert v \right\Vert _{P,2}\geq c$, and
        \begin{align*}
            \left\Vert g(d,Z) \right\Vert _{P,q} &\leq C & \left\Vert Y \right\Vert _{P,q} &\leq C & P\left(\epsilon \leq m_0 (Z) \leq 1-\epsilon\right) &=1 & P\left(\mathbb{E}_P\left[\zeta^2\mid Z\right]\leq C\right)&=1 
        \end{align*}
        \item the ML estimators based on a random subset $I^C_k$ of $\left\{1,\cdots,N\right\}$ of size $N-n$, $\forall N\geq 2K, d\in\left\{0,1\right\}$
        \begin{align*}
            \left\Vert \hat{g}_0\left(d,Z;I^C_k\right)-g_0(d,Z) \right\Vert _{P,2}\cdot \left\Vert \hat{m}_0\left(Z;I^C_k\right) -m_0(Z) \right\Vert _{P,2} &\leq \frac{\delta_n}{\sqrt{n}} \\
            \left\Vert \hat{g}_0\left(d,Z;I^C_k\right)-g_0(d,Z) \right\Vert _{P,2} + \left\Vert \hat{m}_0\left(Z;I^C_k\right) -m_0(Z) \right\Vert _{P,2} &\leq \delta_n
        \end{align*}
        and $P\left(\epsilon\leq \hat{m}_0\left(Z;I^C_k\right)\leq 1-\epsilon\right)=1$ with $P_P-$probability no less than $1-\Delta_n$\footnote{The conditions are fairly sharp, in the sense that for a sparse regression function $g_0$ and a propsensity function $m_0$ with sparsity indices $s^g\ll n,s^m\ll n$, and estimators $\hat{g}_0$ and $\hat{m}_0$ have sparsity indices of order $s^g$ and $s^m$, converging to $g_0$ and $m_0$ at the rates $\sqrt{s^g/n}$ and $\sqrt{s^m/n}$. The rate conditions in the assumption require $$ \sqrt{s^g/n} \cdot \sqrt{s^m/n} \Leftrightarrow s^gs^m\ll \sqrt{n} $$ which is much weaker than the without-sample-splitting condition $\left(s^g\right)^2 + \left(s^m\right)^2\ll n$}.
    \end{itemize}
\end{algorithm}

the estimator from Algorithm \ref{algm:chernozhukov_doublyrobust_alg} achieves asymptotic normality:
$\sigma^{-1}\sqrt{N}\left(\tilde{\theta}_0-\theta_0\right) \rightarrow \mathcal{N}\left(0,1\right)$, where $\sigma^2 = \mathbb{E}_P\left[\psi^2\left(W;\theta_0,\eta_0\left(Z\right)\right)\right]$, the results hold with $\hat{\sigma}^2$. The confidence regions based upon $\tilde{\theta}_0$ have uniform asymptotic validity $$ \sup_{P\in \mathcal{P}}\left\vert P\left(\theta_0\in CI_n\right)-\left(1-\alpha\right) \right\vert \rightarrow 0$$.

\textbf{Uncertainty due to sample-splitting}: although sample partitions have no impact on estimation results asymptotically, but may be important in finite samples. \citet{chernozhukov2017double} proposes to repeat the procedure $S$ times and repartition each time, get a set of estimates $\tilde{\theta}^s_0$.
Then, consider 2 quantities:
\begin{itemize}
    \item sample \textbf{average} of the $S$ estimates: $\tilde{\theta}_0^{\text{mean}}$
    \item sample \textbf{median} of the $S$ estimates: $\tilde{\theta}_0^{\text{median}}$
\end{itemize}
$\tilde{\theta}_0^{\text{median}}$ is less affected by extreme estimates, hence more robust. But asymptotically, the specific random partition is irrelevant, hence $\tilde{\theta}_0^{\text{mean}}$ and $\tilde{\theta}_0^{\text{median}}$ should be close.
\citet{chernozhukov2017double} also introduce the repsective standard error measures:
\begin{align*}
    \hat{\sigma}^{\text{mean}} &= \sqrt{\frac{1}{S}\sum^S_{s=1}\left(\hat{\sigma}^2_s+\underbrace{\left(\tilde{\theta}_0^s-\frac{1}{S}\sum^S_{j=1}\tilde{\theta}^j_0\right)^2}_{\text{variation of sample splitting}}\right)} & \hat{\sigma}^{\text{median}} &= \text{median} \left\{ \sqrt{\hat{\sigma}^2_i + \underbrace{\left(\hat{\theta}_i-\hat{\theta}^{\text{median}}\right)^2}_{\text{variation of sample splitting}} } \right\}^S_{i=1}
\end{align*}

\textbf{In practice}, one might want to consider the following
\begin{itemize}
    \item for \textbf{extreme propensity score}, set cutoffs close to 0 and 1 
    \item for \textbf{nuisance function estimation}, some typical methods are tree-based methods (Random Forest, Regression Tree, Boosting), or $l_1-$penalization method (Lasso), or neural network.
    One can also use the weighted averages of these methods s.t. the average gives the lowest mean squared out-of-sample prediction errors.
\end{itemize}

\citet{athey2018approximate} proposed a similar framework, focusing on high dimensional linear models. They showed that in linear models, it is enough to correct for linear biases in two steps:
\begin{itemize}
    \item[\textbf{S1}] Fit a regularized linear model for the outcome given the features separately in the 2 treatment groups 
    \item[\textbf{S2}] reweight the first-stage residuals by using weights that approximately balance all the features between the treatment and control groups
\end{itemize}
\citet{athey2018approximate} showed that it is often possible to achieve approximate balance under reasonable assumptions and that approximate balance suffices for eliminating bias due to regularization, combined with a lasso regression adjustment.

Consider the conditional ATE for the treated sample
\begin{equation*}
    \tau = \frac{1}{n_t} \sum_{i:D_i=1} \mathbb{E}\left[Y_i(0)-Y_i(1)\mid X_i\right]
\end{equation*}
In addition to unconfoundedness, they also assume \textbf{linearity}:
\begin{align*}
\mu_c(x) &= \mathbb{E}\left[Y_i(0)\mid X=x\right] = x\beta_c & \mu_t(x) = \mathbb{E}\left[Y_i(1)\mid X=x\right] = x\beta_t
\end{align*}
for all $x\in \mathbb{R}^p$. This assumption is strong, but generally used in high-dimension cases. Given linearity, we have 
\begin{align*}
    \tau &=\mu_t-\mu_c & \mu_t &=\bar{X}_t\beta_t & \mu_c&=\bar{X}_t\beta_c &\bar{X}_t &= \frac{1}{n_t}\sum^n_{i=1}\mathbf{1}\left\{D_i=1\right\}X_i
\end{align*}
the main focus of this paper is to estimate $\mu_c$ when $p$ is large, combining two approaches:
\begin{itemize}
    \item \textbf{weighted} estimation: weighting control to mimic treatment
    \item \textbf{regression} adjustment: estimate $\beta_c$ using control observations and get $\hat{\mu}_c=\bar{X}_t\hat{\beta}_c$
\end{itemize}
both of them perform poorly in high-dimension setting alone. \citet{athey2018approximate} propose a meta-algorithm that estimate 
\begin{equation*}
    \hat{\mu}_c = \bar{X}_t \hat{\beta}_c + \sum_{i:W_i=0}\gamma_i \left(Y_i^{obs}-X_i\hat{\beta}_c\right)
\end{equation*}
where $\bar{X}_t \hat{\beta}_c$ captures the strong signals, and $\gamma_i$ rebalance the residuals and effectively extract left-over signals. It satisfies 
\begin{equation*}
    \left\vert \hat{\mu}_c-\mu_c \right\vert \leq \underbrace{\left\Vert \bar{X}_t-X^T_c\gamma \right\Vert _{\infty}\left\Vert \hat{\beta}_c-\beta_c \right\Vert _1}_{\text{dimensionality bias}} + \underbrace{\left\vert \Sigma_{i:W_i=0}\gamma_i\epsilon_i \right\vert}_{\text{variance}} 
\end{equation*}
the dimensionality bias should be expected to scale as 
\begin{align*}
    \left\Vert \bar{X}_t-X^T_c\gamma \right\Vert _{\infty} &=O\left(\sqrt{\log (p)/n}\right) & \left\Vert \hat{\beta}_c-\beta_c \right\Vert _1 &=O\left(k\cdot \sqrt{\log (p)/n} \right)
\end{align*}
then in a sufficiently sparse setting, i.e., $k$ is sufficiently small, the bias could be \textbf{variance dominated}. The estimation procedure is 
\begin{itemize}
    \item[S1] compute the \textbf{positive} approximately balancing weights $\gamma$ to make the mean of the reweighted control sample $X^T_c\gamma$ match the treated sample mean $\bar{X}_t$ as closely as possible, 
    \begin{equation*}
        \gamma = \arg\min_{\tilde{\gamma}} \left\{ \left(1-\zeta\right)\left\Vert \tilde{\gamma} \right\Vert^2_2 + \zeta \left\Vert \bar{X}_t- X^T_c\tilde{\gamma} \right\Vert^2_{\infty}\text{ s.t. }\sum_{i:W_i=0}\tilde{\gamma}_i=1,0\leq \tilde{\gamma}_i\leq n_c^{-2/3} \right\}
    \end{equation*}
    $\gamma$ is constrained to be positive to prevent aggresive extrapolating.
    \item[S2] fit $\beta_c$ in the linear model by using a lasso or an elastic net to achieve sufficiently good risk bounds under $L_1-$risk
    \begin{equation*}
        \hat{\beta}_c = \arg \min_{\beta} \left[ \sum_{i:W_i=0} \left(Y_i^{obs}-X_i\beta\right)^2 + \lambda\left\{(1-\alpha)\left\Vert \beta \right\Vert^2_2 + \alpha\left\Vert \beta \right\Vert _1 \right\} \right]
    \end{equation*}
    \item[S3] estimate the ATE as 
    \begin{equation*}
        \hat{\tau} = \bar{Y}_t - \left\{ \bar{X}_t\hat{\beta}_c + \sum_{i:W_i=0} \gamma_i\left(Y^{obs}_i-X_i\hat{\beta}_c\right) \right\}
    \end{equation*}
\end{itemize}
an analogous estimator for the ATE $\mathbb{E}\left[Y(1)-Y(0)\right]$ can also be constructed as 
\begin{equation*}
    \hat{\tau}_{ATE} = \bar{X}\left(\hat{\beta}_t-\hat{\beta}_c\right) + \sum_{i:W_i=1} \gamma_{t,i}\left(Y^{obs}_i-X_i\hat{\beta}_t\right) -\sum_{i:W_i=0} \gamma_{c,i}\left(Y_i^{obs}-X_i\hat{\beta}_c\right)
\end{equation*}
where 
\begin{equation*}
    \gamma_t = \arg\min_{\tilde{\gamma}} \left\{ \left(1-\zeta\right)\left\Vert \tilde{\gamma} \right\Vert^2_2 + \zeta \left\Vert \bar{X}- X^T_t\tilde{\gamma} \right\Vert^2_{\infty}\text{ s.t. }\sum_{i:W_i=0}\tilde{\gamma}_i=1,0\leq \tilde{\gamma}_i\leq n_c^{-2/3} \right\}
\end{equation*}
and $\gamma_c$ is constructed similarly.

There are 3 key features of this algorithm:
\begin{itemize}
    \item[(a)] direct covariance adjustment based on the outcome data with regularization to deal with the \textbf{high dimensionality}
    \item[(b)] \textbf{weighting} using the relationship between the treatment and the features
    \item[(c)] the weights are based on \textbf{direct measures of imbalance} rather than on propensity score estimates
\end{itemize}

In comparison with doubly robust estimators,
\begin{itemize}
    \item doubly robust estimators estimate the outcome model and the propensity score, this method instead relies on linearity assumption twice
    \item doubly robust estimators are preferable given sufficient sparsity and well-specified propensity models
    \item this method is not estimating propensity score, but just balancing, hence substantially easier
\end{itemize}

\paragraph{Unconfoundedness versus DiD/TWFE}
The unconfoundedness assumption and TWFE model validate different non-nested comparisons:
\begin{itemize}
    \item \myhl[myblue]{\textbf{TWFE}} model assumes: the treated differs from the control in unobserved characteristics that are potentially \textbf{correlated} with a persistent component of the outcomes (the fixed effects $\alpha_i$)
    \item \myhl[myblue]{\textbf{Unconfoundedness}} assumes: the selection is based \textbf{solely on past} rather than future outcomes
\end{itemize}
The literature in general does not provide a lot of guidance on the choice between the two strategies\footnote{\citet{xu2023causal} touches on this topic by comparing \textbf{strict exogeneity} (TWFE) and \textbf{sequential ignorability} (unconfoundedness).}.
In two cases, unconfoundedness and TWFE lead to similar results:
\begin{itemize}
    \item the averages of previous period outcomes are similar for the control and the treatment group 
    \item the avearge in the control group does not change much over time
\end{itemize}
when the control group changes over time, and the control group and treatment group differ in the initial period, then TWFE and unconfoundedness give different results. The differences can be bounded under additional assumptions. Consider the case of 2 periods, where everyone in the first period with treatment equal to $0$, and a positive true treatment effect $\beta$. 
\begin{itemize}
    \item If treatment is correlated with an unobserved individual fixed effects $a_i$, and outcomes are 
    \begin{align*}
        Y_{it} =& a_i+\beta D_{it}+\epsilon_{it} & Y_{it-1} &=a_i + \epsilon_{it-1}
    \end{align*}
    where $\epsilon_{it}$ is serially uncorrelated, and uncorrelated with $a_i$ and $D_{it}$. If one controls for $Y_{it-1}$ but ignore fixed effects, that is, estimate $Y_{it}$ on the residual from a regression of $D_{it}$ on $Y_{it-1}$, the resulting estimator is $\frac{\mathrm{Cov}\left(Y_{it},D_{it}-\gamma Y_{it-1}\right)}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)}$.
    Substituting $a_i\equiv Y_{it-1}-\epsilon_{it-1}$, get the real $$ Y_{it} = Y_{it-1} + \beta D_{it}+\epsilon_{it}-\epsilon_{it-1} $$
    then the estimator controlling unconfoundedness only is 
    \begin{align*}
        \frac{\mathrm{Cov}\left(Y_{it},D_{it}-\gamma Y_{it-1}\right)}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)} = \beta - \frac{\mathrm{Cov}\left( \epsilon_{it-1},D_{it}-\gamma Y_{it-1} \right)}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)} = \beta + \gamma\cdot \frac{\sigma^2_{\epsilon}}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)}
    \end{align*}
    if $ Y_{it-1} $ is larger in the control group, meaning that $\gamma<0$, just assumming unconfoundedness and adjusting for the lagged outcome will overestimate the true effects.
    \item If the correct specification is instead $$ Y_{it}=\alpha + \theta Y_{it-1}+\beta D_{it}+\epsilon_{it} $$
    where $\epsilon_{it}$ is serially uncorrelated. Just run a TWFE (in this case, first-difference) model, get an estimator $ \frac{\mathrm{Cov}\left(Y_{it}-Y_{it-1},D_{it}\right)}{\mathrm{Var}\left(D_{it}\right)} $, plug in $Y_{it}-Y_{it-1}=\alpha + \left(\theta-1\right)Y_{it-1}+\beta D_{it}+\epsilon_{it}$, get 
    \begin{align*}
        \frac{\mathrm{Cov}\left(Y_{it}-Y_{it-1},D_{it}\right)}{\mathrm{Var}\left(D_{it}\right)} = \beta + \left(\theta-1\right)\cdot \frac{\mathrm{Cov}\left(Y_{it-1},D_{it}\right)}{\mathrm{Var}\left(D_{it}\right)}
    \end{align*}
    in general $\theta\in \left(0,1\right)$ for stationary, if $Y_{it-1}$ is larger in control group, TWFE will overestimate the true effects.
\end{itemize}

In the more general setting, to estimate $\mu_0 = \mathbb{E}\left[Y_{i,t+1}(0)\mid G_i=1\right]$, 
we are comparing 
\begin{align*}
    \tilde{\mu}_{0,TWFE} &= \mathbb{E}\left(Y_{it}\mid G_i=1\right) + \mathbb{E}\left(Y_{i,t+1}\mid G_i=0\right) - \mathbb{E}\left(Y_it\mid G_i=0\right) & \text{TWFE}\\
    \tilde{\mu}_{0,uncf} &= \mathbb{E}\left[\mathbb{E}\left(Y_{t+1}\mid G=0,Y_t\right)\mid G=1\right] = \int \mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)F_{Y_t}\left(\mathrm{d}y\mid G=1\right) & \text{unconfoundedness}
\end{align*}
the difference between the two is then
\begin{equation*}
    \tilde{\mu}_{0,uncf}-\tilde{\mu}_{0,TWFE} = \int \Delta (y) F_{Y_t}\left(\mathrm{d}y\mid =1\right) - \int \Delta(y)F_{Y_t}\left(\mathrm{d}y\mid G=0\right)
\end{equation*}
where $\Delta (y)=\mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)-y = \mathbb{E}\left(Y_{t+1}-Y_t\mid G=0,Y_t=y\right)$ equals the expectation of change in the outcome conditioning on the lagged outcome in the \textbf{control} group.
\citet{ding2019bracketing} establishes that the relative magnitude of $\tilde{\mu}_{0,uncf}$ and $\tilde{\mu}_{0,TWFE}$ depends 
\begin{itemize}
    \item[(a)] $\mathbb{E}\left(Y_{t+1}-Y_t\right)$ conditional on $Y_t$ in the \textbf{control} group: the dependence of the outcome on the lagged outcome
    \item[(b)] difference between the distribution of $Y_t$ in the \textbf{treated} versus \textbf{control} group: the dependence of the treatment assignment on the lagged outcome
\end{itemize}
Assuming stationarity, $$ \frac{\partial \mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)}{\partial y}<1,\forall y $$we have
\begin{itemize}
    \item if $Y_t \bot G $, or equivalently, $F_{Y_t}\left(y\mid G=1\right) = F_{Y_t}\left(y\mid G=0\right)$
    \item if $F_{Y_t}(y\mid G=1)\geq F_{Y_t}(y\mid G=0), \forall y$, $\tilde{\mu}_{0,TWFE}\leq \tilde{\mu}_{0,uncf}$, thus $\tilde{\tau}_{TWFE}\geq \tilde{\tau}_{uncf}$
    \item if $F_{Y_t}(y\mid G=1)\leq F_{Y_t}(y\mid G=0), \forall y$, $\tilde{\mu}_{0,TWFE}\geq \tilde{\mu}_{0,uncf}$, thus $\tilde{\tau}_{TWFE}\leq \tilde{\tau}_{uncf}$
\end{itemize}
and both estimation could be biased for the true ${\tau}_{ATT}$\footnote{Stationary can be tested by estimating the derivative of the conditional mean function $\mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)$; $F_{Y_t}\left(y\mid G=1\right)$ and $F_{Y_t}\left(y\mid G=0\right)$ can be visually compared via the empirical CDF of the outcomes in the treatment and control groups.}.

For the semiparametric inverse probability weighting estimator proposed by \citet{abadie2005semiparametric},
\begin{equation*}
    \tilde{\mu}'_{0,TWFE} = \mathbb{E} \left[GY_t + \frac{e(1-G)\left(Y_{t+1}-Y_t\right)}{1-e}\right]/\Pr\left(G=1\right)
\end{equation*}
and the semiparametric estimator under unconfoundedness
\begin{equation*}
    \tilde{\mu}'_{0,uncf} = \mathbb{E} \left[\frac{e(Y_t)}{1-e(Y_t)}(1-G)Y_{t+1}\right]/\Pr\left(G=1\right)
\end{equation*}
the comparison results still hold.

\citet{imai2023matching} propose a TWFE/DID mixed method by conditioning on lagged outcomes other than the most recent one, which is differenced out by TWFE. For each treated observation, 
\begin{itemize}
    \item[1] find a set of control observations that have the idential treatment history up to the prespecified number of periods
    \item[2] refine the \textit{matched set} by adjusting for observed confounding via standard matching and weighting techniques s.t. the treated and matched control observations have similar covariate values 
    \item[3] apply a DiD estimator to account for an underlying time trend
\end{itemize}

\citet{imai2023matching} defined 2 parameters
\begin{itemize}
    \item the number of \textit{leads}, $F\geq 0$: how many periods \textbf{after} the treatment being administered, selected by researchers
    \item the number of \textit{lags}, $L\geq 0$: how many periods \textbf{before} the treatment being administered, selected based on identification assumption. This allows for some carryover effects up to $L$ periods.
\end{itemize}
assuming no spillover effect, the ATT is defined as 
\begin{align*}
    \delta\left(F,L\right) = \mathbb{E}&\left[ Y_{i,t+F}\left(D_{it}=1,D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2}\right) \right.\\
    &\left. - Y_{i,t+F}\left(D_{it}=0,D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2}\right) \mid D_{it}=1,D_{i,t-1}=0 \right]
\end{align*}
here, the treatment status can go back to the control condition before the outcome is measures, that is $D_{i,t+l}$ for some $l\in\left[1,F\right]$.
Researchers choose the value of $L$ and $F$ based on:
\begin{itemize}
    \item $L$: large $L$ improves the credibility of the carryover effect assumption, but reduce the number of matches
    \item $F$: larger $F$ reveals long-term casual effects, but hard to interpret if many units switch the treatment status during the $F$ leading periods
\end{itemize}
unconfoundedness in this setting is defined as 
\begin{align*}
    &\left[ Y_{i,t+F}\left(D_{it}=1,D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2}\right), Y_{i,t+F}\left(D_{it}=0,D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2}\right) \right] \bot D_{it}\\
    & \left\vert D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2},\left\{Y_{i,t-l}\right\}^L_{l=1},\left\{\mathbf{X}_{i,t-l}\right\}^L_{l=0} \right.
\end{align*}
where $\mathbf{X}_{it}$ is a vector of observed time-varying confounders for unit $i$ at period $t$. But if there exist \textbf{unobserved} confounders, or further histories before $t-L$ confound the casual relationship between $X_{it}$ and $Y_{i,t+F}$, this assumption will be violated.
Therefore, \citet{imai2023matching} propose to adopt the TWFE (DiD) design, and assumme \textbf{parallel trend} assumption after conditioning on the treatment, outcome, covariate histories:
\begin{align*}
    &\mathbb{E}\left[ Y_{i,t+F} \left(D_{it}=0,D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2}\right) - Y_{i,t-1} \mid D_{it}=1, D_{i,t-1}=0,\left\{D_{i,t-l},Y_{i,t-l}\right\}^L_{l=2},\left\{\mathbf{X}_{i,t-l}\right\}^L_{l=0} \right]\\
    =&\mathbb{E}\left[ Y_{i,t+F} \left(D_{it}=0,D_{i,t-1}=0,\left\{D_{i,t-l}\right\}^L_{l=2}\right) - Y_{i,t-1} \mid D_{it}=0, D_{i,t-1}=0,\left\{D_{i,t-l},Y_{i,t-l}\right\}^L_{l=2},\left\{\mathbf{X}_{i,t-l}\right\}^L_{l=0} \right]
\end{align*}
the conditioning set includes the \textbf{treatment history}, \textbf{covariate history}, and \textbf{lagged outcomes}, except the \textbf{immediate lag} $Y_{i,t-1}$.
The parallel trend assumption cannot account for unobserved timing-varying confounders, so one must examine the pretrends of the treated and matched control units.

\citet{imai2023matching} construct the matched control sets as:
\begin{equation*}
    \mathcal{M}_{it} = \left\{ i': i'\neq i,D_{i't}=0,D_{i't'}=D_{it'},\forall t'=t-1,\cdots,t-L \right\}
\end{equation*}
for the treated observations with $D_{i,t}=1$ and $D_{i,t-1}=0$. Unlike staggered adoption, units are allowed to switch treatment status multiple times over time.

There are several methods to adjust for other confounders for the parallel pre-trend assumption,
\begin{itemize}
    \item \textbf{matching}: suppose $\left\vert \mathcal{M}_{it} \right\vert \leq J$, that is, each treated observation is matched with at most $J$ control units from the mathced set with replacement. Then one can compute
    \begin{equation*}
        S_{it}\left(i'\right) = \frac{1}{L} \sum^L_{l=1}\sqrt{ \left( \mathbf{V}_{i,t-l}-\mathbf{V}_{i',t-l}\right)'\boldsymbol{\Sigma}^{-1}_{i,t-l} \left( \mathbf{V}_{i,t-l}-\mathbf{V}_{i',t-l} \right) }
    \end{equation*}
    for a matched control unit $i'\in \mathcal{M}_{it}$ where $\mathbf{V}_{it'}$ represents the time-varying covariates one wishes to adjust for and $\boldsymbol{\Sigma}_{it'}$ is the sample covariance matrix of $\mathbf{V}_{it'}$. That is, given a matched control unit, compute the standardized distance using the time-varying covariates and average it across time periods.

    One can also use the estimated propensity scores to compute the distance measure. One can 
    \begin{itemize}
        \item[S1] create a subset of the data consisting all treated observations and their matched observations from the same period
        \item[S2] fit a treatment assignment model and estimate the propensity scores $$\hat{e}_{it}\left(\left\{\left(D_{i,t-l},\mathbf{V}_{i,t-l}'\right)'_{i,t-l}\right\}^L_{l=1}\right)$$ 
    \end{itemize}
    then adjust for the lagged covariates by matching on the estimated propensity score,
    \begin{equation*}
        S_{it}\left(i'\right) = \left\vert \mathrm{logit} \left\{ \hat{e}_{it}\left(\left\{\left(D_{i,t-l},\mathbf{V}_{i,t-l}'\right)'_{i,t-l}\right\}^L_{l=1}\right) \right\} -\mathrm{logit} \left\{ \hat{e}_{it}\left(\left\{\left(D_{i',t-l},\mathbf{V}_{i',t-l}'\right)'_{i',t-l}\right\}^L_{l=1}\right) \right\} \right\vert 
    \end{equation*}
    After computing the distance measure $S_{it}(i')$ for all control units, we can refine $\mathcal{M}_{it}$ by selecting the $J$ most similar control units:
    \begin{equation*}
        \mathcal{M}^*_{it} = \left\{ i':i'\in\mathcal{M}_{it},S_{it}(i')<C,S_{it}(i')\leq S^{(J)}_{it} \right\}
    \end{equation*}
    where $S^{J}_{it}$ is the $J$th-order statistic of $S_{it}(i')$ among control units in the original $\mathcal{M}_{it}$.
    \item \textbf{weighting}: construct a weight for each control unit $i'$ within a matched set of a given treated observation $(i,t)$ where a greater weight is assigned to a more similar unit.
    One can use the inverse propensity score weighting method of \citet{hirano2003efficient}, where 
    \begin{align*}
        w^{i'}_{it} &\varpropto \frac{ \hat{e}_{it}\left(\left\{\left(D_{i,t-l},\mathbf{V}_{i,t-l}'\right)'_{i,t-l}\right\}^L_{l=1}\right) }{ 1- \hat{e}_{it}\left(\left\{\left(D_{i,t-l},\mathbf{V}_{i,t-l}'\right)'_{i,t-l}\right\}^L_{l=1}\right) } & \text{s.t. } \sum_{i'\in\mathcal{M}_{it}} w^{i'}_{it} &=1 \text{ and } w^{i':i'\not\in \mathcal{M}_{it}}_{it}=0
    \end{align*}
    this model should be fitted to the \textbf{entire} sample of treated and control observations.
\end{itemize}

After constructing the refined matched sets, then we can compute the DiD estimate as 
\begin{align*}
    \hat{\delta}\left(F,L\right) &= \frac{1}{\sum^N_{i=1}\sum^{T-F}_{t=L+1}D_{it}}\sum^N_{i=1}\sum^{T-F}_{t=L+1}D_{it} \left\{ \left(Y_{i,t+F}-Y_{i,t-1} \right) - \sum_{i'\in\mathcal{M}_{it}}w^{i'}_{it}\left(Y_{i',t+F}-Y_{i',t-1}\right) \right\}
\end{align*}
where the counterfactual outcome is $$ Y_{i,t+F}\left(X_{it}=0,X_{i,t-1}=0,X_{i,t-2},\cdots,X_{i,t-L}\right) $$
and $D_{it}=X_{it}\left(1-X_{i,t-1}\right)\cdot \mathbf{1}\left\{\left\vert \mathcal{M}_{it} \right\vert>0\right\}$, $D_{it}=1$ only if observation $(i,t)$ changes the treatment status from the control condition at $t-1$ to treatment at $t$, with at least 1 matched control unit, $w^{i'}_{it}$ is nonnegative normalized weight.

If $F>0$, that is, we are interested in carryover effects, the matched control units may include those units that receive the treatment after $t$ but before the outcome is measured at $t+F$, some treated units may return to control before $t+F$. One may want to specify the future treatment sequence s.t. the causal quantity is defined w.r.t. the counterfactual scenario of interests.

One must check the balance of \textbf{the lagged outcome variables} and \textbf{time-varying covariates} over multiple pretreatment periods. For each treated observation $(i,t)$ with $D_{it}=1$, define the balance for variable $j$ at the period $t-l$ as 
\begin{equation*}
    B_{it}\left(j,l\right) = \frac{ V_{i,t-l} -\sum_{i'\in\mathcal{M}_{it}} w^{i'}_{it}V_{i',t-l,j} }{ \sqrt{ \frac{1}{N_1-1} \sum^N_{i'=1}\sum^{T-F}_{t'=L+1}D_{i't'}\left(V_{i',t'-l,j} - \bar{V}_{t'-l,j} \right)^2 } }
\end{equation*}
where $N_\equiv \frac{1}{N_1-1} \sum^N_{i'=1}\sum^{T-F}_{t'=L+1}D_{i't'}$ is the total number of treated observations, and $\bar{V}_{t-l,j}=\sum^N_{i,t-l,j}/N$. Then aggregate it across all treated observations for each covariate and pretreatment time period, get 
\begin{equation*}
    \bar{B} \left(j,l\right) = \frac{1}{N_1} \sum^N_{i=1} \sum^{T-F}_{t=L+1} D_{it} B_{it} (j,l)
\end{equation*}

Assuming at least one treat and control unit, $0<\sum^N_{i=1}\sum^T_{t=1}X_{it}<NT$, and at least one unit with $D_{it}=1$, $0<\sum^N_{i=1}\sum^T_{t=1}D_{it}$, \citet{imai2023matching}'s DiD estimate $\hat{\delta}\left(F,L\right)$ is equivalent to a \textbf{weighted} TWFE estimate:
\begin{theorem}{\citet{imai2023matching}'s DiD estimate and weighted TWFE equivalance}{imai_2wfeequiv}
    $\hat{\delta}(F,L)$ is equivalent to 
    \begin{align*}
        \hat{\beta}_{TWFE} = & \arg\min_{\beta} \sum^N_{i=1}\sum^T_{t=1}W_{it}\left\{ \left(Y_{it}-\bar{Y}^*_i-\bar{Y}^*_t+\bar{Y}^* - \beta \left( X_{it}-\bar{X}^*_i-\bar{X}^*_t+\bar{X}^* \right) \right) \right\}
    \end{align*}
    where $\bar{\square}^*$ is the weighted average of $\square$ by $W$, where the weight is 
    \begin{align*}
        W_{it}&= \sum^N_{i'=1}\sum^T_{t'=1} D_{i't'}\cdot \nu^{i't'}_{it}, & \nu^{i't'}_{it}&= \begin{cases}
            1, & (i,t)=(i',t'+F)\\
            1, & (i,t)=(i',t'-1)\\
            w^i_{i't'}, & i\in\mathcal{M}_{i't'},t=t'+F\\
            -w^i_{i't'}, & i\in\mathcal{M}_{i't'},t=t'-1\\
            0,&\text{otherwise}
        \end{cases}
    \end{align*}
    in many cases, $W_{it}$ can take a negative value, implying the TWFE estimator relies heavily on its parametric assumption.
\end{theorem}

To computate the standard errors of $\hat{\delta}(F,L)$, we use an observation-specific weight 
\begin{align*}
    W_{it}&= \sum^N_{i'=1}\sum^T_{t'=1} D_{i't'}\cdot \nu^{i't'}_{it}, & \nu^{i't'}_{it}&= \begin{cases}
        1, & (i,t)=(i',t'+F)\\
        -1, & (i,t)=(i',t'-1)\\
        -w^i_{i't'}, & i\in\mathcal{M}_{i't'},t=t'+F\\
        w^i_{i't'}, & i\in\mathcal{M}_{i't'},t=t'-1\\
        0,&\text{otherwise}
    \end{cases}
\end{align*}
and calculate the \textbf{conditional standard errors} as 
\begin{align*}
    \mathbb{V}\left(\hat{\delta}(F,L)\mid \mathbf{D}\right) = \frac{N^* \cdot \mathbb{V}\left( \sum^N_{i=1}\sum^T_{t=1}W^*_{it} Y_{it} \right)}{\left(\sum^N_{i=1}\sum^T_{t=1}D_{it}\right)^2}
\end{align*}
where $N^*$ is the total number of units with at least one nonzero weight; and the \textbf{unconditional standard errors} as
\begin{align*}
    \mathbb{V}\left(\hat{\delta}(F,L)\right) =& \frac{1}{\mathbb{E}\left[\sum^N_{i=1}\sum^T_{t=1}D_{it}\right]^2}\cdot \\
    & \left\{ N\cdot \mathbb{E}\left[\sum^T_{t=1}D_{it}\right] - 2\frac{\mathbb{E}\left(\sum^N_{i=1}\sum^T_{t=1}W^*_{it} Y_{it}\right)}{\sum^N_{i=1}\sum^T_{t=1}D_{it}} \cdot N\cdot \mathrm{Cov}\left(\sum^T_{t=1}W^*_{it} Y_{it},\sum^T_{t=1}D_{it}\right)\right.\\
    & \left. + \frac{\mathbb{E}\left[\sum^N_{i=1}\sum^T_{t=1}W^*_{it} Y_{it}\right]}{\mathbb{E}\left[\sum^N_{i=1}\sum^T_{t=1}D_{it}\right]^2}\cdot N\cdot\mathbb{V}\left(\sum^T_{t=1}D_{it}\right) \right\}
\end{align*}
one can also apply the block boostrap procedure for within-unit time dependence.

\subsubsection{Distributional effects}
Heterogeneity in treatment effects does not creat problems for the DiD estimator, but the distributional effects might be of interest.

\paragraph*{\citet{bonhomme2011recover}} consider the 2 period case, where in period 1 (pre-treatment), the realized outcome is denoted as $Y_{i1}$; in period 2 (post-treatment), the observed outcome $Y_{i2} = D_iY^1_{i2} + (1-D_i)Y^0_{i2}$ where $Y_{i2}^0$ is the potential outcome if not treated, $Y_{i2}^1$ is the potential outcome if treated.

The outcomes are model as 
\begin{align*}
    Y_{i2}^0 &= f_2^0\left(X_i\right) + \beta_2^0\eta_i + \nu^0_{i2} & Y_{i1} &= f_1\left(X_i\right) + \beta_1\eta_i + \nu_{i1}
\end{align*}
where outcomes depend on observed covariates $X_i$ and initial endowment $\eta_i$. For a special case of this model with no covariates and equal returns to the endowment
\begin{align*}
    Y_{i2}^0 &= \alpha_2^0 + \eta_i + \nu^0_{i2} & Y_{i1} &= \alpha_1 + \eta_i+\nu_{i1}
\end{align*}
\citet{bonhomme2011recover} made the following assumptions to identify the \textbf{distribution} of $Y^0_{i2}$ given $D_i=1$:
\begin{itemize}
    \item[A1] \textbf{consistent DiD}: $\nu_{i1}$ and $\nu_{i2}^0$ are independent of $D_i$
    \item[A2] \textbf{fixed effects}: $\nu_{i1}$ and $\nu_{i2}^0$ are independent of $\eta_i$ given $D_i$
    \item[A3] \textbf{characteristic function}: the characteristic function of $Y_{i1}$ given $D_i=0$ is nonvanishing on $\mathbb{R}$, where the function is $ \Psi_W(t) = \mathbb{E}\left[\exp( it W)\right] $\footnote{This is a common assumption in the nonparametrically deconvolution literature. Many usual parametric distribution (normal, gamma) satisfiy this condiditon.}. 
\end{itemize}
Under A1-A3, the characteristic function of $Y^0_{i2}$ given $D_i=1$ is identified as 
\begin{equation*}
    \Psi _{Y^0_{i2}\mid D_i=1}(t) = \frac{\Psi _{Y_{i1}\mid D_i=1}(t)}{\Psi _{Y_{i1}\mid D_i=0}(t)} \Psi _{Y_{i2}\mid D_i=0}(t)
\end{equation*}
in log form
\begin{equation*}
    \log \Psi _{Y_{i2}^0\mid D_i=1}(t) = \log \Psi _{Y_{i2}\mid D_i=0}(t) + \Psi _{Y_{i1}\mid D_i=1}(t) - \Psi _{Y_{i1}\mid D_i=0}(t)
\end{equation*}
which is a generalization of the DiD logic: the distribution of realized outcomes for the treated is corrected for the fact that treated and controls do not have the same distribution of unobservables $\eta_i$.
Then, from the identified characteristic function, recover the distribution of the potential outcomes by taking the inverse Fourier transformation
\begin{equation*}
    f _{Y_{i2}^0\mid D_i=1}(y) = \frac{1}{2\pi} \int \exp \left(-ity\right) \left[ \frac{\Psi _{Y_{i1}\mid D_i=1}(t)}{\Psi _{Y_{i1}\mid D_i=0}(t)} \Psi _{Y_{i2}\mid D_i=0}(t) \right] \mathrm{d}t
\end{equation*}
then the quantile treatment effects can be identified as 
\begin{equation*}
    \Delta\left(\tau\right) \equiv F^{-1}_{Y_{i2}\mid D_i=1}(\tau) - F^{-1}_{Y^0_{i2}\mid D_i=1}(\tau),\ \tau\in[0,1]
\end{equation*}
Notice that the distribution of $\left(v_{i1},v^0_{i2}\right)$ is not restricted, at the cost of imposing additivity on the outcome variables. 

In the general case, where covariates enter as $f_2^0(X_i)$ and $f_1(X_i)$, and the initial endowment to effect different periods differently with $\beta_1,\beta_2\neq 1$, \citet{bonhomme2011recover} make two extra assumptions
\begin{itemize}
    \item[A4] \textbf{overlapping}: $p_D>0$ and $p_D(X_i)<1$ with probability 1, where the propensity scores are $p_D=P(D_i=1)$, $P_D(x)=P\left(D_i=1\mid X_i=x\right)$
        This assumption do not restrict the correlations between $\eta_i,v_{i1},v^0_{i2}$ and $X_i$. Then we get 
    \begin{align*}
        \Psi_{Y^0_{i2}\mid D_i=1,X_i}(t\mid x) &= \frac{\Psi_{Y_{i1}\mid D_i=1,X_i}(t\mid x)}{ \Psi_{Y_{i1}\mid D_i=0,X_i}(t\mid x) } \Psi_{Y_{i2}\mid D_i=0,X_i}(t\mid x) & \Psi_{Y^0_{i2}\mid D_i=1}(t) &= \frac{1}{p_D}\mathbb{E}\left[ \omega (t\mid X_i)(1-D_i)\exp(itY_{i2}) \right]
    \end{align*}
    where
    \begin{equation*}
        \omega(t\mid X_i) \equiv \frac{p_D(X_i)}{1-p_D(X_i)} \cdot \frac{\Psi_{Y_{i1}\mid D_i=1,X_i}(t\mid X_i)}{\Psi_{Y_{i1}\mid D_i=0,X_i}(t\mid X_i)} = \frac{\mathbb{E} \left[D_i\exp(itY_{i1})\mid X_i\right]}{ \mathbb{E}\left[(1-D_i)\exp(itY_{i1})\mid X_i\right] }
    \end{equation*}
    \item[A5] \textbf{instrument}: $\exists \tilde{Y}_{i0}$ s.t. $v_{i1}$ and $v_{i2}^0$ are uncorrelated with $\tilde{Y}_{i0}$ given $D_i=0$, $Y_{i1}$ and $\tilde{Y}_{i0}$ are correlated given $D_i=0$. $\tilde{Y}_{i0}$ is essentially like the linear panel data instruments, not orthogonal to fixed effects. 
    For example, consider 
    \begin{align*}
        Y^0_{i2} &= \alpha^0_2 + \beta^0_2\eta_i + v_{i2}^0 & Y_{i1} &= \alpha_1 + \beta_1\eta_i + v_{i1}
    \end{align*}
    where $Y_{i1}$ is endogenous: $$ Y_{i2}^0 = \alpha^0_2 - \rho\alpha_1 + \rho Y_{i1} + v^0_{i2}- \rho v_{i1}$$
    with the instrument $\tilde{Y_{i0}}$, we can identify $\rho$ as $$ \rho = \frac{\mathrm{Cov}\left(\tilde{Y}_{i0},Y_{i2}\mid D_i=0\right)}{\mathrm{Cov}\left(\tilde{Y}_{i0},Y_{i1}\mid D_i=0\right)} $$
    which gives the identification of the entire distribution 
    \begin{equation*}
        f_{Y^0_{i2}\mid D_i=1}(y) = \frac{1}{2\pi} \int \exp\left(-ity\right) \left[\frac{\Psi_{Y_{i1}\mid D_i=1}(\rho t)}{\Psi_{Y_{i1}\mid D_i=0} (\rho t)} \Psi_{Y_{i2}\mid D_i=0}(t) \right]\mathrm{d}t
    \end{equation*}
\end{itemize}

This identification strategy yields the estimation for a random sample $\left(Y_{i2},Y_{i1},D_i\right),i=1,\cdots,N$:
\begin{itemize}
    \item No covariates, $\eta_i$'s effects are \textbf{the same} in both periods, $$ \hat{f}_{Y^0_{i2}\mid D_i=1}(y) = \frac{1}{2\pi} \int^{T_N}_{-T_N} \exp\left(-ity\right) \times \left[ \frac{\hat{\Psi}_{Y_{i1}\mid D_i=1}(t)}{\hat{\Psi}_{Y_{i1}\mid D_i=0}(t)}\hat{\Psi}_{Y_{i2}\mid D_i=0}(t) \right] \mathrm{d}t $$
    \item No covariates, $\eta_i$'s effects are \textbf{different} in both periods, $$ \hat{f}_{Y^0_{i2}\mid D_i=1}(y) = \frac{1}{2\pi} \int^{T_N}_{-T_N} \exp\left(-ity\right) \times \left[ \frac{\hat{\Psi}_{Y_{i1}\mid D_i=1}(\hat{\rho}t)}{\hat{\Psi}_{Y_{i1}\mid D_i=0}(\hat{\rho}t)}\hat{\Psi}_{Y_{i2}\mid D_i=0}(t) \right] \mathrm{d}t $$
    where $\rho$ is estimated in an IV regression of $Y_{i2}$ on $Y_{i1}$ on the control sample, using $\tilde{Y}_{i0}$ as an instrument
    \item With covariates, $\eta_i$'s effects are \textbf{different} in both periods, $$ \hat{f}_{Y^0_{i2}\mid D_i=1}(y) = \frac{1}{2\pi} \int^{T_N}_{-T_N}\exp \left(ity\right) \frac{1}{\hat{p}_D}\times \left( \frac{1}{N} \sum^N_{i=1}\hat{\omega}\left(\hat{\rho}t\mid X_i\right)\left(1-D_i\right)\exp\left(itY_{i2}\right) \right) \mathrm{d}t$$
    where $\hat{p}_D$ is the logit-estimated propensity score, $\hat{\rho}$ is estimated via a 2SLS of $Y_{i2}$ on $Y_{i1}$ on the control sample, using $\tilde{Y}_{i0}$ and $X_i$ as instruments. 
\end{itemize}
where $\hat{\Psi}_W$ denotes the empirical characteristic function of $W$\footnote{For example, $\hat{\Psi}_{Y_{i2}\mid D_i=0}(t) = \frac{1}{N_0}\sum_{i,D_i=0}\exp(itY_{i2})$.}, the trimming parameter $T_N$ should go to infinity with the sample size $N$. Pointwise CIs can be computated using the nonparametric bootstrap.

\paragraph*{\citet{callaway2018quantile,callaway2019quantile}} introduce a Copula Stability Assumption where the missing dependence is constant over time and can be recovered with panel data.

For a panel data where at least 3 periods, $t,t-1,t-2$, of data of all individuals in the sample are available, treatment takes place at $t$. Define the $\tau-$quantile of a random variable $W$ as 
$$ \omega_{\tau} = F^{-1}_W(\tau) \coloneq \inf \left\{\omega: F_W(\omega)\geq \tau \right\} $$
denote the distributions of potential outcomes of treated and control units $Y_{1t},Y_{0t}$ as $F_{Y_{1t}\mid D=1},F_{Y_{0t}\mid D=1}$, and define the \textbf{Quantile Treatment Effect on the Treated} (QTT) as 
$$ QTT(\tau) = F^{-1}_{Y_{1t}\mid D=1}(\tau) - F^{-1}_{Y_{0t}\mid D=1}(\tau ) $$
the identification relies on the assumptions 
\begin{itemize}
    \item[A1] \textbf{Mean DiD}: denote difference in untreated potential outcomes as $\Delta Y_{0t}=Y_{0t}-Y_{0t-1}$, then $$ \mathbb{E}\left[\Delta Y_{0t}\mid D=1\right] = \mathbb{E}\left[\Delta Y_{0t}\mid D=0\right] $$ This is just the parallel trend assumption commonly used.
    \item[A2] \textbf{Distribution DiD}: the distribution of the change in untreated potential outcomes does not depend on whether or not the individual belongs to the treated or the untreated group $$ \Delta Y_{0t} \bot D $$ which is a gneralization of the parallel trend assumption.
    \item[A3] \textbf{Copula stability}: denote a copula function as $ C_{\Delta Y_{0t},Y_{0t-1}\mid D=1}\left(\cdot,\cdot\right) $, then $$ C_{\Delta Y_{0t},Y_{0t-1}\mid D=1}\left(\cdot,\cdot\right) = C_{\Delta Y_{0t-1},Y_{0t-2}\mid D=1}\left(\cdot,\cdot\right) $$
    which assumes the stability of the dependence between the \textbf{change in untreated potential outcomes for the treated}, and the \textbf{initial level of untreated potential outcomes for the treated}\footnote{This assumption is about how to identify $F_{\Delta Y_{0t},Y_{t-1}\mid D=1}$, i.e., the joint distribution $\left(\Delta Y_{0t},Y_{0t-1}\mid D=1\right)$, from which the distribution of interest $F_{Y_{0t}\mid D=1}$ can be derived. Notice that we can identify $F_{\Delta Y_{0t}\mid D=1}$ via the distributional DiD assumption, and $F_{Y_{0t-1}\mid D=1}$ directly from the data, but the joint distribution can only be identified with additional information on the copula.}.
    Let $F_{\Delta Y_{0t},Y_{0t-1}\mid D=1}$ be the joint distribution of $\Delta Y_{0t}$ and $Y_{0t-1}$ for the treatment group, then by Sklar's theorem,
    \begin{equation*}
        F_{\Delta Y_{0t},Y_{0t-1}\mid D=1} \left(\delta,y'\right) = C_{\Delta Y_{0t},Y_{0t-1}\mid D=1 } \left( F_{\Delta Y_{0t}\mid D=1}(\delta), F_{Y_{0t-1}\mid D=1}(y') \right)
    \end{equation*}
    under this assumption, the marginal distributions can change in unrestricted ways.
\end{itemize}

There are two additional assumptions for identification:
\begin{itemize}
    \item \textbf{uniqueness of copula}: $\Delta Y_t$ for the untreated and $\Delta Y_{t-1},Y_{t-1},Y_{t-2}$ for the treated are \textbf{continuously} distributed with densities that are uniformly bounded from above and away from 0.
    \item \textbf{panel data, treatment only happens at $t$}: The observed data $\left\{ Y_{it},Y_{it-1},Y_{it-2},X_i,D_i \right\}^n_{i=1}$ are i.i.d. draws from the joint distribution $F_{Y_t,Y_{t-1},Y_{t-2},X,D}$ and $Y_{it}=D_iY_{1it}+(1-D_i)Y_{0it}$, $Y_{it-1}=Y_{0it-1}$ and $Y_{it-2}=Y_{0it-2}$
\end{itemize}

then \citet{callaway2019quantile}'s identification result is given as 
$$ F_{Y_{0t}\mid D=1}(y) = \mathbb{E}\left[\mathbf{1}\left\{ F^{-1}_{\Delta Y_t\mid D=0} \left(D_{\Delta Y_{t-1}\mid D=1} \left(\Delta Y_{t-1}\right)\right) \leq y-F^{-1}_{Y_{t-1}\mid D=1}\left(D_{Y_{t-2}\mid D=1}(Y_{t-2})\right) \right\} \mid D=1 \right] $$
and 
$$ \mathrm{QTT}(\tau) = F^{-1}_{Y_t\mid D=1}(\tau) - F^{-1}_{Y_{0t}\mid D=1}(\tau) $$
Consider the case of TWFE models $Y_{0it} = \theta_t + \eta_i + \nu_{it}$, where time fixed effects $\theta_t$ are common for the treated and untreated groups, individual effects $\eta_i$ may be distributed differently across the treated and untreated. For the identification of ATT, it requires $\mathbb{E}\left[\Delta \nu_t\mid D=1\right] = \mathbb{E}\left[\Delta \nu_t\mid D=0\right]$, and to identify QTT, 
it is additionally required that 
\begin{align*}
    \Delta \nu_t &\bot D \text{ (distribution DiD)}  & C_{\Delta \nu_t,\eta+\nu_{t-1}\mid D=1} &= C_{\Delta v_{t-1},\eta+\nu_{t-2}\mid D=1}\text{ (copula stability)}
\end{align*}

One can check sufficient condition of $\left(\nu_t,\nu_{t-1}\mid \eta,D=1\right)$ and $\left(\nu_{t-1},\nu_{t-2}\mid \eta,D=1\right)$ following the same distribution. A stronger assumption also satisfies the copula stability is $\nu_{it}$ being i.i.d\footnote{Copula Stability here allows for the distribution of $\nu_{it}$ to change over time, to be serially correlated, to be correlated with the individual heterogeneity.}.

With \textbf{covariates}, there are two main scenarios that the methods can be extended to
\begin{itemize}
    \item \textbf{distributional DiD} holds conditional on covariates, \textbf{copula stability} holds unconditionally
    \item \textbf{distribution DiD} and \textbf{copula stability} hold conditional on covariates
\end{itemize}

The conditional distributional DiD assumption is $$ \Delta Y_{0t} \bot D\mid X $$
and assume overlapping 
$$
p \coloneq P(D=1)>0,\ p(x)\coloneq P(D=1\mid X=x)<1,\forall x\in \mathcal{X}
$$
then under \textbf{conditional} distributional DiD and \textbf{unconditional} copula stability, and overlapping
\begin{equation*}
    F_{Y_{0t}\mid D=1}(y) = \mathbb{E} \left[ \mathbf{1} \left\{ F^{p,-1}_{\Delta Y_{0t}\mid D=1}\left(F_{\Delta Y_{t-1}\mid D=1}(\Delta Y_{t-1})\right) \leq y-F^{-1}_{Y_{t-1}\mid D=1}\left( F_{Y_{t-2}\mid D=1}(Y_{t-2}) \right) \right\} \mid D=1 \right]
\end{equation*}
where 
$$ F^p_{\Delta Y_{0t}\mid D=1}(\delta) = \mathbb{E}\left[ \frac{1-D}{p}\frac{p(X)}{1-p(X)} \mathbf{1}\left\{\Delta Y_t\leq \delta\right\} \right] $$
and the QTT $$ \mathrm{QTT}(\tau) = F^{-1}_{Y_{1t}\mid D=1}(\tau) - F^{-1}_{Y_{0t}\mid D=1}(\tau) $$
is identified.

The difference from the unconditional results is: $F_{\Delta Y_{0t}\mid D=1}$ is not identified by the distribution of untreated potential outcomes for the untreated, but the weighted distribution $F^p_{\Delta Y_{0t}\mid D=1}(\delta)$

For the case of \textbf{conditional} copula stability, assume 
\begin{equation*}
    C_{\Delta Y_{0t},Y_{0t-1}\mid X,D=1}\left(\cdot,\cdot \mid x\right) = C_{\Delta Y_{0t-1},Y_{0t-2}\mid X,D=1}\left(\cdot,\cdot \mid x\right)
\end{equation*}
then 
\begin{align*}
    P \left( Y_{0t}\leq y \mid X=x,D=1 \right) =& \mathbb{E} \left[ \mathbf{1} \left\{F^{-1}_{\Delta Y_{0t}\mid X,D=0} \left(F_{\Delta Y_{0t-1}\mid X,D=1} \left(\Delta Y_{0t-1}\mid x\right)\right)\right. \right.\\
    & \left.\left.\leq y-F^{-1}_{Y_{0t-1}\mid X,D=1}\left( F_{Y0t-2\mid X,D=1} (Y_{0t-2}\mid x) \right)\right\} \mid X=x,D=1 \right]
\end{align*}
and $$ \mathrm{QTT}\left(\tau;x\right) = F^{-1}_{Y_{1t}\mid X,D=1}(\tau\mid x) - F^{-1}_{Y_{0t}\mid X,D=1}(\tau\mid x) $$
also 
\begin{equation*}
    P\left(Y_{0t}\leq y \mid D=1 \right) = \int_{\mathcal{X}} P\left(Y_{0t}\leq y \mid X=x,D=1\right)\mathrm{d}F_{X\mid D=1}(x)
\end{equation*}
and $$ \mathrm{QTT}(\tau) = F^{-1}_{Y_{1t}\mid D=1}(\tau) - F^{-1}_{Y_{0t}\mid D=1}(\tau) $$
are all identified.

\subsection{Staggered Adoption}
In the block-assignment setting, the constant treatment effect assumption (\ref{assump:const_stat_treat}) is not important. Therefore, allowing unrestricted heterogeneity in the treatment effects $Y_{it}(1)-Y_{it}(0)$, the TWFE/DiD estimator continues to estimate a well-defined ATE for the treated in the periods of treatment 
\begin{equation*}
    \frac{1}{\sum^N_{i=1}\sum^T_{t=1}W_{it}} \sum^N_{i=1}\sum^T_{t=1}W_{it} \left(Y_{it}(1)-Y_{it}(0)\right)
\end{equation*}
but this robustness does \textbf{not} extend outside of block assignment settings. Recent literature allows for general heterogeneity in the treatment effects in the staggered adoption setting
\begin{equation*}
    \mathbf{W}^{\text{stag}} = \begin{pmatrix}
        0&0&0&0&\cdots& 0 &0\\
        0&0&0&0&\cdots& 0 &1\\
        0&0&0&0&\cdots& 1 &1\\
        0&0&0&1&\cdots&1 &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots &\vdots & \vdots \\
        0&1&1&1&\cdots &1 & 1
    \end{pmatrix}
\end{equation*}
Let $A_i\equiv T+1-\sum^T_{t=1}W_{it}$ be the adoption date (the first time unit $i$ is treated), with $A_i\equiv \infty$ for never adopters, and $N_a$ is the number of units with adoption date $A_i=a$.
Define the ATE by time and adoption date as 
\begin{equation*}
    \tau_{t\mid a} \equiv \mathbb{E} \left[Y_{it}(1)-Y_{it}(0)\mid A_i=a \right]
\end{equation*}
which can vary \textbf{both} time and adoption date. 

\subsubsection{Decompositions of the TWFE estimator}
Consider the TWFE estimate $\hat{\tau}$ based on the least squares regression $$ \min_{\alpha,\beta,\tau}\sum^N_{i=1}\sum^T_{t=1}\left(Y_{it}-\alpha_i-\beta_t-\tau W_{it}\right)^2 $$
define for all time-periods $t$ and all adoption dates $a$ the average outcome in period $t$ for units with adoption date $a$ as $$ \bar{Y}_{t\mid a} \equiv \frac{1}{N_a}\sum_{i:A_i=a}Y_{i,t} $$
then for all pairs of time periods $t,t'$ and pairs of adoption dates $a,a'$ such that $t'<a\leq t$ and either $a'\leq t'$ or $t < a'$, define the building block for the TWFE estimator
\begin{equation*}
    \hat{\tau}^{a,a'}_{t,t'} \equiv \left( \bar{Y}_{t\mid a}-\bar{Y}_{t'\mid a} \right) - \left( \bar{Y}_{t\mid a'}-\bar{Y}_{t'\mid a'} \right)
\end{equation*}

\begin{itemize}
    \item For group $a$ ($t'< a \leq t$), $\bar{Y}_{t,a}-\bar{Y}_{t',a}$ reflects a change in treatment but the treatment effect is contaminated by the time trend in control $$ \mathbb{E}\left[\bar{Y}_{t\mid a}-\bar{Y}_{t'\mid a'}\right] =\beta_t-\beta_{t'}+\tau_{t\mid a} $$
    \item For group $a'$, there are 2 cases:
    \begin{itemize}
        \item $t'<t<a'$: group $a'$ never get treated, then $\hat{\tau}^{a,a'}_{t,t'}$ is the standard DiD 
        \item $a'<t'<t$: group $a'$ always get treated, then in the presence of treatment effect heterogeneity, the expected difference in average outcomes under the treatment is $ \mathbb{E}\left[\bar{Y}_{t\mid a'}-\bar{Y}_{t'\mid a'}\right] = \beta_t - \beta_{t'} + \left(\tau_{t\mid a'}-\tau_{t'\mid a'}\right) $, which leads to $$ \mathbb{E}\left[\hat{\tau}^{a,a'}_{t,t'}\right] =\tau_{t\mid a} - \left(\tau_{t\mid a'}-\tau_{t'\mid a'}\right) $$ it is a weighted average of treatment effects, with weights adding up to 1 and some negative weights under heterogeneous treatment effects.
    \end{itemize}
\end{itemize}

\paragraph*{Issues of TWFEs} in this scenario, TWFE has two issues
\begin{itemize}
    \item[1] without further assumptions, the estimator cannot be interpreted as an estimate of an average treatment effect with nonnegative weights in general 
    \item[2] the combination of weights on the $\hat{\tau}^{a,a'}_{t,t'}$ chosen by TWFE depends on data (the distribution of units across the adoption groups).
\end{itemize}

\subsubsection{Alternative DiD Estimators}

\paragraph*{\citet{callaway2021difference}} propose 2 ways of dealing with the negative weights: 
\begin{itemize}
    \item never-adopters as control: $\hat{\tau}_{t,a-1}^{a,\infty} = \left(\bar{Y}_{t,a}-\bar{Y}_{a-1,a}\right) - \left(\bar{Y}_{t,\infty},\bar{Y}_{a-1,\infty}\right) $. It is not ideal since the never-adopters are likely to be fundamentally different, and there may be too few of them in long panels
    \item later-adopters as control: $\hat{\tau}_{t,a-1}^{a,>t} = \left(\bar{Y}_{t,a}-\bar{Y}_{a-1,a}\right) - \frac{1}{T-t} \sum^T_{a'=t+1}\left(\bar{Y}_{t,a'}-\bar{Y}_{a-1,a'}\right) $
\end{itemize}

\citet{callaway2021difference} assume 
\begin{assumption}{\citet{callaway2021difference}'s assumptions}{cs21assumptions}
    \begin{itemize}
        \item[1] \textbf{Irreversibility of Treatment}: $D_1=0$ a.s.; for $t=2,\cdots,T$, $D_{t-1}=1\Rightarrow D_t=1$ a.s.
        \item[2] \textbf{Random Sampling}: $\left\{Y_{i,1},\cdots,Y_{i,\mathcal{T}},X_i,D_{i,1} \cdots, D_{i,\mathcal{T}}\right\}^n_{i=1}$ is i.i.d.. This implies the availability of panel data
        \item[3] \textbf{Limited Treatment Anticipation}: $\exists \delta \geq 0$ s.t. $$ \mathbb{E}\left[Y_{t,a}\mid X \right] = \mathbb{E}\left[Y_{0,a}\mid X\right] \text{ a.s. }\forall t< a-\delta $$
        where $\delta$ captures the anticipation horizon.
        \item[4] \textbf{Conditional Parallel Trends, \textit{Never-Treated}}: $\forall a, t\in\left\{2,\cdots,T\right\}$ s.t. $t\geq a-\delta$, $$ \mathbb{E}\left[Y_{t,a}-Y_{t-1,a}\mid X\right] = \mathbb{E}\left[ Y_{t,\infty}-Y_{t-1,\infty}\mid X \right] \text{ a.s.}$$
        \item[5] \textbf{Conditional Parallel Trends, \textit{Later-Treated}}: $\forall a, (s,t)\in\left\{2,\cdots,T\right\}\times \left\{2,\cdots,T\right\}$ s.t. $t\geq a-\delta$ and $t+\delta\leq s< \bar{a}$, $$ \mathbb{E}\left[Y_{t,a}-Y_{t-1,a}\mid X\right] = \mathbb{E}\left[ Y_{t,s}-Y_{t-1,s} \mid X \right] \text{ a.s.} $$
    \end{itemize}
    There is a tradeoff between Assumption 4 and 5:
    \begin{itemize}
        \item A4 is preferable when there is a sizable group of units of never-treated, and similar enough to the treated units
        \item without treatment anticipation $(\delta = 0)$, A5 restricts observed pre-treatment trends across groups. This could be an issue for A5 when the early pre-treatment periods are different form late-treatment periods
        \item As $\delta$ increase (more anticipation), the contitional parallel trend assumptions become stronger
    \end{itemize}
    \begin{itemize}
        \item[6] \textbf{Overlapping}: a positive fraction of population starts treatment in period $a$, and $\forall a,t$, propensity score is bounded aways from 1.
    \end{itemize}
\end{assumption}

Under A1-6 of Assumption \ref{assump:cs21assumptions}, 3 different estimands, outcome regression, inverse probability weighting, doubly robust, can all recover the group-time ATT.
There are several remarks:
\begin{itemize}
    \item the time period $t=a-\delta -1$ is used as an appropriate reference time: the most recent time period when untreated potential outcomes are observed in group $a$
    \item never-treated units can be used as a fixed comparision group for all treated units, but under A5, the ATT for the last treated group can not be estimated since there is no control group for it.
    \item when A3 and A4 hold \textbf{unconditionally} and there is no-anticipation, then one can subset the data to \textbf{only} contain observations at $t$ and $a-1$, from group $a$ and never-treated units, then run on ths subset $$ Y =\alpha_1^{a,t}+\alpha_2^{a,t}\cdot \mathbf{1}(a) + \alpha^{a,t}_3 \cdot \mathbf{1}(T) + \beta^{a,t}\cdot \mathbf{1}(a)\times \mathbf{1}(T) + \epsilon^{a,t} $$ where $\beta^{a,t} = ATT(a,t)$
    \item when there are covariates, control them in the model yields $\tilde{\beta}^{a,t}$, which is generally not equal to $ATT(a,t)$, unless the treatment effects are homogeneous in covariates, and there are not covariate-specific trends 
    \item DR estimators are more robust than IPW and OR estimands (larger standard errors)
    \item usually $ATT(a,t)$ can only be identified until $t=\mathcal{T}-\delta$ due to participation of treatments, but $ATT(a,t)$ can be identified up to $t=\mathcal{T}$ if some units are known to never participate in the treatment.
\end{itemize}

Next, \citet{callaway2021difference} propose an aggregation scheme of the form $$ \theta = \sum_{a}\sum^{\mathcal{T}}_{t=2}\omega(a,t)\cdot ATT(a,t) $$
where $\omega(a,t)$ are weighting functions and focus on three questions
\begin{itemize}
    \item[a] \textbf{event-study style}: how the treatment effect varies with length of exposure to the treatment 
    \begin{itemize}
        \item composition-changing aggregation $$ \theta_{es}(e) = \sum_{a}\mathbf{1}\left\{a+e\leq \mathcal{T}\right\} P\left(a\mid a+e\leq \mathcal{T}\right)\cdot ATT(a,a+e) $$ which is the average effect of participating in the treatment $e$ periods after the treatment across all groups observed to have participated in the treatment for $e$ periods. $\theta_{es}(e)$ across different $e$ can not be interpreted as the actual dynamic effects unless $ATT(a,a+e)$ is homogeneous across groups for all $e$, which is too strong of an assumption
        \item composition-constant aggregation $$ \theta_{es}^{bal}(e;e') = \sum_a \mathbf{1}\left\{a+e'\leq \mathcal{T}\right\}P\left(a\mid a+e'\leq \mathcal{T}\right)\cdot ATT(a,a+e) $$ which limits the aggregation to the groups participate in the treatment for at least $e'$ periods. This aggregation use fewer groups for estimate, hence the inference is \textbf{noisier}.
    \end{itemize}
    \item[b] how earlier-treated groups are different from the later-treated groups: consider $$ \theta_{sel}\left(\tilde{a}\right) = \frac{1}{\mathcal{T}-\tilde{a}+1} \sum^{\mathcal{T}}_{t=\tilde{a}} ATT\left(\tilde{a},t\right) $$ this averages the treatment effect for group $\tilde{a}$ across \textbf{all} post-treatment periods
    \item[c] the calendar-time ATE and cumulative ATE across all groups: for treatmented groups in period $t$ that have treated by $t$, $$ \theta_c\left(\tilde{t}\right) = \sum_{a}\mathbf{1}\left\{\tilde{t}\geq a\right\} P\left(a\mid a\leq \tilde{t}\right) ATT\left(a,t\right)$$
    and cumulatively, the ATE among units that have been treated by $\tilde{t}$ is $$ \theta_c^{cum}\left(\tilde{t}\right) =\sum^{\tilde{t}}_{t=2}\theta_c (t) $$
\end{itemize}

And one can aggregate group-time ATE into an overall effect of treatment:
\begin{itemize}
    \item \textbf{navie} aggregation, weighted by group size $$ \theta_W^O = \frac{ \sum_a\sum^{\mathcal{T}}_{t=2}\mathbf{1}\left\{t\geq a\right\} ATT(a,t)P(a\mid a\leq \mathcal{T}) }{ \sum_a\sum^{\mathcal{T}}_{t=2}\mathbf{1}\left\{t\geq a\right\} P(a\mid a\leq \mathcal{T}) } $$ which guarantees non-negative weights unlike TWFE. But mechanically, more weights are put on groups that participate in the treatment for longer.
    \item \textbf{across time within group, then across group} aggregation $$ \theta^O_{sel} = \sum_a\theta_{sel}(a) P(a\mid  a\leq \mathcal{T}) $$ which is recommended by \citet{callaway2021difference}, resembling the interpretation of the canonical DiD.
    \item \textbf{across group, then across time} aggregation
    \begin{align*}
        \theta^O_{es} &= \frac{1}{\mathcal{T}-1}\sum^{\mathcal{T}-2}_{e=0}\theta_es(e) & \theta^O_{c} &= \frac{1}{\mathcal{T}-1}\sum^{\mathcal{T}}_{t=2}\theta_c(t) & \theta^{O,bal}_{es}(e') &= \frac{1}{e'+1}\sum^{e'}_{e=0}\theta^{bal}_{es}(e,e')
    \end{align*}
\end{itemize}
and in general, these aggregations give different results, unless $ATT(a,t)$ is homogeneous across all $a$ and $t$.

\paragraph*{\citet{sun2021estimating}}

\citet{sun2021estimating} propose to compare the change for a group that adopted in period $t$, $\bar{Y}_{t,a}-\bar{Y}_{a-1,a}$, relative to the change for the never-adopters over the same period 
$$ \hat{\tau}^{a,\infty}_{t,a-1} = \left(\bar{Y}_{t,a}-\bar{Y}_{a-1,a}\right) - \left(\bar{Y}_{t,\infty},\bar{Y}_{a-1,\infty}\right) $$
and a simple unweighted average over the periods $t$ after the first period
$$ \hat{\tau} = \sum^T_{t=2}\sum^t_{a=2}\hat{\tau}^{a,\infty}_{t,a-1} \cdot \frac{\Pr \left(A_i=a\mid 2\leq A_i \leq t\right)\cdot \mathbf{1}_{2\leq a\leq t\leq T} }{T-1} $$

\begin{assumption}{\citet{sun2021estimating}'s assumptions}{sa2021assump}
    \begin{itemize}
        \item[A1] parallel trend
        \item[A2] no anticipation
        \item[A3] homogeneous treatment effect path across $a$   
    \end{itemize}
\end{assumption}

The main issue discussed by \citet{sun2021estimating} is the contamination that treatment effects from other periods affect the estimate for agiven $\beta_a$. 
For units $l$ periods aways from treatment adoption $a$ at period $t$, define $D^l_t \coloneq \mathbf{1}\left(t-a=l\right)$, then the relative period bin indicator is $$ \mathbf{1}(t-a\in L) = \sum_{l\in L}\mathbf{1}(t-a=l) = \sum_{l\in L}D^l_{t} $$
and 3 specifications can be represented as:
\begin{align*}
    Y_{i,t} &= \alpha_i + \lambda_t + \mu_l \sum_{l\geq 0} D^l_{i,t} + \nu_{i,t} & \text{static} \\
    Y_{i,t} &= \alpha_i + \lambda_t + \sum^{-2}_{l=-\underline{l}} \mu_l D^l_{i,t} + \sum^{\bar{l}}_{l=0}\mu_l D^l_{i,t} + \nu_{i,t} & \text{full dynamic} \\
    Y_{i,t} &= \alpha_i + \lambda_t + \beta\cdot \sum_{l< -\underline{l}} D^l_{i,t} + \sum^{-2}_{l=-\underline{l}} \mu_l D^l_{i,t} + \sum^{\bar{l}}_{l=0}\mu_l D^l_{i,t} + \gamma \cdot \sum_{l>\bar{l}} D^l_{i,t} + \nu_{i,t} & \text{binned dynamic}
\end{align*}
then the population regression coefficient on relative period bin $L$ is a linear combination of differences in trends from its own relative period $l\in L$, from relative periods $l\in L'$ in other bins, and from relative periods excluded from the specification $l\in L^{excl}$
\begin{align*}
    \mu_L = & \sum_{l\in L}\sum_a \omega^{L}_{a,l}\left( \mathbb{E}\left[Y_{i,a+l}-Y^{\infty}_{i,0}\mid a \right] - \mathbb{E} \left[Y^{\infty}_{i,a+l}-Y^{\infty}_{i,0}\right] \right) \\
    &+ \sum_{L'\neq L} \sum_{l\in L'}\sum_a \omega^{L}_{a,l} \left( \mathbb{E}\left[Y_{i,a+l}-Y^{\infty}_{i,0}\mid a\right] - \mathbb{E}\left[Y^{\infty}_{i,a+l}-Y^{\infty}_{i,0}\right] \right) \\
    &+ \sum_{l\in L^{excl}} \sum_a \omega^{L}_{a,l} \left(\mathbb{E}\left[Y_{i,a+l}-Y^{\infty}_{i,0}\mid a \right] - \mathbb{E}\left[Y^{\infty}_{i,a+l}-Y^{\infty}_{i,0}\right] \right)
\end{align*}
and the weights $\omega_{a,l}^L$ satisfy:
\begin{itemize}
    \item for periods of \textbf{own bin} $l\in L$: $\sum_{l\in L}\sum_a \omega^L_{a,l}=1$
    \item for periods of \textbf{other bins} $l\in L',L'\neq L$: $\sum_{l\in L }\sum_a \omega ^L_{a,l}=0$
    \item for excluded periods $l\in L^{excl}$: $\sum_{l\in L^{excl}} \sum_a \omega^L_{a,l}=-1$
    \item for never-treats: $\omega^L_{\infty,l}=0$
\end{itemize}
then under \textbf{A1 (parallel trend)} of Assumption \ref{assump:sa2021assump},
$$ \mu_L = \sum_{l\in L}\sum_a \omega^{L}_{a,l} CATT_{a,l} + \sum_{L'\neq L} \sum_{l\in L'}\sum_a \omega^{L}_{a,l} CATT_{a,l} + \sum_{l\in L^{excl}} \sum_a \omega^{L}_{a,l} CATT_{a,l} $$
since we have $$ \mathbb{E}\left[Y_{i,a+l}-Y^{\infty}_{i,0}\mid a \right] - \mathbb{E} \left[Y^{\infty}_{i,a+l}-Y^{\infty}_{i,0}\right] = CATT_{a,l} + \underbrace{\mathbb{E}\left[Y_{i,a+l}-Y^{\infty}_{i,0}\mid a \right] - \mathbb{E} \left[Y^{\infty}_{i,a+l}-Y^{\infty}_{i,0}\right]}_{=0} $$
there are still two issues: (1) $\mu_L$ is an average of not only $CATT_{a,l}$ of own bin $l\in L$, but also from other periods.

And under \textbf{A1 (parallel trend)} and \textbf{A2 (no anticipation)} of Assumption \ref{assump:sa2021assump}, 
$$ \mu_L = \sum_{l\in L,l>0}\sum_a \omega^{L}_{a,l} CATT_{a,l} + \sum_{L'\neq L} \sum_{l\in L',l>0}\sum_a \omega^{L}_{a,l} CATT_{a,l} + \sum_{l\in L^{excl},l>0} \sum_a \omega^{L}_{a,l} CATT_{a,l} $$
where only post-treatment $CATT$s enter. This challenges the pre-trend testing since all pre-treatment effects are zero $CATT_{a,l<0}=0$.

Under \textbf{A1 (parallel trend)} and \textbf{A3 (treatment effect homogeneity)}, then $CATT_{a,l}=ATT_l$ is constant across $a$ for given $l$, then 
$$ \mu_L = \sum_{l\in L}\omega^L_l ATT_l + \sum_{L'\neq L}\sum_{l\in L'}\omega^L_{l}ATT_l + \sum_{l\in L^{excl}}\omega^L_{l}ATT_l $$

These results show that even under the strong assumption of homogeneity, the contamination of treatments from other periods still exists. To tackle this issue, \citet{sun2021estimating} propose to estimate a weighted average of $CATT_{a,l}$ with reasonable weights (sum to 1 and non-negative) that are shares of cohorts that experience at least $l$ periods relative to treatment, normalized by the size of $L$ 
\begin{align*}
    v_L = \frac{1}{\left\vert L \right\vert } \sum_{l\in L}\sum_a CATT_{a,l} \Pr \left\{a \mid a \in [-l,T-l] \right\}
\end{align*}
the estimation procedure is 
\begin{itemize}
    \item[1] estimate $CATT_{a,l}$ with TWFE that interacts relative period indicators with cohort indicators, excluding cohort indicators from some set $C$
    $$ Y_{i,t} = \alpha_i + \lambda_t + \sum_{a\not \in C}\sum_{l\neq -1} \delta_{a,l}\left(\mathbf{1}\left\{a\right\}\cdot D^l_{i,t}\right) + \epsilon_{i,t} $$
    $C=\left\{\infty\right\}$ if a never-treated cohort is available.
    \item[2] estimate weights $\Pr \left\{a\mid a \in [-l,T-l]\right\}$ by sample shares of each cohort in the relevant periods $l\in L$
    \item[3] form the IW estimator as $$ \hat{v}_L = \frac{1}{\left\vert L \right\vert } \sum_{l\in L}\sum_a \hat{\delta}_{a,l} \hat{\Pr} \left\{a \mid a \in [-l,T-l] \right\} $$
\end{itemize}
One can also estimate $CATT_{a,l}$ via a DiD approach: assume $\exists$ pre-period $s<a$, then DiD estimate of $CATT_{a,l}$ can be defined as 
\begin{align*}
    \hat{\delta}_{a,l} = \frac{\mathbb{E}_N \left[ \left(Y_{i,a+l}-Y_{i,s}\cdot \mathbf{1}\left\{a\right\}\right) \right]}{\mathbb{E}_N\left[\mathbf{1}\left\{a\right\}\right]} - \frac{\mathbb{E}_N \left[ \left(Y_{i,a+l}-Y_{i,s}\cdot \mathbf{1}\left\{C\right\}\right) \right]}{\mathbb{E}_N\left[\mathbf{1}\left\{C\right\}\right]}
\end{align*}
where $C\subseteq \left\{c:a+l< c\leq T\right\}$ are non-empty control cohorts. If $s=a-1$, then the DiD estimator is equivalent to the Step 1 estimator.
Under A1 and A2 of Assumption \ref{assump:sa2021assump}, the DiD estimator using any pre-period $s<a$ and non-empty control cohorts $C$ is unbiased and consistent, there is a trade-off between A1 and A2:
\begin{itemize}
    \item if A2 is more likely to hold: choose any $s<a$ and include not-yet-treated units in control cohorts $C=\left\{c:c>a+l\right\}$, which relaxes the parallel trend assumption to be $$ \mathbb{E}\left[Y^{\infty}_{i,a+l},Y^{\infty}_{i,0} \mid a \right] = \mathbb{E}\left[Y^{\infty}_{i,a+l},Y^{\infty}_{i,0} \mid >a+l \right] $$
    \item if A2 is relaxed, the the parallel trend assumption needs to hold
\end{itemize}

\paragraph*{\citet{de2020two}} focus on one-period ahead double differences, with control groups that adopt later $a>t$: $$ \hat{\tau}^{t,a}_{t,t-1} = \left(\bar{Y}_{t,t}-\bar{Y}_{t-1,t}\right) - \left(\bar{Y}_{t,a}-\bar{Y}_{t-1,a}\right) $$
and aggregate these by averaging over all groups that adopt later: $$\hat{\tau}_{+,t} = \frac{1}{T-(a-1)} \sum_{a>t} \hat{\tau}^{t,a}_{t-1,t} $$
and then average over time periods, weighted by the fraction of adopters in each period $$ \hat{\tau} = \sum^T_{t=2} \hat{\tau}_{+,t} \cdot \Pr \left(A_i=a\mid A_i\geq 2\right) $$
by limiting the comparsons to those that are separated by \textbf{a single period}, the standard errors may be larger. The additivity assumption is more likely over such short horizons, but it also increases sensitivity to dynamic effects.

\paragraph*{\citet{borusyak2024revisiting}} focus on a model richer than TWFE model: $$ Y_{it}(0) = A^T_{it}\lambda_i + X_{it}^T\delta +\epsilon_{it} $$
where $A_{it}$ and $X_{it}$ are observed covariates, leading to a factor-type structure\footnote{For $A_{it}\equiv 1$ and $X_{it}\equiv \left(\mathbf{1}_{t=1},\cdots,\mathbf{1}_{t=T}\right)$}. 
\citet{borusyak2024revisiting} propose estimating $\lambda_i$ and $\delta$ by least squares using observations for control units only, and later construct \textbf{unit-time} specific imputations for treated units $$ \hat{\tau}_{it} = Y_{it} - A^T_{it}\hat{\lambda}_{i} + X^{T}_{it}\hat{\delta} $$
which can then be aggregated into an estimator of interest. Even each unit-time specific treatment effect estimator $\hat{\tau}_{it}$ might be inconsistent, the aggregated estimator is well-behaved.

\subsubsection{Comparision of Estimators}
Out of the four alternatives: \citet{callaway2021difference}'s $\hat{\tau}^{CS}$, \citet{sun2021estimating}'s $\hat{\tau}^{SA}$, \citet{de2020two}'s $\hat{\tau}^{CH}$ and \citet{borusyak2024revisiting}'s $\hat{\tau}^{BJS}$, which should we use?
One should consider the four arguments:
\begin{itemize}
    \item[i] how different is the never-adopter group 
    \item[ii] for long differences, how additive and stable over time are the differences between units
    \item[iii] dynamic effect: how one-period differences are different from multiple-period differences 
    \item[iv] efficiency 
\end{itemize}
\citet{arkhangelsky2023causal} recommended to text the presence of sysmteric variation in $\hat{\tau}^{a,a'}_{t,s}$ by adoption date $a$, by the length of the period between before and after $t-s$, and the time since adoption $t-a$, instead of reporting all four.

\subsection{Synthetic Control Methods}
\subsubsection{Setting}
Consider data for $J+1$ units over $T$ periods: $j=1,2,\cdots,J+1$, w.l.o.g., assume the first unit $(j=1)$ is the treated unit, and the first $T_0$ periods are before the intervention. For each unit $j$, we observe outcome $Y_{jt}$ and predictor vector $\mathbf{X}_{j}$, define the true treatment effect as 
\begin{align*}
    \tau_{1t} = Y^I_{1t} - Y^N_{1t}
\end{align*}
where $Y^{I}_{1t}$ is the observed treated outcome, $Y^{N}_{1t}$ is the unobservable counterfactual outcome. A synthetic control can then be presented by a $J\times 1$ vector of weights $\mathbf{W}=\left(\omega_2,\cdots,\omega_{J+1}\right)'$, and the synthetic control estimators are then 
\begin{align*}
    \hat{Y}^N_{1t} &= \sum^{J+1}_{j=2}\omega_j Y_{jt} & \hat{\tau} &= Y_{1t}-\hat{Y}^N_{1t}
\end{align*}
and the weights $\omega_j$ are nonnegative and summing to 1, excluding extrapolation and generally grant sparsity.
One way to choose $\mathbf{W}$ to resemble the pre-intervention values for the treated unit of predictors of the outcome variable. 
That is, for nonnegative $v_1,\cdots,v_k$, to \textbf{minimize}
\begin{equation}\label{eq:synthetic_control_weight}
    \mathbf{W}^* = \arg\min_{\mathbf{W}} \left\Vert \mathbf{X}_1 - \mathbf{X}_0\mathbf{W} \right\Vert = \arg\min_{\omega_{j}} \sqrt{ \sum^k_{h=1}v_h \left(X_{h1}-\omega_2 X_{h2} - \cdots - \omega_{J+1} X_{hJ+1} \right)^2 }
\end{equation}
then the estimated treatment effect at $t=T_0+1,\cdots,T$ is $$ \hat{\tau}_{1t} = Y_{1t} - \sum^{J+1}_{j=2}\omega^*_j Y_{jt} $$
the positive constants $v_1,\cdots,v_k$ reflect the relative importance of the synthetic control reproducing the values of each of the $k$ predictors for the treated unit $X_{11},\cdots,X_{k1}$.
This is essentially a double-minimizing procedure: 
\begin{itemize}
    \item for each given set of weights $\mathbf{V}=\left(v_1,\cdots,v_k\right)$, there is a $\mathbf{W}^*\left(\mathbf{V}\right)$ that solves Equation \ref{eq:synthetic_control_weight}
    \item choose the $\mathbf{V}^*$ among all possible $\mathbf{V}$:
    \begin{itemize}
        \item simple selector: $v_h$ as the \textbf{inverse} of the variance of $X_{h1},\cdots,X_{hJ+1}$, which rescales all rows of to have unit variance
        \item mean squared prediction error (\textbf{MSPE}) selector:
        \begin{equation*}
            \mathbf{V}^* = \arg\min_{\mathbf{V}} \sum_{t\in\mathcal{T}_0} \left( Y_{1t}-\omega_2\left(\mathbf{V}\right)Y_{2t} - \cdots - \omega_{J+1}\left(\mathbf{V}\right)Y_{J+1t} \right)^2
        \end{equation*}
        for some set $\mathcal{T}_0\subseteq \left\{1,2,\cdots,T_0\right\}$ of the pre-treatment periods.
        \item out-of-sample validation selector: 
        \begin{itemize}
            \item[1] divide the pre-treatment periods into a \textit{training} period $\{1,\cdots,t_0\}$ and a \textit{validation} periods $\{t_0+1,\cdots,T_0\}$
            \item[2] for a $\mathbf{V}$, let $\tilde{\omega}_2\left(\mathbf{V}\right),\cdots,\tilde{\omega}_{J+1}\left(\mathbf{V}\right)$ be the weights computed with training period data, apply the MSPE selector on the validation period data as $$ \mathbf{V}^* = \arg\min_{\mathbf{V}} \sum_{t\in\mathcal{T}_0} \left( Y_{1t}-\tilde{\omega}_2\left(\mathbf{V}\right)Y_{2t} - \cdots - \tilde{\omega}_{J+1}\left(\mathbf{V}\right)Y_{J+1t} \right)^2 $$
            \item[3] calculate $\mathbf{W}^* = \mathbf{W}\left(\mathbf{V}^*\right)$ for the \textbf{last $t_0$ periods} before the intervention $t=\left\{T_0-t_0+1,\cdots,T_0\right\}$ 
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsubsection{Bias bounds}
Consider the linear factor model (a generalization of DiD) for $Y_{jt}^N$: $$ Y_{jt}^N = \delta_t + \boldsymbol{\theta}_t\mathbf{Z}_j + \boldsymbol{\lambda}_t \boldsymbol{\mu}_j + \epsilon_{jt} $$
where $\delta_t$ is a time trend; $\mathbf{Z}_j$ and $\boldsymbol{\mu}_j$ are vectors of observed and unobserved predictors of $Y^N_{jt}$\footnote{If restricting $\boldsymbol{\lambda}_t\equiv \boldsymbol{\lambda}$, it reduces to DiD/TWFE.}.

When the synthetic control reproduces the characteristics of the treated unit, let $\mathbf{X}_1$ be the vector that includes $\mathbf{Z}_1$ and the pre-intervention outcomes for the treated unit, and let $\mathbf{X}_0$ be the matrix that collects the same variables for the untreated units, and suppose $\mathbf{X}_1=\mathbf{X}_0\mathbf{X}^*$,
then the bias of $\hat{\tau}_{it}$ is controled by $\epsilon_{it}$ and $T_0$:
\begin{itemize}
    \item small $\epsilon_{it}$ or large $T_0$: smaller bias
    \item large $\epsilon_{it}$ or small $T_0$: larger bias\footnote{The intuition: The synthetic control matches $\mathbf{Z}_1$, on the other hand, $\boldsymbol{\mu}_1$ cannot be matched (unobserved), only when differences in the values of the individual transitory shocks between the treated and the synthetic cnotrols compensate for the differences in unobserved factor loadings $\boldsymbol{\mu}_j$.} 
\end{itemize}
In practice, one should 
\begin{itemize}
    \item \myhl[myblue]{check the fitting}: if $\mathbf{X}_1 - \mathbf{X}_0\mathbf{W}^*$ is \textbf{large}, synthetic is substantially biased 
    \item \myhl[myblue]{check the $T_0$}: if the fitting is good, increasing $T_0$ reduces the bias, but if the fitting is bad, infinite $T_0$ won't eliminate bias
    \item \myhl[myblue]{check the size of pool}: a larger $J$ increase the risk of over-fitting since it gets easier to fit pretreatment outcomes\footnote{In high-dimensional setting $J\rightarrow\infty$ and $T_0\rightarrow \infty$, synthetic control estimators may be asymptotically unbiased.}
    \item \myhl[myblue]{check linearity}: if $Y^N_{it}$ is determined nonlinearly, even a good fit cannot rule out bias
\end{itemize}
\citet{abadie2010synthetic} derived the bias bound as $$ C(p)^{1/p} \left( \frac{\bar{\lambda}^2 F}{\underline{\xi}} \right) J^{1/p} \max  \left\{ \frac{\bar{m}^{1/p}_p}{T^{1-1/p}_0},\frac{\bar{\sigma}}{T^{1/2}_0} \right\} $$
which also increases with the number of components in $\boldsymbol{\mu}_j$ (the number on unobserved factors).

\subsubsection{Variable selection}
How to choose the predictors $\mathbf{X}_1$ and $\mathbf{X}_0$ is a fundamental task for the synthetic control method. Typically, $\mathbf{X}_1$ and $\mathbf{X}_0$ include 
\begin{itemize}
    \item pre-intervention values of outcome variable $\mathbf{Y}$
    \item other predictors $\mathbf{Z}_j$: excluded $\mathbf{Z}_j$ become part of $\boldsymbol{\mu}_j$, which increase the bound of bias
\end{itemize}
one can use data-driven methods for variable selection by measuring the predictive power of alternative sets of variables on the pre-intervention training and validation periods.

\subsubsection{Inference}
Consider the test statistic that measures the ratio of the post-intervention fit relative to the pre-intervention fit. For $0\leq t_1\leq t_2\leq T$ and $j=\left\{1,\cdots,J+1\right\}$, let 
\begin{equation*}
    R_j\left(t_1,t_2\right) = \sqrt{\frac{1}{t_2-t_1+1}\sum^{t_2}_{t=t_1} \left(Y_{jt}-\hat{Y}^N_{jt}\right)^2}
\end{equation*}
where $\hat{Y}^N_{jt}$ is the outcome on $t$ produced by a synthetic control when unit $j$ is coded as treated and using all other $J$ units to construct the donor pool\footnote{This is the root mean squared prediction error (RMSPE) of the synthetic control estimator for unit $j$ and time periods $t_1,\cdots,t_2$.}. The ratio between the post-intervention RMSPE and pre-intervention RMSPE for unit $j$ is 
$$ r_j = \frac{R_j(T_0+1,T)}{R_j(1,T_0)} $$
$r_j$ measures the quality of the fit of a synthetic control for unit $j$ in the post-treatment period, relative to the quality of the fit in the pre-treatment period. One then use the permutation distribution of $r_j$ for inference. the $p-$value for the inferential procedure based on the permutation distribution of $r_j$ is given by $$ p = \frac{1}{J+1}\sum^{J+1}_{j=1}I_{+}\left(r_j-r_1\right) $$ 
where $I_{+}(\cdot)$ is an indicator function that returns one for nonnegative arguments and 0 otherwise\footnote{Replace $Y_{jt}-\hat{Y}_{jt}^N$ in $R(T_0+1,T)$ with their positive or negative parts $\left(Y_{jt}-\hat{Y}_{jt}^N\right)^+$ or $\left(Y_{jt}-\hat{Y}_{jt}^N\right)^-$, leads to one-sided inference, this may increase the power substantially, very important in many cases where samples are considerably small.}.
This permutation method is not trying to approximate the sampling distributions of test statistics, instead, the design-based inference focuses on the variation in the test statistic induced by the assignment mechanism.

\subsubsection{Pros and limitations}

Relative to regression-based counterfactual, synthetic controls have the following advantage:
\begin{itemize}
    \item \myhl[myblue]{\textbf{No extrapolation}}: the synthetic control weights are non-negative and sum to 1.
    \item \myhl[myblue]{\textbf{Transparency of the fit}}: synthetic controls make transparent the actual $\mathbf{X}_1-\mathbf{X}_0\mathbf{W}^*$
    \item \myhl[myblue]{\textbf{Safeguard against specification searches}}: similar to classic matching methods, synthetic controls do not require access to post-treatment outcomes in the design phase, hence the synthetic control weights can be preregistered
    \item \myhl[myblue]{\textbf{Transparency of the counterfactuals}}: the contribution of each comparison unit is clear
    \item \myhl[myblue]{\textbf{Sparsity}}: synthetic controls are sparse, making the results easier to interpret
\end{itemize}

and the synthetic control method requires 
\begin{itemize}
    \item \myhl[myred]{\textbf{Size and volatility of the outcome}}: high level of volatility in outcome variables increase the risk of over-fitting, one should \textbf{apply filtering first} to remove the volatility first in both the exposed units and the donor pool\footnote{The challenge of volatility comes only from the fraction that generated by unit-specific factors.}.
    \item \myhl[myred]{\textbf{availability of the donor pool}}: units that adopt an intervention similar to the unit of interest should not be included in the donor pool; units that have experienced large idiosyncratic shocks should not be included; characteristics $\mathbf{Z}$ should also be included for matching to avoid interporlation\footnote{One should avoid averaging away large discrepancies between the characteristics of the affected unit and the characteristics of the units in the synthetic control.}.
    \item \myhl[myred]{\textbf{No anticipation}}: one should backdate to before anticipation, and since synthetic control estimator does not restrict the time variation in the effect of the intervention, one don't need to worry about backdating mechanically biases the estimator.
    \item \myhl[myred]{\textbf{No interference}}: in the design phase, one can drop spillover-affected donor units\footnote{There usually is a tradeoff between the no-interference and the availability requirement.}; in the analysis phrase, one should be aware of the potential direction of the bias due to spillover effects
    \item \myhl[myred]{\textbf{Convex hull condition}}: sometimes the treated unit might be too extreme in the values of the outcome variable pre-intervention to be approximated by a weighted average of untreated units. One can use L1 difference $\Delta Y_{jt} = Y_{jt}-Y_{jt-1}$ or growth rates $\Delta Y_{jt}/Y_{jt-1}$, or the change relative to pre-treatment means $\tilde{Y}_{jt} = Y_{jt} - \frac{1}{T_0}\sum^{T_0}_{h=1}Y_{jh}$\footnote{One should be cautious about transforming the outcome variables since: first sometimes credible counterfactuals require reproducing not only the trend but the level; second, it can substantially increase the bias.}.
\end{itemize}

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}