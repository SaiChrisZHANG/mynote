\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{6}{DID and TWFE}{}{Sai Zhang}{This note is on the causal panel data, building upon \citet{arkhangelsky2023causal}.}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Panel Data Configurations}

\subsection{Data Types}
\subsubsection{Panel Data}
For observations on $N$ units, indexed by $i=1,\cdots,N$, over $T$ periods, indexed by $t=1,\cdots,T$, the outcome of interest is denoted by $Y_{it}$, the treatment $W_{it}$.
These observations may themselves consist of averages over more basic units:
\begin{align*}
    \mathbf{Y} &= \begin{pmatrix}
        Y_{11} & \cdots & Y_{1T}\\
        \vdots & \ddots & \vdots \\
        Y_{N1} & \cdots & Y_{NT}
    \end{pmatrix} &
    \mathbf{W} = \begin{pmatrix}
        W_{11} & \cdots & W_{1T}\\
        \vdots & \ddots & \vdots \\
        W_{N1} & \cdots & W_{NT}
    \end{pmatrix}
\end{align*}
we may also observe exogenous variables $X_{it}$ or $X_i$. Typically, we focus on a balanced panel where for all units $i=1,\cdots,N$ we observe outcomes for all $t=1,\cdots,T$.

\subsubsection{Grouped Repeated Cross-Section Data}
In a GRCS data, we have observations on $N$ units, each observed only once in period $T_i$ for unit $i$. Different units may be observed at diffrent points in time, $T_i$ typically takes on only a few values, with many units sharing the same value for $T_i$. The outcome $Y_i$ and treatment $W_i$ are indexed by the unit index $i$.
The set of units is \textbf{partitioned} into 2 or more groups, with the group that unit $i$ belongs to denoted by $G_i\in \mathcal{G}=\left\{1,2,\cdots,G\right\}$.

Define the average outcomes for each group-time-period pair:
\begin{equation*}
    \bar{Y}_{gt} \equiv \frac{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}Y_i}{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}}
\end{equation*}
for treatment 
\begin{equation*}
    \bar{W}_{gt} \equiv \frac{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}W_i}{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}}
\end{equation*}
then treat the $G\times T$ group averages $\bar{Y}_{gt}$ and $\bar{W}_{gt}$ as the unit of observation, then the grouped data is just a panel.
The major issue in practice is that the number of groups is very small comparing to proper panel data.

\subsubsection{Row and Column Exchangeable Data}
The data are doubly indexed by $i=1,\cdots,N$ and $j=1,\cdots,J$, with outcomes $Y_{ij}$. They are different from panel data in that there is \textbf{no time ordering} for the second index. Many methods developed for panel data are also applicable here.

\subsection{Shapes of Data Frames}
Panel data can also be loosely classified by the shape:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Thin Frames} $(N\gg T)$}, where the number of cross-section units is large relative to the number of time periods:
    \begin{itemize}
        \item unit-specific parameters (individual FEs) \textbf{can not be estimated consistently} due to the short time series
        \item REs might be more suitable since they place a stocahstic structure on the individual components
    \end{itemize}
    \item \myhl[myblue]{\textbf{Fat Frames} $(N\ll T)$}, where the number of cross-section units is large relative to the number of time periods.
    \item \myhl[myblue]{\textbf{Square} $N\simeq T$}, where the number of units and time periods is comparable.
\end{itemize}

\subsection{Assignment Mechanisms}
\subsubsection{The General Case}
In the most general case, the treatment may vary both across units and over time, with units \textbf{switching} in and out of the treatment group:
\begin{equation*}
    \mathbf{W}^{\text{general}} = \begin{pmatrix}
        1&1&0&0&\cdots &1\\
        0&0&1&0&\cdots &0\\
        1&0&1&1&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        1&0&1&0&\cdots &0
    \end{pmatrix}
\end{equation*}
This is more relevant for the RCED configurations, and for panel data of products and promotions as treatments.
The assumption on the absence/presence of \textbf{dynamic treatment} effects is very important.

\subsubsection{Single Treated Period}
One special case arises when a substantial number of units is treated, but these units are only treated \textbf{in the last period}
\begin{equation*}
    \mathbf{W}^{\text{last}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&0&\cdots &1
    \end{pmatrix}
\end{equation*}
If $T$ is relatively small, this case is often analyzed as a cross-section problem, the lagged outcomes are used as exogenous covariates or pre-treatment variables to be adjusted.
Here, dynamic effects are not testable, nor do they matter since the shortness of the panel.
\begin{equation*}
    \mathbf{W}^{\text{last}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &0
    \end{pmatrix}
\end{equation*}
this setting is prominent in the original applications of the synthetic control literature, here $T$ is usually small.

\subsubsection{Single Treated Unit and Single Treated Period}
An extreme case is where only a single unit is treated, and it is only treated in a single period (typically the last). 
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&0&\cdots &1
    \end{pmatrix}
\end{equation*}
Normally, we focus on the effect for the single treated/time-period pair and construct prediction intervals.

\subsubsection{Block Assignment}
The case of block assignment is where a subset of units is treated every period after a common starting date:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&1&1&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &1
    \end{pmatrix}
\end{equation*}
There is typically a sufficient number of treated unit/time-period pairs to allow for reasonable approximations. The presence of dynamic effects change the interpretation of the average effect of the treated: the average effect for the treated now is an average over short \textbf{and} medium term effects during different periods.

\subsubsection*{Staggered Adoption (a.k.a. absorbing treatment setting)}
The staggered adoption is the case where units adopt the treatment at various period, and remain in the treatment group once they adopt the treatment:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &1\\
        0&0&0&1&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &1
    \end{pmatrix}
\end{equation*}
Here, with some assumptions, we can separate dynamic effects from heterogeneity across calendar time.

\subsubsection{Event Study Designs}
In the event-study design, units are exposed to the treatment in at most one period:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        1&0&0&0&\cdots &0\\
        0&1&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&1&\cdots &0
    \end{pmatrix}
\end{equation*}
There are often dynamic effects of the treatment past hte time of initial treatment, however, the effects might be changing over time.

\subsubsection{Clustered Assignment}
In many applications, units are grouped together in clusters. Units within the same clusters are always assigned to the treatment:
\begin{equation*}
    \mathbf{W}^{\text{cluster}} = \begin{pmatrix}
        &&&&&&&\text{cluster}\\
        0&0&0&0&\cdots &0&0&1\\
        0&0&0&0&\cdots &0&0&1\\
        0&0&0&0&\cdots &0&0&2\\
        0&0&0&0&\cdots &0&0&2\\
        0&0&0&0&\cdots &1&1&3\\
        \vdots&\vdots&\vdots&\vdots&\ddots &\vdots&\vdots&\vdots\\
        0&0&0&1&\cdots &1&1&C\\
        0&0&0&1&\cdots &1&1&C
    \end{pmatrix}
\end{equation*}
Clustering creates complications for inference.

\subsection{Outcomes, Assumptions and Estimands}
For a treatment assignment matrix $\mathbf{W}$, denote:
\begin{itemize}
    \item the full $T-$component column vector of treatment assignments as $$ \underline{\mathbf{w}} \equiv \left(w_1,\cdots,w_T\right)' $$
    \item the $t$-component column vector of treatment assignments \myhl[myblue]{\textbf{up to} time $t$} as $$ \underline{\mathbf{w}}^{t} \equiv \left(w_1,\cdots,w_t\right)' $$ hence $\underline{\mathbf{w}}^T=\underline{\mathbf{w}}$
    \item the row vector of treatment values for unit $i$ as $\underline{\mathbf{W}}_i$
\end{itemize}
Then in general, we can index the potential outcomes for unit $i$ in period $t$ by the full $T-$component vector of assignments $\underline{\mathbf{w}}$
$$Y_{it}\left(\underline{\mathbf{w}}\right)$$
A key underlying assumption is the \myhl[myblue]{\textbf{Stable Unit Treatment Value Assumption (SUTVA)}}, which requires that there is no interference or spillovers between units\footnote{SUTVA can hold on a cluster/group level, where the spillover effects are within clusters/groups.}.

In this setup, there are $2^T$ potential outcomes for each unit and each time period, as a function of multi-valued treatment $\underline{\mathbf{w}}$. Then, define for each $t$-unit treatment effects for each pair of assignment vectors $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$: $$ \tau^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}_{it}\equiv Y_{it}\left(\underline{\mathbf{w}}'\right) - Y_{it}\left(\underline{\mathbf{w}}\right) $$
and the corresponding population average effect $$ \tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}\equiv \mathbb{E}\left[Y_{it}\left(\underline{\mathbf{w}}'\right)-Y_{it}\left(\underline{\mathbf{w}}\right) \right] $$
where the expectation is implicitly assumed to be taken over a \textbf{large} population.

Under completely random assignment, all $\tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}$ are identified, and are \textbf{just-identified}, given sufficient variation in the treatment paths. Dynamic treatment effects can also be identified\footnote{For example, consider that in the 2-period case $$ \tau_2^{(1,1),(0,1)} $$ is the average effect in the second period of being exposed to $(1,1)$, \textit{treated in both period}, rather than $(0,1)$, \textit{treated only in the second period}.}.
However, we have \myhl[myblue]{$2^{T-1}\times \left(2^T-1\right)$} distinct average effects of the form $\tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}$,
in practice, we often need to focus on summary measures of these causal effects, which requires some addition assumptions:

\begin{assumption}{No Anticipation}{no_anticipation}
    The potential outcomes satisfy $$ Y_{it}\left(\underline{\mathbf{w}}\right) = Y_{it}\left(\underline{\mathbf{w}}'\right)$$ for all $i$, and for all combinations of $t$, $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$ such that $\underline{\mathbf{w}}t=\underline{\mathbf{w}}'^{t}$. 
\end{assumption}
This is a testable assumption with experimental data and sufficient variation in treatment paths, by comparing units that have the same treatment path up to and including $t$ and diverge after $t$.
\begin{itemize}
    \item \underline{Units \textbf{are not} active decision-makers}: the assumption can be guaranteed by design (random treatment assignment each period, or staggered adoption with randomly assigned adoption date)
    \item \underline{\textbf{Limited} antici}p\underline{ation}: assuming the treatment can be anticipated for a \textbf{fixed number} of periods, which shifts $\underline{\mathbf{w}}$ by that number of periods.
    \item \underline{Units \textbf{are} active decision-makers}: potential outcomes are functions of $\underline{\mathbf{w}}$ and the distribution of $\underline{\mathbf{w}}$ (experimental design itself):
    \begin{itemize}
        \item one can define potential outcomes for a given randomized experimental design: the beliefs about the future treatment paths are incorporated in the definition of the potential outcomes, the actual values are by construction unknown. This does change the interpretation of the casual effects\footnote{Think about the differences between a surprise deviation from a given policy rule versus the effect of a permanent chagne int he policy rule itself.}.
        \item In obserational studies, one cannot directly control the information about the future treatment paths. In this case, different units need to be gauranteed to face the \textbf{same information environment} for Assumption \ref{assump:no_anticipation} to hold.
    \end{itemize}
\end{itemize}
Under Assumption \ref{assump:no_anticipation}, the total number of potential treatment effects is reduced from $2^{T-1}\times \left(2^T-1\right)$ to $\left(\sum^T_{t=1}2^{t-1}\right)\left(\sum^T_{t=1}2^t-1\right)$. The unit-period specific treatment effects are now of the type 
\begin{equation*}
    \tau^{\underline{\mathbf{w}}^t,\underline{\mathbf{w}}^{t'}}_{it} \equiv Y_{it}\left(\underline{\mathbf{w}}^{t'}\right) - Y_{it}\left(\underline{\mathbf{w}}^t\right)
\end{equation*}
with the potential outcomes for period $t$ indexed by treatments up to period $t$ only. Here, one can still distinguish
\begin{itemize}
    \item static treatment effects: $\tau_{it}^{\left(\underline{\mathbf{w}}^{t-1},0\right),\left(\underline{\mathbf{w}}^{t-1},1\right)}$, which measures the response of current outcome to the current treatment, holding the past ones fixed.
    \item dynamic treatment effects: $\tau_{it}^{\left(\underline{\mathbf{w}}^{t-1},w^t\right),\left(\underline{\mathbf{w}}^{t-1},w^t\right)}$, which does the opposite.
\end{itemize}

\begin{assumption}{No Dynamic/Carry-Over Effects}{no_dynamic}
    The potential outcomes satisfy $$ Y_{it}\left(\underline{\mathbf{w}}\right) = Y_{it}\left(\underline{\mathbf{w}}'\right) $$
    for all $i$ and for all combinations of $t$, $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$ such that $w_{it}=w'_{it}$.
\end{assumption}
This assumption is \textbf{not} guaranteed by randomization. It restricts the treatment effects and the potential outcomes for the \textbf{post}-treatment periods.
It has testable restrictions given the random assignment of the treatment and sufficient variation in the treatment paths. It does \textbf{not} restrict the time path of the potential outcomes in the absence of any treatment $Y_{it}\left(\mathbf{0}\right)$.

This assumption greatly reduce the total number of treatment effects for each unit to $T$:
$$ \tau_{it}\equiv Y_{it}(1) - Y_{it}(0)$$
where $\tau_{it}$ has no superscripts because there are only 2 possible arguments of the potential outcomes $w\in \left\{0,1\right\}$.

\begin{assumption}{Staggered Adoption}{stag_adopt}
    In staggered adoption, $$W_{it}\leq W_{it-1},\ \forall t=2,\cdots,T$$
    define the adoption date $A_i$ as the date of the first treatment, $A_i\equiv T+1-\sum^T_{t=1}W_{it}$ for treated units, and $A_i\equiv \infty$ for never-treated units.
\end{assumption}
Under Assumption \ref{assump:stag_adopt}, the potential outcomes can be written in terms of the adoption date as $Y_{it}(a)$, for $a=1,\cdots,T,\infty$, and the realized outcome as $Y_{it}=Y_{it}\left(A_i\right)$. There are 2 broad classes of settings that are viewed as staggered adoption designs:
\begin{itemize}
    \item interventions adopted and remain in place 
    \item one-time interventions with a long-term, or even permanent, impact (where the post-intervention period effects are dynamic effects)
\end{itemize}
Under Assumption \ref{assump:stag_adopt}, but \textbf{not} Assumption \ref{assump:no_anticipation} and \ref{assump:no_dynamic}, we can write 
$$ \tau_{it}^{a,a'}\equiv Y_{it}\left(a'\right)-Y_{it}(a) $$
with the corresponding population average
$$ \tau_t^{a,a'}\equiv \mathbb{E}\left[ \equiv Y_{it}\left(a'\right)-Y_{it}(a) \right] $$
we can also denote the average for subpopulations conditional on the adoption dates as 
$$ \tau_{t\mid a''}^{a,a'}\equiv \mathbb{E}\left[\equiv Y_{it}\left(a'\right)-Y_{it}(a)\mid A_i=a''\right] $$
which explicitly depends on the details of the assignment process. This estimand is conceptually similar to the average effect on the treated in cross-sectional settings, but with selection operating over both unit and period dimensions.

\subsection{Conventional TWFE and DiD}
\subsubsection{TWFE Characterization}
First, consider a panel setting with no anticipation, no dynamics, and constant treatment effects:
\begin{assumption}{The TWFE Model}{twfe_model}
    The control outcome $Y_{it}(0)$ satisfies $$ Y_{it}(0) = \alpha_i + \beta_t + \epsilon_{it} $$
    The unobserved component $\epsilon_{it}$ is (mean-)independent of the treatment assignment $W_{it}$
\end{assumption}
And 
\begin{assumption}{Constant Static Treatment Effects}{const_stat_treat}
    The potential outcomes satisfy $$ Y_{it}(1) = Y_{it}(0) + \tau \ \ \forall (i,t) $$
\end{assumption}
Under Assumption \ref{assump:twfe_model} and \ref{assump:const_stat_treat}, for the realized $Y_{it}\equiv W_{it}Y_{it}(1) + \left(1-W_{it}\right)Y_{it}(0)$ we have a model 
$$ Y_{it} = \alpha_i + \beta_t +\tau W_{it}+\epsilon_{it} $$
then we can estimate the parameters of this model by least squares
\begin{equation*}
    \left(\hat{\tau}^{TWFE},\hat{\alpha},\hat{\beta}\right) = \arg\min_{\tau,\alpha,\beta} \sum^N_{i=1}\sum^T_{t=1} \left(Y_{it}-\alpha_i-\beta_t -\tau W_{it}\right)^2
\end{equation*}
one restriction on the $\alpha_i$ or $\beta_t$ needs to be imposed to avoid perfect collinearity, but this normalization does not affect the estimation of $\tau$.

Under a block assignment structure, we have $W_{it}=1$ only for a subset of the units\footnote{The \textit{treatment group} with $i\in \mathcal{J}$, where the cardinality for the set $\mathcal{J}$ is $N^{\mathrm{tr}}$ and $N^{\mathrm{co}}\equiv N-N^{\mathrm{tr}}$.}, and those units are treated only during periods $t$ with $t>T_0$.
Define the averages in the four groups as 
\begin{align*}
    \bar{Y}^{\mathrm{tr,post}} &\equiv \frac{\sum_{i\in\mathcal{J}}\sum_{t>T_0}Y_{it}}{N^{\mathrm{tr}}\left(T-T_0\right)} & \bar{Y}^{\mathrm{tr,pre}} &\equiv \frac{\sum_{i\in\mathcal{J}}\sum_{t\leq T_0}Y_{it}}{N^{\mathrm{tr}}T_0} \\
    \bar{Y}^{\mathrm{co,post}} &\equiv \frac{\sum_{i\not\in\mathcal{J}}\sum_{t>T_0}Y_{it}}{N^{\mathrm{co}}\left(T-T_0\right)} & \bar{Y}^{\mathrm{co,pre}} &\equiv \frac{\sum_{i\not\in\mathcal{J}}\sum_{t\leq T_0}Y_{it}}{N^{\mathrm{co}}T_0}
\end{align*}
and then write the estimator for the treatment effect as 
\begin{equation*}
    \hat{\tau}^{TWFE} = \left(\bar{Y}^{\mathrm{tr,post}}-\bar{Y}^{\mathrm{tr,pre}}\right) - \left(\bar{Y}^{\mathrm{co,post}}-\bar{Y}^{\mathrm{co,pre}}\right)
\end{equation*}

\subsubsection{DiD Estimator in the Grouped Repeated Cross-Section Setting}
In GRCS setting, we observe each physical unit only once. With blocked assignment, the notation only has a single index for the unit $i=1,\cdots,N$.
Let $G_i\in\mathcal{G}=\left\{1,\cdots,G\right\}$ denote the cluster or group unit $i$ belongs to, and $T_i\in \left\{1,\cdots,N\right\}$ the time period unit $i$ is observed in.

The set of clusters $\mathcal{G}$ is partitioned into two groups: control group $\mathcal{G}_C$ and treatment group $\mathcal{G}_T$, with cardinality $G_C$ and $G_T$. Only units with $G_i\in \mathcal{G}_T$, indicated by $D_i=\mathbf{1}_{G_i\in\mathcal{G}_T}$, are exposed to the treatment if they are observed after the treatment date $T_0$: $W_i=\mathbf{1}_{G_i\in\mathcal{G}_T,T_i>T_0}$

Assuming that the treatment within group and time period pairs is constant, the cluster/time-period average treatment $\bar{W}_{gt}$ is binary if the original treatment is. Then the DiD estimator is 
\begin{align*}
    \hat{\tau}^{DiD} =& \frac{1}{G_T\left(T-T_0\right)} \sum_{g\in\mathcal{G}_T,t>T_0} \bar{Y}_{gt} - \frac{1}{G_C\left(T-T_0\right)} \sum_{g\in\mathcal{G}_C,t>T_0}\bar{Y}_{gt}\\
    &- \frac{1}{G_TT_0}\sum_{g\in\mathcal{G}_T,t\leq T_0} \bar{Y}_{gt} + \frac{1}{G_CT_0}\sum_{g\in\mathcal{G}_C,t\leq T_0}\bar{Y}_{gt}
\end{align*}
and at the group level, we have a proper panel setup:
\begin{align*}
    \bar{Y}_{gt}(0) &= \alpha_g+\beta_t+\epsilon_{gt} & \bar{Y}_{gt}(1) &\bar{Y}_{gt}(0)+\tau
\end{align*}
and the potential outcomes $\bar{Y}_{gt}(0)$ and $\bar{Y}_{gt}(1)$ should be interpreted as the average of the potential outcomes if all units in a group/time-period pair are exposed to the control treatment.

\subsubsection{Inference}
There are two ways to conduct inference about $\hat{\tau}^{\mathrm{DiD}}$ and $\hat{\tau}^{\mathrm{TWFE}}$:
\begin{itemize}
    \item the assignment process is known: \myhl[myblue]{\textbf{design-based}} or \myhl[myblue]{\textbf{randomization-based}} inference
    \item otherwise: \myhl[myblue]{\textbf{sampling-based}} inference
\end{itemize}

\paragraph*{Design-Based Inference}


\paragraph*{Sampling-Based Inference}
\begin{itemize}
    \item \myhl[myblue]{\textbf{proper panel setting}}: it is often assumed that all units are randomly sampled from a large population and thus \textbf{exchangeable}. Inference about $\hat{\tau}^{\mathrm{TWFE}}$ reduces to joint inference about four means with i.i.d. observations.
    \item \myhl[myblue]{\textbf{GRCS setting}}: one can allow for non-vanishing errors at the group level, but it cannot be done in the two-group case.
\end{itemize}

\subparagraph*{Standard errors}
Regardless of the level of aggregation, inference for TWFE and DiD estimators typically takes into account the correlation in outcomes over time within units in applications with more than two periods. So it is \textbf{NOT} appropriate to use the robust Eicker-Huber-White standard errors. 
Instead, one should use clustered standard errors based on clustering observations by units. It can also be approximated by bootstrapping all observations for each unit.

\subsubsection{The Parallel Trend Assumption}

The \textbf{parallel trend assumption} is the fundamental justification for the DiD estimator. It states that the units who are treated would have followed a path that is parallel to the path followed by the control units on average, in the absence of the treatment.

\paragraph*{Proper panel settings} the assumption is that the expected difference in control outcomes in any period for units who later are exposed to the treatment and units who are always in the control group is \textbf{contstant}:
\begin{assumption}{Parallel Trend Assumption: Proper Panel}{parallel_trend_panel}
    For all $t,t'$,
    \begin{equation*}
        \mathbb{E}\left[Y_{it}\left(0\right)\mid D_i=1\right] - \mathbb{E}\left[Y_{it}\left(0\right)\mid D_i=0\right] = \mathbb{E}\left[Y_{it'}\left(0\right)\mid D_i=1\right] - \mathbb{E}\left[Y_{it'}\left(0\right)\mid D_i=0\right]
    \end{equation*}
    equivalently, we can formulate it in terms of changes over time\footnote{the expected change in control outcomes is the same for those who will eventually be exposed to the treatment and those who will not}:
    \begin{equation*}
        \mathbb{E}\left[Y_{it}(0)-Y_{it'}(0)\mid D_i=1\right] = \mathbb{E}\left[Y_{it}(0)-Y_{it'}(0)\mid D_i=0\right]
    \end{equation*}
    alternatively, postulate a TWFE model for the control outcomes, additionally assuming that the treatment assignment $D_i$ is independent of the vector of residuals $\epsilon_{it},t=1,\cdots,T$, conditional on FEs: 
    \begin{equation*}
        D_i \perp \left(\epsilon_{i1},\cdots,\epsilon_{iT}\right) \mid \alpha_i 
    \end{equation*}
\end{assumption}
From the point of view of the modern casual inference literature, the parallel trend assumption is non-standard in the sense that it combbines restrictions on the potential outcomes with restrictions on the assignment mechanism.

\paragraph*{GRCS settings} Suppose in the population, all groups are (infinitely) large in each period, and we have random samples from these populations for each period. Then the expectations are well defined as population averages.
The parallel trends assumption can the nbe formulated as requiring that the difference in expected control outcomes between two groups remains constant over time: 
\begin{assumption}{Parallel Trend Assumption: Grouped Repeated Cross-Section}{parallel_trend_grcs}
    For all pairs of groups $g,g'$ and for all pairs of time periods $t,t'$, the average difference between the groups remains the same over time, irrespective of their treatment status: 
    \begin{equation*}
        \mathbb{E}\left[Y_{gt}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{g't}(0)\mid D_i=0\right] = \mathbb{E}\left[Y_{gt'}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{g't'}(0)\mid D_i=0\right]
    \end{equation*}
    an alternative formulation is that expected change between periods $t'$ and $t$ is the same for all groups:
    \begin{equation*}
        \mathbb{E}\left[Y_{gt}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{gt'}(0)\mid D_i=1\right] = \mathbb{E}\left[Y_{g't}(0)\mid D_i=0\right] - \mathbb{E}\left[Y_{g't'}(0)\mid D_i=0\right]
    \end{equation*}
\end{assumption}
If $Y_{gt}(0)$ for all $g$ and $t$ are observed, the presence of the two groups and two time periods would be sufficient for the assumption to have testable implications. However, in the 2-group/2-period case, at least one of the four cells is exposed to the treatment, there are no testable restrictions implied by this assumption\footnote{If there are more than 2 periods, or more than 2 groups, there are testable restrictions by the parallel trend assumption.}.

\subsubsection{Pre-treatment Variables}

Time-invariant characteristics of the units in addition to the time path of the outcome are observed, these variables are colinear with the individual fixed effects $\alpha_i$ hence cannot be incorporated simply by adding them to the TWFE specification.
A reason one might want to include these pre-treatment variables is that the parallel trend and constant treatment effect assumptions hold only within subpopulations defined by them.

\paragraph*{Semi-parametric DiD}
\citet{abadie2005semiparametric} proposed a solution based on \textbf{re-weighting} the differences in outcomes by the propensity score for balance. TO estimate the average treatment effect on the treated (ATT):
\begin{equation*}
    ATT\equiv \mathbb{E}\left(\mathbf{y}_{1t}-\mathbf{y}_{0t}\mid \mathbf{d}=1\right)
\end{equation*}
where the 2 potential outcomes $\mathbf{y}_{1t}$ is the value of $\mathbf{y}$ if the participant received the treatment by $t$, $\mathbf{y}_{0t}$ is the value of $\mathbf{y}$ if the participant had not received the treatment by time $t$. $\mathbf{d}$ is an indicator of treatment.

ATT cannot be directly estimated since $\mathbf{y}_{0t}$, the counterfactual, is never observed. For a set of pretreatment characteristics $\mathbf{x}_{b}$, define the probability to be in the treatment group conditional on $\mathbf{x}_b$ as $$ \pi\left(\mathbf{x}_b\right)\equiv \mathbb{P}\left(\mathbf{d}=1\mid \mathbf{x}_b\right) $$
define the change of $\mathbf{y}$ from baseline $b$ to $t$ as $$\Delta \mathbf{y}_t \equiv \mathbf{y}_t - \mathbf{y}_b $$
then
\begin{equation}\label{eq:abadie_att_estimate}
    \mathbb{E}\left\{ \frac{\Delta \mathbf{y}_t}{\mathbb{P}\left(\mathbf{d}=1\right)} \times \frac{\mathbf{d}-\pi\left(\mathbf{x}_b\right)}{1-\pi\left(\mathbf{x}_b\right)} \right\}
\end{equation}
gives an unbiased estimate of the ATT if 
\begin{align*}
    \mathbb{E}\left(\mathbf{y}_{0t}-\mathbf{y}_{0b}\mid \mathbf{d}=1,\mathbf{x}_b\right) &= \mathbb{E}\left(\mathbf{y}_{0t}-\mathbf{y}_{0b}\mid \mathbf{d}=0,\mathbf{x}_b\right)\\
    \mathbb{P}\left(\mathbf{d}=1\right) &>0\\
    \pi\left(\mathbf{x}_b\right) &<1
\end{align*}
This estimator is a weighted average of the difference of trend, $\Delta \mathbf{y}_t$, across treatment groups: it reweights the trend of the untreated based on the propensity score $\pi\left(\mathbf{x}_b\right)$\footnote{$\frac{\pi\left(\mathbf{x}_b\right)}{1-\pi\left(\mathbf{x}_b\right)}$ is an increasing function of $\pi\left(\mathbf{x}_b\right)$, hence untreated participants with a higher propensity score are given a higher weight.}.

\citet{abadie2005semiparametric} suggests to approximate the propensity score $\pi\left(\mathbf{x}_b\right)$ semiparametrically using a polynomial series of the predictors and plug the predicted values into the sample analogue of the ATT estimates \ref{eq:abadie_att_estimate}.
There are two main ways to do the approximation:
\begin{itemize}
    \item \textbf{linear probability model (LPM)}: higher order improves the approximation, but less precise
    \item \textbf{series logit estimator (SLE)}: using a logit specification to constrain the estimated propensity score to vary between 0 and 1
\end{itemize}

consider $\hat{\pi}\left(\mathbf{x}_b\right)$, the approximated propensity score, and $k$, the order of the polynomial function for approximation. Then the \textbf{LPM} approximation is 
\begin{align*}
    \hat{\pi}\left(\mathbf{x}_b\right) = \hat{\gamma}_0 + \hat{\gamma}_1 \times \mathbf{x}_1 + \sum^k_{i=1}\hat{\gamma}_{2i}\times \mathbf{x}^i_2
\end{align*}
where $\mathbf{x}_1$ is a binary variable, $\mathbf{x}_2^i = \prod^i_{j=1}\mathbf{x}_2$, with $\mathbf{x}_2$ being a continuous variable. Then the coefficients $\hat{\gamma}_0,\hat{\gamma}_1,\hat{\gamma}_{21},\cdots,\hat{\gamma}_{2i},\cdots,\hat{\gamma}_{2k}$ are estimated using OLS estimators.

The \textbf{SLE} approximation is 
\begin{equation*}
    \hat{\pi}\left(\mathbf{x}_b\right) = \Lambda\left(\hat{\gamma}_0 + \hat{\gamma}_1\times \mathbf{x}_1 + \sum^K_{k=1}\hat{\gamma}_{2k}\times \mathbf{x}^k_2\right)
\end{equation*}
where $\Lambda(x) = \frac{\exp(x)}{1+\exp(x)}$ is the logistic function. Higher order binary variables are not considered here since $\mathbf{x}_1^k=\mathbf{x}_1$ for any value $k>1$.

\paragraph*{Doubly robust DiD} \citet{sant2020doubly} adjust for time-invariant covariates in a doubly robust way, by combining inverse-propensity score weighting with outcome modeling. 

\paragraph*{Timing varying covariates}
With finite $T$, strictly exogenous time-varying covariates $X_{it}$ can be converted to time invariant $X_i\equiv \left(X_{i1},\cdots,X_{iT}\right)$, in practice, applied researchers only rely on linear specifications with contemporaneous covariates instead.

\citet{sant2020doubly} also assume that covariates and treatment status are stationary as \citet{abadie2005semiparametric}. Let $T_i$ be a dummy variable that takes value one if the observation $i$ is only observed in the post-treatment period, and 0 if only observed in the pre-treatment period. Define $Y_i = T_iY_{i1}+\left(1-T_i\right)Y_{i0}$. Let $n_1$ and $n_0$ be the sample size of the post- and pre-treatment periods such that $n=n_1+n_0$, and let $\lambda = \mathbb{P}\left(T=1\right)\in \left(0,1\right)$:
\begin{assumption}{Main assumptions of \citet{sant2020doubly}}{santanna_zhao}
    Assume that
    \begin{itemize}
        \item[1] the data $\left\{Y_{i0},Y_{i1},D_i,X_i\right\}^n_{i=1}$ are \textbf{i.i.d.}, or the pooled repeated cross-section data $\left\{Y_i,D_i,X_i,T_i\right\}^n_{i=1}$ consisting of i.i.d. draws from the mixture distribution
        \begin{align*}
            \mathbb{P}\left(Y\leq y, D=d, X\leq x, T=t\right) =& t\cdot \lambda \cdot \mathbb{P}\left(Y_1\leq y, D=d, X\leq x \mid T=1\right)\\
            &+ \left(1-t\right)\cdot \left(1-\lambda\right) \mathbb{P}\left(Y_0\leq y, D=d, X\leq x \mid T=0\right)
        \end{align*}
        where $\left(y,d,x,t\right)\in \mathbb{R}\times \left\{0,1\right\}\times \mathbb{R}^k\times \left\{0,1\right\}$, with the joint distribution of $\left(D,X\right)$ invariant to $T$.
        \item[2] \textbf{Conditional Parallel Trend Assumption (PTA)}\footnote{It allows for covariate-specific time trends but not unit specific trends.}: $$ \mathbb{E}\left[Y_1(0)-Y_0(0)\mid D=1,X\right] \overset{a.s.}{=} \mathbb{E}\left[Y_1(0)-Y_0(0)\mid D=1,X\right] $$
        \item[3] $\exists \epsilon>0$, $\mathbb{P}\left(D=1\right)>\epsilon$ and $\mathbb{P}\left(D=1\mid X\right)\leq 1-\epsilon$ a.s.\footnote{This overlapping condition states that at least a small fraction of the population is treated and that for every value of $X$, at least a small probability that the unit is not treated.}
    \end{itemize}
\end{assumption}
Under Assumption \ref{assump:santanna_zhao}, there are 2 main flexible estimation procedures to estimate the ATT:
\begin{itemize}
    \item[1] outcome regression (\textbf{OR}) approach
    \begin{equation*}
        \hat{\tau}^{\mathrm{reg}} = \bar{Y}_{1,1}-\left[ \bar{Y}_{1,0} + n^{-1}_{\mathrm{treat}} \sum_{i\mid D_i=1}\left(\hat{\mu}_{0,1}\left(X_i\right) - \hat{\mu}_{0,0}\left(X_i\right) \right) \right]
    \end{equation*}
    where $\bar{Y}_{d,t}= \sum_{i\mid D_i=d,T_i=t}Y_{it}/n_{d,t}$ is the sample average outcome among units in treatment group $d$ and time $t$, $\hat{\mu}_{d,t}(x)$ is an estimator of the unknown $m_{d,t}(x)\equiv \mathbb{E}\left[Y_t\mid D=d,X=x\right]$
    \item[2] inverse propensity weighting (\textbf{IPW}) approach, as in \citet{abadie2005semiparametric}.
\end{itemize}

\citet{sant2020doubly} proposed to combine both the \textbf{OR} and \textbf{IPW} approaches to form the doubly robust (\textbf{DR}) moments/estimands for the ATT.

\subparagraph*{Notation} Let $\pi(X)$ be an arbitrary model for the true, unknown propensity score.
\begin{itemize}
    \item with proper panel data, let $\Delta Y= Y_1-Y_0$ and define $\mu^p_{d,\Delta}(X)\equiv \mu^p_{d,1}(X)-\mu^p_{d,0}(X)$ being a model for the true, unknown outcome regression $m^p_{d,t}(x)\equiv \mathbb{E}\left[Y_t\mid D=d,X=x\right]$, $d,t=0,1$. 
    \item with repeated cross-section data, let $\mu_{d,t}^{rc}(x)$ be an arbitrary model for the true, unknown regression $m^{rc}_{d,t}(x)\equiv \mathbb{E}\left[Y\mid D=d,T=t,X=x\right], d,t=0,1$, $\mu_{d,Y}^{rc}(T,X)\equiv T\cdot \mu^{rc}_{d,1}(X)+(1-T)\cdot \mu^{rc}_{d,0}(X)$, and $\mu^{rc}_{d,\Delta}(X)\equiv \mu^{rc}_{d,1}(X)-\mu^{rc}_{d,0}(X)$.
\end{itemize}

\paragraph*{Estimands} consider
\begin{itemize}
    \item for proper panel data: 
    \begin{equation*}
        \tau^{dr,p} = \mathbb{E}\left[ \left(w^p_1(D)-w^p_0\left(D,X;\pi\right)\right) \left(\Delta Y-\mu^p_{0,\Delta}(X)\right) \right]
    \end{equation*}
    where, for a generic $g$,
    \begin{align*}
        w^p_1(D) &= \frac{D}{\mathbb{E}\left[D\right]} & w^p_0\left(D,X;g\right)&= \frac{g(X)(1-D)}{1-g(X)}\cdot \left(\mathbb{E}\left[\frac{g(X)(1-D)}{1-g(X)}\right]\right)^{-1}
    \end{align*}
    \item for repeated cross-section data, consider 2 different estimands
    \begin{align*}
        \tau_1^{dr,rc} =& \mathbb{E}\left[\left(w^{rc}_1(D,T)-w_0^{rc}\left(D,T,X;\pi\right)\right) \cdot \left(Y-\mu^{rc}_{0,Y}(T,X)\right)\right] \\
        \tau^{dr,rc}_2 =& \tau^{dr,rc}_1 + \left(\mathbb{E}\left[\mu^{rc}_{1,1}(X)-\mu^{rc}_{0,1}(X)\mid D=1\right] -\mathbb{E}\left[\mu^{rc}_{1,1}(X)-\mu^{rc}_{0,1}(X)\mid D=1,T=1\right] \right)\\
        &- \left( \mathbb{E}\left[\mu^{rc}_{1,0}(X)-\mu^{rc}_{0,0}(X)\mid D=1\right] - \mathbb{E}\left[\mu^{rc}_{1,0}(X)-\mu^{rc}_{0,0}(X)\mid D=1,T=0\right] \right)
    \end{align*}
    where for a generic $g$,
    \begin{align*}
        w^{rc}_1(D,T)&=w^{rc}_{1,1}(D,T)-w^{rc}_{1,0}(D,T) & w^{rc}_0\left(D,T,X;g\right) &=w^{rc}_{0,1}\left(D,T,X;g\right) - w^{rc}_{0,0}\left(D,T,X;g\right) 
    \end{align*}
    and for $t=0,1$
    \begin{align*}
        w^{rc}_{1,t}\left(D,T\right) &= \frac{D\cdot 1\left\{T=t\right\}}{\mathbb{E} \left[D\cdot 1\left\{T=t\right\}\right]}\\
        w^{rc}_{0,t}\left(D,T,X;g\right) &= \frac{g(X)\left(1-D\right)\cdot 1\left\{T=t\right\}}{1-g(X)} \left(\mathbb{E}\left[\frac{g(X)\left(1-D\right)\cdot 1\left\{T=t\right\}}{1-g(X)}\right]\right)^{-1}
    \end{align*}
\end{itemize}
Then if at least 
\begin{itemize}
    \item for \textbf{panel} data, either $\pi(X)\overset{a.s.}{=} p(X)$ or $\mu^p_{\Delta}(X) \overset{a.s.}{=} m^p_{0,1}(X)-m^p_{0,0}(X)$
    \item for \textbf{repeated cross-section} data, either $\pi(X)\overset{a.s.}{=} p(X)$ or $\mu^p_{0,\Delta}(X) \overset{a.s.}{=} m^{rc}_{0,1}(X)-m^{rc}_{0,0}(X)$\footnote{For repeated cross-section data, $\tau^{dr,rc}_1$ does not reply on OR models for the treated group but $\tau_2^{dr,rc}$ does, however, $\tau^{dr,rc}_1$ is not more robust against model misspecification than $\tau_2^{dr,rc}$ since they identify the ATT under the same conditions. Given that $\mathbb{E}\left[g(X)\mid D=1\right] = \mathbb{E}\left[g(X)\mid D=1,T=t\right],t=0,1$ holds for any $g(\cdot)$, it must hold for $\mu^{rc}_{1,t}(\cdot)-\mu^{rc}_{0,t}(\cdot),t=0,1$, even when $\mu^{rc}_{d,t}(\cdot)$ are misspecified.} 
\end{itemize}
that is, at least one of the working nuisance models is correctly specified, the ATT can be estimated. This is less demanding than both OR and IPW approach.

\paragraph*{Semiparametric efficiency bound} 
Let $m^p_{0,\Delta} \equiv m^p_{0,1}(x)-m^p_{0,0}(x)$ and $m^{rc}_{d,\Delta}(X)\equiv m^{rc}_{0,1}(X)-m^{rc}_{0,0}(X)$ for $d=0,1$. Then 
\begin{itemize}
    \item for \textbf{panel} data, the \textbf{efficient influence function} for the ATT is
    \begin{align*}
        \eta^{e,p}\left(Y_1,Y_0,D,X\right) =& w^p_1(D)\left(m^p_{1,\Delta}(X) - m^p_{0,\Delta}(X) -\tau \right) \\
        &+ w_1^p(D)\left(\Delta Y-m^p_{1,\Delta}(X)\right) - w^p_0(D,X;p)\left(\Delta Y-m^p_{0,\Delta}(X)\right)
    \end{align*}
    and the \textbf{semiparametric efficiency bound} for all regular ATT estimators is
    \begin{align*}
        \mathbb{E}\left[\eta^{e,p}\left(Y_1,Y_0,D,X\right)\right]^2 =& \frac{1}{\mathbb{E}\left[D\right]^2} \left[ D\left(m^p_{1,\Delta}(X)-m^p_{0,\Delta}(X)-\tau\right)^2 \right. \\
        & \left. + D\left(\Delta-m^p_{1,\Delta}(X)\right)^2 + \frac{(1-D)p(X)^2}{\left(1-p(X)\right)^2} \left(\Delta Y-m^p_{0,\Delta}(X)\right)^2 \right]
    \end{align*}
    \item for \textbf{repeated cross-section} data, the \textbf{efficient influence function} for the ATT is
    \begin{align*}
        \eta^{e,rc}\left(Y,D,T,X\right) =& \frac{D}{\mathbb{E}[D]}\left(m^{rc}_{1,\Delta}(X)-m^{rc}_{0,\Delta}(X)-\tau\right) \\
        &+\left( w^{rc}_{1,1}(D,T)\left(Y-m^{rc}_{1,1}(X)\right)-w^{rc}_{1,0}(D,T)\left(Y-m^{rc}_{1,0}(X)\right) \right)\\
        &- \left(w^{rc}_{0,1}(D,T,X;p)\left(Y-m^{rc}_{0,1}(X)\right) - w^{rc}_{0,0}(D,T,X;p)\left(Y-m^{rc}_{0,0}(X)\right) \right)
    \end{align*}
    and the \textbf{semiparametric efficiency bound} for all regular ATT estimators is
    \begin{align*}
        \mathbb{E}\left[\eta^{e,rc}\left(Y,D,T,X\right)^2\right] = \frac{1}{\mathbb{E}[D]^2} \mathbb{E}& \left[ D \left(m^{rc}_{1,\Delta}(X)-m^{rc}_{0,\Delta}(X)-\tau\right)^2 \right. \\
        & + \frac{DT}{\lambda^2}\left(Y-m^{rc}_{1,1}(X)\right)^2 + \frac{D(1-T)}{(1-\lambda)^2}\left(Y-m^{rc}_{1,0}(X)\right)^2 \\
        &+ \left. \frac{(1-D)p(X)^2T}{\left(1-p(X)\right)^2\lambda^2}\left(Y-m^{rc}_{0,1}(X)\right)^2 + \frac{(1-D)p(X)^2 (1-T)}{\left(1-p(X)\right)^2(1-\lambda)^2}\left(Y-m^{rc}_{0,0}(X)\right)^2 \right]
    \end{align*}
\end{itemize}

Both $\eta^{e,p}$ and $\eta^{e,rc}$ depends on the true, unknown, outcome regression functions for the treated group, $m_{1,1}(\cdot)$ and $m_{1,0}(\cdot)$ in an asymmetric manner.

The key difference between the two estimators is that for panel data, 
\begin{align*}
    \eta^{e,p}\left(Y_1,Y_0,D,X\right) =& w^p_1(D)\left(m^p_{1,\Delta}(X) - m^p_{0,\Delta}(X) -\tau \right) \\
    &+ w_1^p(D)\left(\Delta Y-m^p_{1,\Delta}(X)\right) - w^p_0(D,X;p)\left(\Delta Y-m^p_{0,\Delta}(X)\right) \\
    =& \left[w^p_1(D) - w^p_0(D,X;p)\right] Y \left[\Delta Y - m_{0,\Delta}(X)\right] - w_1^p(D)\cdot\tau 
\end{align*}
which ends up \textbf{not} depending are $m_{1,1}(\cdot)$ or $m_{1,0}(\cdot)$. 

Comparing the efficiency bound of the two cases, we have, if $T$ is independent of $\left(Y_1,Y_0,D,X\right)$,
\begin{align*}
    &\mathbb{E}\left[\eta^{e,rc}\left(Y,D,T,X\right)^2\right] - \mathbb{E}\left[\eta^{e,p}\left(Y_1,Y_0,D,X\right)^2\right]\\
    =& \frac{1}{\mathbb{E}\left[D\right]^2} \left[ D\left(\sqrt{\frac{1-\lambda}{\lambda}}\left(Y_1-m_{1,1}(X)\right) + \sqrt{\frac{\lambda}{1-\lambda}}\left(Y_0-m_{1,0}(X)\right)\right)^2 \right.\\
    & \left. + \frac{(1-D)P(X)^2}{\left(1-p(X)\right)^2} \left(\sqrt{\frac{1-\lambda}{\lambda}}\left(Y_1-m_{0,1}(X)\right)+\sqrt{\frac{\lambda}{1-\lambda}}\left(Y_0-m_{0,0}(X)\right)  \right)^2 \right] \geq 0
\end{align*}
which gives that under the DiD framework, it is possible to form more efficient estimators for the ATT when the panel data are available.

This result also gives an efficiency-loss-minimizing $\lambda$:
\begin{align*}
    \lambda &= \frac{\tilde{\sigma}_1}{\tilde{\sigma}_0+ \tilde{\sigma}_1} & \text{where }\tilde{\sigma}^2_t &=\mathbb{E}\left[D\left(Y_t-m_{1,t}(X)\right)^2 + \frac{(1-D)p(X)^2}{\left(1-p(X)\right)^2}\left(Y_t-m_{0,t}(X)\right)^2 \right], t=0,1
\end{align*}
hence, in principle, one may benefit from \textit{oversampling} from either the pre- or post-treamtent period. But it's generally infeasible to do so during the design stage since $\tilde{\sigma}_1^2$ depends on post-treatment data. \citet{sant2020doubly} recommand $\lambda=0.5$ for DiD with repeated cross-section units as a reasonable choice.

\paragraph*{Estimation and inference} 
\citet{sant2020doubly} proposed a two-step procedure for estimation:
\begin{itemize}
    \item first, estimate the true, unknown $p(\cdot)$ with $\pi(\cdot)$, the true unknown $m^p_{d,t}(\cdot)$ and $m^{rc}_{d,t}(\cdot)$ with $\mu^p_{d,t}(\cdot)$ and $\mu^{rc}_{d,t}(\cdot)$, $d,t=0,1$
    \item second, plug the fitted values of the estimated propensity score and regression models into the sample analogue of $\tau^{dr,p}$, $\tau^{dr,rc}_1$, $\tau^{dr,rc}_2$
\end{itemize}
instead of using semi-parametric estimators as \citet{abadie2005semiparametric}, \citet{sant2020doubly} use generic parametric estimators for the first step, assuming:
\begin{itemize}
    \item $\pi\left(x;\gamma^*\right)$ is a parametric model for $p(x)$ s.t. $\pi(\cdot)$ is known up to the \textbf{finite} dimensional pseudo-true $\gamma^*$
    \item for $d,t=0,1$, $\mu^p_{d,t}\left(x;\beta^{*,p}_{d,t}\right)$ and $\mu^{rc}_{d,t}\left(x;\beta^{*,rc}_{d,t}\right)$ s.t. they are known up to the finite dimensional pseudo-true parameter $\beta^{*,p}_{d,t}$ and $\beta^{*,rc}_{d,t}$
\end{itemize}
this approach is most suitable when the sample size is moderate and the dimension of covariates is high. The estimations are 
\begin{itemize}
    \item for \textbf{panel data}
    \begin{equation*}
        \hat{\tau}^{dr,p} = \mathbb{E}_n\left[\left(\hat{w}^p_1(D)-\hat{w}^p_0\left(D,X;\hat{\gamma}\right)\right) \left(\Delta Y- \mu^p_{0,\Delta}\left(X;\hat{\beta}^p_{0,0},\hat{\beta}^p_{0,1}\right)\right) \right]
    \end{equation*}
    where 
    \begin{align*}
        \hat{w}^p_1(D)&=\frac{D}{\mathbb{E}_n[D]} & \hat{w}^p_0\left(D,X;\gamma\right)&= \frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)} \left(\mathbb{E}_n\left[\frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\right]\right)^{-1}
    \end{align*}
    \item for \textbf{repeated cross-section data},
    \begin{align*}
        \hat{\tau}^{dr,rc}_1 =& \mathbb{E}_n \left[ \left(\hat{w}^{rc}_1(D,T) - \hat{w}_0^{rc}(D,T,X;\hat{\gamma}) \right) \left(Y-\mu^{rc}_{0,Y}\left(T,X;\hat{\beta}^{rc}_{0,0},\hat{\beta}^{rc}_{0,1}\right)\right) \right] \\
        \hat{\tau}^{dr,rc}_2 =& \hat{\tau}_1^{dr,rc} + \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,1}(D,T) \right) \left(\mu^{rc}_{1,1}\left(X;\hat{\beta}^{rc}_{1,1}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{rc}_{0,1}\right)\right) \right]\right) \\
        & - \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,0}(D,T) \right) \left(\mu^{rc}_{1,0}\left(X;\hat{\beta}^{rc}_{1,0}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{rc}_{0,0}\right)\right) \right]\right)
    \end{align*}
    where
    \begin{align*}
        \mu^{rc}_{0,Y}\left(Y,X;\beta^{rc}_{0,0},\beta^{rc}_{0,1}\right) &= T\cdot \mu^{rc}_{0,1}\left(\cdot;\beta^{rc}_{0,1}\right)+(1-T)\mu^{rc}_{0,0}\cdot \left(\cdot; \beta^{rc}_{0,0}\right) \\
        \mu^{rc}_{d,\Delta}\left(\cdot;\beta^{rc}_{d,1},\beta^{rc}_{d,0}\right) &= \mu^{rc}_{d,1}\left(\cdot;\beta^{rc}_{d,1}\right) - \mu^{rc}_{d,0}\left(\cdot;\beta^{rc}_{d,0}\right)
    \end{align*}
\end{itemize}
These estimators can be improved to achieve not only \textbf{consistency} doubly robustness, but also \textbf{inference} doubly robustness\footnote{This way, there is no estimation effect from first-step estimators, the asymptotic variance of the results DR DiD estimator for the ATT is invariant to which working modesl for the nuisance functions are correctly specified. This in practice usually translates to simpler and more stable inference procedures.}. 
For the improvement, \citet{sant2020doubly} assume, in addition, 
\begin{itemize}
    \item linear regression working models, for the outcome of interest
    \item a logistic working model, for the propensity score
    \item covariates $X$ entering all nuisance models in a symemtric manner 
\end{itemize}
which are more stringent than the generic DR DiD estimators, but weaker than TWFE estimators. Under such assumptions, we have the improved DR DiD estimators 
\begin{itemize}
    \item for \textbf{panel data}, the 3-step estimator is given as 
    \begin{equation*}
        \hat{\tau}^{dr,p}_{imp} = \mathbb{E}_n\left[ \left(\hat{w}^p_1(D)-\hat{w}^p_0 \left(D,X;\hat{\gamma}^{ipt}\right)\right) \left( \Delta Y-\mu^{lin,p}_{0,\Delta} \left(X;\hat{\beta}^{wls,p}_{0,\Delta}\right) \right) \right]
    \end{equation*}
    the first two steps compute 
    \begin{align*}
        \hat{\gamma}^{ipt} &= \arg\max_{\gamma\in \Gamma}\mathbb{E}_n\left[DX'\gamma -(1-D)\exp(X'\gamma)\right], & \hat{\beta}^{wls,p}_{0,\Delta} & \arg\min_{b\in\Theta}\mathbb{E}_n \left[ \frac{\Lambda\left(X'\hat{\gamma}^{ipt}\right)}{1-\Lambda\left(X'\hat{\gamma}^{ipt}\right)} \left(\Delta Y-X'b\right)^2 \mid D=0 \right]
    \end{align*}
    where $\hat{\gamma}^{ipt}$ is the inverse probability tilting estimator, $\hat{\beta}^{wls,p}_{0,\Delta}$ is the weighted least squares estimator for $\beta^{*,p}_{0,\Delta}$.
    In the last step, plug the fitted value of the working models for the nuisance functions 
    \begin{align*}
        \pi(X,\gamma) &= \Lambda\left(X'\gamma\right) \equiv \frac{\exp(X'\gamma)}{1+\exp (X'\gamma)} & \mu^p_{0,\Delta}\left(X;\beta^p_{0,1},\beta^p_{0,1}\right)&= \mu^{lin,p}_{0,\Delta}\left(X;\beta^p_{0,\Delta}\right)\equiv X'\beta^p_{0,\Delta}
    \end{align*}
    into the sample analogue of $\tau^{dr,p}$. \citet{sant2020doubly} show that if 
    \begin{align*}
        \mathbb{E}\left[ \left(\frac{D}{\mathbb{E}\left[D\right]} - \frac{\exp(X'\gamma^*)(1-D)}{\mathbb{E}\left[\exp(X'\gamma^*)(1-D)\right]}\right) X \right] &= 0 & \mathbb{E}\left[\exp (X'\gamma^*) \left(\Delta Y-\mu^{lin,p}_{0,\Delta}\left(X;\beta^*_{0,\Delta}\right)\right)X \mid D=0 \right] &=0
    \end{align*}
    there will be no estimation effect from the first stage, with the linear outcome models and logistic propensity score models assumed. 
    
    As $n\rightarrow \infty$, these 2 moment conditions follow from the first-order conditions of the optimization problems associated with $\hat{\gamma}^{ipt}$ and $\hat{\beta}^{wls,p}_{0,\Delta}$, even when the working models are misspecified. Hence, replacing the pseudo-true parameters $\gamma^{*,ipt}$ and $\beta^{*,wls,p}_{0,\Delta}$ with their estimators $\hat{\gamma}^{ipt}$ and $\hat{\beta}^{wls,p}_{0,\Delta}$ guarantee that $\hat{\tau}^{dr,p}_{imp}$ is doubly robust:
    $$
    \hat{\tau}^{dr,p}_{imp} \xrightarrow{p} \tau,\ \  \text{ if \textbf{either} $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{or} $X'\beta^{*,wls,p}_{0,\Delta}\overset{a.s.}{=}m^p_{0,\Delta}(X)$}
    $$
    As for inference, $\hat{\tau}^{dr,p}_{imp}$ is $\sqrt{n}-$consistent and asymptotically normal
    \begin{align*}
        \sqrt{n}\left(\hat{\tau}^{dr,p}_{imp}-\tau^{dr,p}_{imp}\right) &= \frac{1}{\sqrt{n}} \sum^n_{i=1}\eta^{dr,p}_{imp} \left(W; \gamma^{*,ipt},\beta^{*,wls,p}_{0,\Delta},\tau^{dr,p}_{imp}\right) + o_p(1) \xrightarrow{d} \mathcal{N}\left(0,V^p_{imp}\right)
    \end{align*}
    and if \textbf{both}  $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{and} $X'\beta^{*,wls,p}_{0,\Delta}\overset{a.s.}{=}m^p_{0,\Delta}(X)$, $V^p_{imp}$ equals to the semiparametrically efficiency bound, and it can be estimated as 
    \begin{equation*}
        \hat{V}^p_{imp} = \mathbb{E}_n\left[\eta^{dr,p}_{imp}\left(W;\hat{\gamma}^{ipt},\hat{\beta}^{wls,p}_{0,\Delta},\hat{\tau}^{dr,p}_{imp}\right)^2\right]
    \end{equation*}
    \item for \textbf{repeated cross-section data}, again assume the working models
    \begin{align*}
        \pi(X,\gamma) &= \Lambda\left(X'\gamma\right) \equiv \frac{\exp(X'\gamma)}{1+\exp (X'\gamma)} & \mu^{rc}_{d,t}\left(X;\beta^{rc}_{d,t}\right)&= \mu^{lin,rc}_{d,t}\left(X;\beta^{rc}_{d,t}\right)\equiv X'\beta^{rc}_{d,t}
    \end{align*}
    then the two improved estimators are given as 
    \begin{align*}
        \hat{\tau}^{dr,rc}_{1,imp} =& \mathbb{E}_n \left[ \left(\hat{w}^{rc}_1(D,T) - \hat{w}_0^{rc}\left(D,T,X;\hat{\gamma}^{ipt}\right) \right) \left(Y-\mu^{lin,rc}_{0,Y}\left(X;\hat{\beta}^{wls,rc}_{0,0},\hat{\beta}^{wls,rc}_{0,1}\right)\right) \right] \\
        \hat{\tau}^{dr,rc}_{2,imp} =& \hat{\tau}_{1,imp}^{dr,rc} + \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,1}(D,T) \right) \left(\mu^{rc}_{1,1}\left(X;\hat{\beta}^{ols,rc}_{1,1}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{wls,rc}_{0,1}\right)\right) \right]\right) \\
        & - \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,0}(D,T) \right) \left(\mu^{rc}_{1,0}\left(X;\hat{\beta}^{ols,rc}_{1,0}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{wls,rc}_{0,0}\right)\right) \right]\right)
    \end{align*}
    where 
    \begin{align*}
        \hat{\gamma} &= \arg\max_{\gamma\in\Gamma} \mathbb{E}_n \left[DX'\gamma - (1-D)\exp(X'\gamma)\right]\\
        \hat{\beta}^{wls,rc}_{0,t} &= \arg\min_{b\in\Theta} \mathbb{E}_n \left[ \frac{\Lambda \left(X'\hat{\gamma}^{ipt}\right)}{1-\Lambda \left(X'\hat{\gamma}^{ipt}\right)} (Y-X'b)^2 \mid D=0,T=t \right]\\
        \hat{\beta}^{ols,rc}_{1,t} &= \arg\min_{b\in\Theta} \mathbb{E}_n \left[ (Y-X'b)^2 \mid D=1, T=t \right]
    \end{align*}
    OLS is adopted to estimate $\beta^{*,rc}_{1,t},t=0,1$ as there is no estimation effect. 
    
    Let 
    \begin{equation*}
        \tau^{dr,rc}_{imp} = \mathbb{E}\left[ \left(w^{rc}_1(D,T) - w^{rc}_0 \left(D,T,X;\gamma^{*,ipt}\right)\right) \left(Y-\mu^{lin,rc}_{0,Y}\left(T,X;\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right)\right) \right]
    \end{equation*}
    and for $\beta^{*,rc}_{imp}=\left(\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0},\beta^{*,ols,rc}_{1,1},\beta^{*,ols,rc}_{1,0}\right)$, define 
    \begin{align*}
        \eta^{dr,rc}_{1,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) &= \eta^{rc,1}_1 \left(W;\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right) -  \eta^{rc,1}_0 \left(W;\gamma^{*,ipt},\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right) \\
        \eta^{dr,rc}_{2,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) &= \eta^{rc,2}_1 \left(W;\beta^{*,rc}_{imp}\right) - \eta^{rc,1}_1 \eta^{rc,2}_0\left(W;\gamma^{*,ipt},\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right)
    \end{align*}
    
    Let $n=n_1+n_0$, where $n_1$ and $n_0$ are the sample sizes of the post- and pre-treatment periods respectively. If $n_1/n\xrightarrow{p}\lambda\in(0,1)$ as $n_0,n_1\rightarrow\infty$, then
    $$
    \hat{\tau}^{dr,rc}_{j,imp} \xrightarrow{p} \tau,\ \  \text{ if \textbf{either} $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{or} $X'\beta^{*,wls,rc}_{0,1}-X'\beta^{*,wls,rc}_{0,0}\overset{a.s.}{=}m^{rc}_{0,\Delta}(X)$}
    $$
    As for inference, $\hat{\tau}^{dr,p}_{imp}$ is $\sqrt{n}-$consistent and asymptotically normal
    \begin{align*}
        \sqrt{n}\left(\hat{\tau}^{dr,rc}_{j,imp}-\tau\right) &= \frac{1}{\sqrt{n}} \sum^n_{i=1}\eta^{dr,rc}_{j,imp} \left(W; \gamma^{*,ipt},\beta^{*,rc}_{imp}\right) + o_p(1) \xrightarrow{d} \mathcal{N}\left(0,V^{rc}_{j,imp}\right)
    \end{align*}
    and if \textbf{both}  $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{and} $X'\beta^{*,wls,rc}_{d,t}\overset{a.s.}{=}m^{rc}_{d,t}(X)$, $\eta^{dr,rc}_{2,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) \overset{a.s.}{=} \eta^{e,rc}\left(Y,D,T,X\right)$, $V^{rc}_{2,imp}$ equals to the semiparametrically efficiency bound, $V^{rc}_{1,imp}$ does \textbf{not}, where
    \begin{equation*}
        {V}^{rc}_{j,imp} = \mathbb{E}_n\left[\eta^{dr,rc}_{j,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right)^2\right]
    \end{equation*}
    and the efficiency loss of using $\hat{\tau}^{dr,rc}_{1,imp}$ instaed of $\hat{\tau}^{dr,rc}_{2,imp}$ is 
    \begin{align*}
        V^{rc}_{1,imp} - V^{rc}_{2,imp} = \mathbb{E} [D]^{-1} \cdot \mathrm{Var} \left[ \sqrt{\frac{1-\lambda}{\lambda}} \left(m^{rc}_{1,1}(X)-m^{rc}_{0,1}(X)\right) + \sqrt{\frac{\lambda}{1-\lambda}} \left(m^{rc}_{1,0}(X)-m^{rc}_{0,0}(X)\right) \mid D=1\right]\geq 0
    \end{align*}
\end{itemize}

\subsubsection{Unconfoundedness}

Viewing the pre-treatment outcomes as covariates, then one can assume \myhl[myblue]{\textbf{unconfoundedness}}:
\begin{equation*}
    D_i \bot \left(Y_{iT}(0),Y_{iT}(1)\right) \mid Y_{i1},\cdots,Y_{iT-1}
\end{equation*}

under this assumption, one can apply the large literature of treatment effect estimation under unconfoundedness \citep{imbens2004nonparametric} or modern approaches \citep{bang2005doubly,chernozhukov2017double,athey2018approximate}.

\citet{imbens2004nonparametric} pointed out 3 arguments for the assumption of unconfoundedness
\begin{itemize}
    \item statistical motivation: the unconfoundedness assumption is logically nature in program evaluation 
    \item purpose: the unconfoundedness assumption asserts that all variables that need to be adjusted for are observed by the researcher
    \item even when agents choose their treatment optimally, two agents with the same values for observed characteristics may differ in their treatment choices without invalidating the unconfoundedness assumption if the difference in their choices is driven by differences in unobserved characteristics that are themselves unrelated to the outcomes of interest.
\end{itemize}

\citet{imbens2004nonparametric} proposes that is is sufficient to assume a weaker version of unconfoundedness, \textbf{mean independence} $$ \mathbb{E}\left[Y_{iT}(0),Y_{iT}(1) \mid D_i, Y_{i1},\cdots,Y_{iT-1}\right] = \mathbb{E}\left[ Y_{iT}(0),Y_{iT}(1) \mid Y_{i1},\cdots,Y_{iT-1} \right] $$
for population ATE. 

Denote 
$$\mu_d = \mathbb{E}_d(Y_{i1},\cdots,Y_{iT-1}) = \mathbb{E}\left[Y\mid D_i=d,Y_{i1},\cdots,Y_{iT-1} \right]$$
for $d=0,1$, \citet{imbens2004nonparametric} reviews 5 groups of estimation for ATEs under unconfoundedness, 

\paragraph*{A. Regression for population/sample/conditional ATE}
we have the estimand
\begin{equation*}
    \hat{\tau}_{reg} = \frac{1}{N} \sum^N_{i=1}\left(\hat{\mu}_1-\hat{\mu}_0\right) = \frac{1}{N}\sum^N_{i=1}D_i\cdot\left(Y_i - \hat{\mu}_0\right) + (1-D_i)\cdot\left(\hat{\mu}_1-Y_i\right)
\end{equation*}
\begin{itemize}
    \item \textbf{early estimators} for $\mu_d$ included parametric regression functions including least-square estimators with the regression function $\mu_d = \beta x+\tau\cdot d$, then one can use the regression $$ Y_i =\alpha + \beta'\left(Y_{i1},\cdots,Y_{iT-1}\right) + \tau W_i +\epsilon_i $$
    more generally, one can specify separate regressions for the two regimes $\mu_d = \beta_d x$, and estimate the two regressions separately.

    \underline{\textit{cons}}: the regression estimators may rely heavily on extrapolation, hence sensitive to changes in the specification of the models.
    \item \textbf{non-parametric estimators}
    \begin{itemize}
        \item \citet{hahn1998role} proposes to estimate first the 3 conditional expectations
        \begin{align*}
            g_1(x) &= \mathbb{E}\left[DY_t\mid Y_{1},\cdots,Y_{T-1}\right] & g_0(x) &= \mathbb{E}\left[(1-D)Y_t\mid Y_{1},\cdots,Y_{T-1}\right] & e(x)&= \mathbb{E}\left[D\mid Y_{1},\cdots,Y_{T-1}\right]
        \end{align*}
        nonparametrically using series methods, then estimate\footnote{For simplicity, let $X_i \equiv \left(Y_{i1},\cdots,Y_{iT-1}\right)$.}
        \begin{align*}
            \hat{\mu}_1(x) &= \frac{\hat{g}_1(x)}{\hat{e}(x)} & \hat{\mu}_0(x) &= \frac{\hat{g}_0(x)}{1-\hat{e}(x)}
        \end{align*}
        and show that the estimators for population ATE and ATT achieve the semiparametric efficiency bounds. Alternatively, one can use this series approach to directly estimate $\mu_d$.
        \item \citet{heckman1998characterizing,heckman1998matching} propose (local-linear) kernel methods, one simple form is 
        \begin{equation*}
            \hat{\mu}_d = \sum_{i:D_i=d} Y_{iT}\cdot K\left(\frac{X_i-x}{h}\right)/\sum_{i:D_i=d}K\left(\frac{X_i-x}{h}\right)
        \end{equation*}
        with kernel $K(\cdot)$ and bandwidth $h$. In the local linear kernel regression, $\mu_d$ is estimated as the intercept $\beta_0$ in the minimization problem 
        \begin{equation*}
            \min_{\beta_0,\beta_1} \sum_{i:D_i=d} \left[Y_i-\beta_0-\beta_1'(X_i-x)\right]^2 \cdot K\left(\frac{X_i-x}{h}\right)
        \end{equation*}
        for bias control, the order of the kernel should be at least as large as the dimension of the covariates: $\int_z z^{r}K(z)\mathrm{d}z=0$, for $r\leq \mathrm{dim}(X)$.
    \end{itemize}
\end{itemize}
for the population ATT, it is important to note that with the propensity score known, the average $\sum D_iY_i/N_T$ is not efficient for the population expectation $\mathbb{E}\left[Y(1)\mid D=1\right]$. The efficient estimator can instead be obtained by weighting all the estimated treatment effects $\hat{\mu}_1-\hat{\mu}_0$ by the probability of receiving treatment:
\begin{equation*}
    \tilde{\tau}_{reg,T} = \frac{\sum^N_{i=1}e(X_i)\cdot \left[\hat{\mu}_1 - \hat{\mu}_0\right]}{\sum^N_{i=1}e(X_i)}
\end{equation*}
this allows one to exploit the control observations to adjust for imbalances in the sampling of the covariates.

\paragraph*{B. Matching}
similar to nonparametric kernel regression methods, matching estimators also impute the missing potential outcomes, but using \textbf{only} the outcomes of \textbf{nearest neighbors} of the opposite treatment group\footnote{What makes matching estimators more attractive is that the researcher only has to choose the \textbf{number of matches}, instead of smoothing parameters.}. Matching estimators often apply in settings where 
\begin{itemize}
    \item the interest is in the ATT
    \item there is a large reservoir of potential controls
\end{itemize}
the estimator is essentially the difference between 2 sample means, the variance is calculated using standard methods for differences in means or paired randomized experiments. The biggest challenge mostly comes from computation.

\citet{abadie2002simple} propose that: again, for a sample $\left\{ \left(Y_i,X_i,D_i\right) \right\}^N_{i=1}$, let $l_m(i)$ be the index that satisfies $D_l\neq D_i$, and 
    \begin{equation*}
        \sum_{j\mid D_j\neq D_i} \mathbf{1} \left\{ \left\Vert X_j-X_i \right\Vert \leq \left\Vert X_l-X_i \right\Vert \right\} = m
    \end{equation*}
    then $l_m(i)$ is the index of the unit in the opposite treatment group that is the $m^{th}$ closest to unit $i$ based on norm $\left\Vert \cdot \right\Vert$ distance, and $l_1(i)$ is the nearest match. Let the set of indices for the first $M$ matches for unit $i$ be $\mathcal{L}_M(i) = \left\{l_1(i),\cdots,l_M(i)\right\}$, then define the imputed potential outcomes as 
    \begin{align*}
        \hat{Y}_i(0) &= \begin{cases}
            Y_i, & D_i=0\\
            \frac{1}{M} \sum_{j\in\mathcal{L}_M(i)} Y_j, & D_i=1
        \end{cases}
        & \hat{Y}_i(1) &= \begin{cases}
            \frac{1}{M} \sum_{j\in\mathcal{L}_M(i)} Y_j, & D_i=0 \\
            Y_i, & D_i=1
        \end{cases}
    \end{align*}
    the simple matching estimator is 
    \begin{equation*}
        \hat{\tau}^{sm}_M = \frac{1}{N} \sum^N_{i=1} \left[hat{Y}_i(1) - \hat{Y}_i(0)\right]
    \end{equation*}
    the bias of this estimator is $O\left(N^{-1/k}\right)$, where $k$ is the dimension of the covariates. \citet{imbens2004nonparametric} listed 3 caveats to \cite{abadie2002simple}'s result:
    \begin{itemize}
        \item[\textbf{1}] only continuous covariates should be counted in $k$: matching with discrete covariates will be exact in large samples
        \item[\textbf{2}] if only matching the treated and the number of potential controls is much larger, the bias can be ignored asymptotically 
        \item[\textbf{3}] the order of the bias may be high, but the actual bias may be small if the coefficients in the leading term are small\footnote{One such case is that biases for different units are at least partially offsetting.}
    \end{itemize}
    and these matching estimators are generally not efficient.

One key aspect of matching is the choice of distance metrics, one can choose 
\begin{itemize}
    \item standard Euclidean metric: $d_E\left(x,z\right)=\left(x-z\right)'\left(x-z\right)$
    \item the diagonal matrix of the inverse of the covariate variances $d_{AI}\left(x,z\right) = \left(x-z\right)'\mathrm{diag}\left(\Sigma^{-1}_X\right)\left(x-z\right)$
    \item Mahalanobis metric: $d_M(x,z) = \left(x-z\right)' \Sigma^{-1}_X \left(x-z\right)$, which reduces differences in covariates within matched pairs in all directions
    \item metrics depending on the correlation between covariates, treatment assignemnt and outcomes:
    \begin{itemize}
        \item weighting absolute differences by the coefficients in the propensity score $$d_{Z1}\left(x,z\right) = \sum^K_{k=1}\left\vert x_k-z_k \right\vert \cdot \left\vert \gamma_k \right\vert$$ where the propensity score has a logistic form $e(x) = \frac{\exp(x'\gamma)}{1+\exp(x'\gamma)}$
        \item weighting absolute differences by the coefficients in the regression function $$ d_{Z2}\left(x,z\right) = \sum^K_{k=1} \left\vert x_k-z_k \right\vert \left\vert \beta_k \right\vert $$ where the regression functions are linear $\mu_{d}(x) = \alpha_d  +x'\beta $
    \end{itemize}
    \citet{imbens2004nonparametric} comments that when the regression function is misspecified, matching with the particular metrics may lead to inconsistency.
\end{itemize}

\paragraph*{C. Propensity scores}
There are 3 main ways to use propensity scores in estimation:
\begin{itemize}
    \item \myhl[myblue]{\textbf{weighting}}: the units by the reciprocal of the probability of receiving the treatment can undo the imbalance of the covariate distributions conditional on the treatment assignment. Formally,
    \begin{align*}
        \mathbb{E} \left[\frac{DY}{e(X)}\right] = \mathbb{E} \left[\frac{DY(1)}{e(X)}\right] = \underbrace{\mathbb{E}\left[\mathbb{E}\left[ \frac{DY(1)}{e(X)} \mid X \right]\right] = \mathbb{E} \left[ \frac{e(X)\cdot \mathbb{E}\left[Y(1)\mid X\right]}{e(X)} \right]}_{\text{unconfoundedness}} = \mathbb{E}\left[Y(1)\right]
    \end{align*}
    and similarly $\mathbb{E}\left[\frac{(1-D)Y}{1-e(X)}\right]=\mathbb{E}\left[Y(0)\right]$, which imply 
    \begin{equation*}
        \tau^p = \mathbb{E} \left[\frac{D\cdot Y}{e(X)} - \frac{\left(1-D\right)\cdot Y}{1-e(X)}\right]
    \end{equation*} 
    it can be directly estimated as 
    \begin{equation*}
        \tilde{\tau} = \frac{1}{N} \sum^N_{i=1} \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i)\cdot Y_i}{1-e(X_i)} \right]
    \end{equation*}
    here, the weights do not necessarily add to 1\footnote{For the treated units, the weights added up to $\frac{1}{N}\sum^N_{i=1}W_i/e(X_i)$, which equals to 1 in expectation, but not so in any given sample.}. One can normalize the weights within subpopulations, which in the limits leads to the estimator proposed by \citet{hirano2003efficient}
    \begin{equation*}
        \hat{\tau}_{weight} = \frac{\sum^N_{i=1} \frac{D_i\cdot Y_i}{\hat{e}(X_i)}}{\sum^N_{i=1}\frac{D_i}{\hat{e}(X_i)}} - \frac{\sum^N_{i=1} \frac{(1-D_i)\cdot Y_i}{1-\hat{e}(X_i)} }{\sum^N_{i=1} \frac{1-D_i}{1-\hat{e}(X_i)}}
    \end{equation*}
    where they specify a sequence of functions of the covariates, such as power series $h_l(x),l=1,\cdots,\infty$, and choose a number of terms $L(N)$ as a function of the sample size, then esitmate the $L-$dimensional vector $\gamma_L$ in 
    \begin{equation*}
        \Pr \left(D=1\mid X=x\right) = \frac{\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \gamma_L \right]}{1+\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \gamma_L \right]}
    \end{equation*}
    by maximizing the associated likelihood function, and calculate the estimated propensity score as 
    \begin{equation*}
        \hat{e}(x) = \frac{\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \hat{\gamma}_L \right]}{1+\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \hat{\gamma}_L \right]}
    \end{equation*}
    a nonparametric estimator for $e(x)$ is efficient, ignoring the 2 regression functions\footnote{The finite-sample properties of the two approaches (nonparametrically estimating propensity scores versus regression functions) may be different, except for when there are only discrete covariates.}.
    
    For the treatment effects of the treated, weight the contribution for unit $i$ by the propensity score $e(x_i)$,
    \begin{align*}
        \hat{\tau}_{weight,tr} &= \frac{\sum^N_{i=1} D_i\cdot Y_i\cdot \frac{e(X_i)}{\hat{e}(X_i)} }{ \sum^N_{i=1} D_i \frac{e(X_i)}{\hat{e}(X_i)} } - \frac{\sum^N_{i=1} (1-D_i)\cdot Y_i\cdot \frac{e(X_i)}{\left(1-\hat{e}(X_i)\right)} }{ \sum^N_{i=1} (1-D_i) \frac{e(X_i)}{ \left(1-\hat{e}(X_i)\right) } } & \text{propensity scores known} \\
        \hat{\tau}_{weight,tr} &= \left[\frac{1}{N_1} \sum_{D_i=1}Y_i \right] - \left[ \frac{\sum_{i:D_i=0} Y_i \cdot \frac{\hat{e}(X_i)}{\left(1-\hat{e}(X_i)\right)} }{ \sum_{i:D_i=0} \frac{\hat{e}(X_i)}{\left(1-\hat{e}(X_i)\right)} }  \right] & \text{propensity scores unknown}
    \end{align*}

    \underline{\textbf{CAUTIOUS}}: the problem of choosing the smoothing parameters is relevant here too\footnote{\citet*{hirano2003efficient}'s series estimators require choosing the \textbf{number of terms} in the series, a kernel-version alternative requires choosing a bandwidth.}, but here one want to use nonparametric regression methods even if the propensity scores are known.
    
    \item \myhl[myblue]{\textbf{blocking}}: \citet{rosenbaum1983central} suggest using the (estimated) propsensity score divide the sample into $M$ blocks of units of approximately equal probability of treatment, letting $J_{im}$ be an indicator for unit $i$ being in block $m$. One way of implementing this is by dividing the unit interval into $M$ blocks with boundary values equal to $m/M$ for $m=1,\cdots,M-1$, s.t. 
    \begin{equation*}
        J_{im} = \mathbf{1} \left\{ \frac{m-1}{M} < e(X_i) < \leq \frac{m}{M} \right\},\ m=1,\cdots,M 
    \end{equation*}
    within each block there are $N_{dm}$ observations with treatment equal to $d$, $N_{dm}=\sum_i \mathbf{1}\left\{D_i=d, J_{im}=1\right\}$. Within each block, estimate the average treatment effect as if random assignment held:
    \begin{equation*}
        \hat{\tau}_m = \frac{1}{N_{1m}} \sum^N_{i=1} J_{im}W_iY_i - \frac{1}{N_{0m}}\sum^N_{i=1} J_{im}\left(1-D_i\right)Y_i
    \end{equation*}
    then estimate the overall average treatment effect as 
    \begin{equation*}
        \hat{\tau}_{block} = \sum^M_{m=1}\hat{\tau}_m \cdot \frac{N_{1m}+N_{0m}}{N}
    \end{equation*}
    for the treatment effect of the treated, one can weight the within-block average treatment effects by the number of treated units: 
    \begin{equation*}
        \hat{\tau}_{T,block} = \sum^M_{m=1} \hat{\tau}_m \cdot \frac{N_{1m}}{N_T}
    \end{equation*}
    \underline{\textbf{CAUTIOUS}}: The asymptotic properties of such estimators require establishing the relative relationship between the number of blocks and the sample size, so choosing the number of blocks becomes essential 
    \begin{itemize}
        \item \textit{starting point}: a single covariate, assuming normality, \textbf{5 blocks} removes $\geq 95\%$ of the bias
        \item \textit{balance check}: covariates whould be balanced within blocks 
        \item \textit{unbalanced blocks}: if the distributions of the covariates among treated and controled are different, one can 
        \begin{itemize}
            \item split the blocks into a number of subblocks if the propensity score itself is unbalanced.
            \item generalize the specification of the propensity score, if the score is balanced but covariates not
        \end{itemize}
        \item \textit{weighting with modified propensity score estimators}: discretize $\hat{e}(x)$ to 
        \begin{equation*}
            \tilde{e}(x) = \frac{1}{M} \sum^M_{m=1} \sum^M_{m=1} \mathbf{1} \left\{ \frac{m}{M} \leq \hat{e}(x) \right\}
        \end{equation*}
        then use $\tilde{e}(x)$ as the propensity score in the weighting estimator leads to an estimator for the ATE \textbf{identical} to that obtained by using the blocking estimator with $\hat{e}(x)$ as the propensity score and $M$ blocks\footnote{With sufficiently large $M$, the blocking estimator is sufficiently close to the oiriginal weighting estimator, sharing first-order asymptotic properties. Hence a large number of blocks does little harm, with regard to asymptotic properties.}. 
    \end{itemize}
    \item \myhl[myblue]{\textbf{propensity scores as regressors}}: estimate the conditional expectation of $Y$ given $D$ and $e(X)$, define $$ v_d(e) = \mathbb{E}\left[ Y(d) \mid e(X)=e \right] \overset{\text{unconfoundedness}}{=} \mathbb{E}\left[Y \mid D=d, e(X)=e\right] $$
    given an estimator $\hat{v}_w(e)$, one can estimate the ATE as 
    \begin{equation*}
        \hat{\tau}_{regprop} = \frac{1}{N} \sum^N_{i=1} \left[ \hat{v}_1\left(e(X_i)\right) - \hat{v}_0 \left(e(X_i)\right) \right]
    \end{equation*}
\end{itemize}

There are 2 cases in practice when considering propensity score approaches
\begin{itemize}
    \item \textbf{propensity scores known}: all 3 methods are efficient, do not rely on high-dimensional nonparametric regressions, have attractive finite-sample properties
    \item \textbf{propensity scores unknown}: require high-dimensional nonparametric regression of the treatment indicator on the covariates. The relative merits of the 3 approaches will depend on whether the propensity score is more or less smooth than the regression functions, and whether additional information is available about either the propensity score or the regression functions.
\end{itemize}

\paragraph*{D. Mixed} Neither matching nor the propensity score methods directly address the correlation between the covariates and the outcome, incorporating the regression method may eliminate remaining bias and improve precision.

\begin{itemize}
    \item \myhl[myblue]{\textbf{Weighting and Regression}}: estimating $$ Y_i = \alpha + \tau\cdot D_i + \epsilon_i $$ with weights $$ \lambda_i = \sqrt{ \frac{D_i}{e(X_i)} + \frac{1-D_i}{1-e(X_i)} } $$
    the covariates are uncorrelated with the treatment indicator, making the weighted estimator consistent. To improve precision, add covariates 
    $$ Y_i = \alpha + \beta'X_i + \tau\cdot D_i + \epsilon_i $$
    which is doubly robust: consistent if either the regression model or the propensity score are specified correctly.
    \item \myhl[myblue]{\textbf{Blocking and Regression}}: least square estimator in block $m$ as $$ Y_i = \alpha_m + \tau_m\cdot D_i + \epsilon_i $$ using only units in block $m$. One can again add covariates and estimate $Y_i = \alpha_m + \beta_m' X_i + \tau_m\cdot D_i + \epsilon_i$.
    \item \myhl[myblue]{\textbf{Matching and Regression}}: the bias of the simple matching estimator can dominate the variance if the dimension of the covariates is too large, regression can help in this situation.
    
    Let $\hat{Y}_i(0)$ and $\hat{Y}_i(1)$ be the observed or imputed potential outcomes for unit $i$, the \textit{estimated potential} outcomes equal the \textit{observed} outcomes for some unit $i$ for its match $l(i)$, the bias $$ \mathbb{E}\left[\hat{Y}_i(1) - \hat{Y}_i(0)\right] - \left[Y_i(1)-Y_i(0)\right] $$
    arises from the fact that the covariates $X_i$ and $X_[l(i)]$ for units $i$ and $l(i)$ are not equal, although they are close because of the matching process. Focusing on the single-match case, define for unit $i$ 
    \begin{align*}
        \hat{X_i}(0) &= \begin{cases}
            X_i &D_i=0\\
            X_{l_1(i)}&D_1=1
        \end{cases} &
        \hat{X_i}(1) &= \begin{cases}
            X_{l_1(i)} &D_i=0\\
            X_i &D_1=1
        \end{cases}
    \end{align*}
    if the matching is exact, $\hat{X}_i(0)=\hat{X}_i(1)$ for each unit $i$. Suppose $D_1=1$, then $\hat{Y}_i(1)=Y_i(1)$, and $\hat{Y}_i(0)$ is an imputed value for $Y_i(0)$. This value is unbiased for $\mu_0\left(X_{l_1(i)}\right)$, but not necessarily for $\mu_0\left(X_i\right)$. Hence, $\hat{Y}_i(0)$ should be adjusted by an estimate of $\mu_0\left(X_i\right) - \mu_0\left(X_{l_1(i)}\right)$. Typically, these corrections are taken be to linear in the difference in the covariates for unit $i$ and its match, of the form $$\beta_0'\left[\hat{X}_i(1)-\hat{X}_i(0)\right] = \beta'_0\left(X_i - X_{l_1(i)}\right)$$
    then \citet{rubin1973use} proposed 3 modifications
    \begin{itemize}
        \item[1] estimating $$ \hat{Y}_i\left(1\right) -\hat{Y}_(0) = \tau+\left[\hat{X}_i(1)-\hat{X}_i(0)\right]'\beta + \epsilon_i $$
    \end{itemize}
\end{itemize}

\paragraph*{E. Bayesian}



\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}