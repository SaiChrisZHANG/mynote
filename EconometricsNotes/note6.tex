\documentclass[twoside]{article}
\input{settings.tex}

\begin{document}
\lecture{6}{DID and TWFE}{}{Sai Zhang}{This note is on the causal panel data, building upon \citet{arkhangelsky2023causal}.}{This note is compiled by Sai Zhang.}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Panel Data Configurations}

\subsection{Data Types}
\subsubsection{Panel Data}
For observations on $N$ units, indexed by $i=1,\cdots,N$, over $T$ periods, indexed by $t=1,\cdots,T$, the outcome of interest is denoted by $Y_{it}$, the treatment $W_{it}$.
These observations may themselves consist of averages over more basic units:
\begin{align*}
    \mathbf{Y} &= \begin{pmatrix}
        Y_{11} & \cdots & Y_{1T}\\
        \vdots & \ddots & \vdots \\
        Y_{N1} & \cdots & Y_{NT}
    \end{pmatrix} &
    \mathbf{W} = \begin{pmatrix}
        W_{11} & \cdots & W_{1T}\\
        \vdots & \ddots & \vdots \\
        W_{N1} & \cdots & W_{NT}
    \end{pmatrix}
\end{align*}
we may also observe exogenous variables $X_{it}$ or $X_i$. Typically, we focus on a balanced panel where for all units $i=1,\cdots,N$ we observe outcomes for all $t=1,\cdots,T$.

\subsubsection{Grouped Repeated Cross-Section Data}
In a GRCS data, we have observations on $N$ units, each observed only once in period $T_i$ for unit $i$. Different units may be observed at diffrent points in time, $T_i$ typically takes on only a few values, with many units sharing the same value for $T_i$. The outcome $Y_i$ and treatment $W_i$ are indexed by the unit index $i$.
The set of units is \textbf{partitioned} into 2 or more groups, with the group that unit $i$ belongs to denoted by $G_i\in \mathcal{G}=\left\{1,2,\cdots,G\right\}$.

Define the average outcomes for each group-time-period pair:
\begin{equation*}
    \bar{Y}_{gt} \equiv \frac{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}Y_i}{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}}
\end{equation*}
for treatment 
\begin{equation*}
    \bar{W}_{gt} \equiv \frac{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}W_i}{\sum^N_{i=1} \mathbf{1}_{G_i=g,T_i=t}}
\end{equation*}
then treat the $G\times T$ group averages $\bar{Y}_{gt}$ and $\bar{W}_{gt}$ as the unit of observation, then the grouped data is just a panel.
The major issue in practice is that the number of groups is very small comparing to proper panel data.

\subsubsection{Row and Column Exchangeable Data}
The data are doubly indexed by $i=1,\cdots,N$ and $j=1,\cdots,J$, with outcomes $Y_{ij}$. They are different from panel data in that there is \textbf{no time ordering} for the second index. Many methods developed for panel data are also applicable here.

\subsection{Shapes of Data Frames}
Panel data can also be loosely classified by the shape:
\begin{itemize}
    \item \myhl[myblue]{\textbf{Thin Frames} $(N\gg T)$}, where the number of cross-section units is large relative to the number of time periods:
    \begin{itemize}
        \item unit-specific parameters (individual FEs) \textbf{can not be estimated consistently} due to the short time series
        \item REs might be more suitable since they place a stocahstic structure on the individual components
    \end{itemize}
    \item \myhl[myblue]{\textbf{Fat Frames} $(N\ll T)$}, where the number of cross-section units is large relative to the number of time periods.
    \item \myhl[myblue]{\textbf{Square} $N\simeq T$}, where the number of units and time periods is comparable.
\end{itemize}

\subsection{Assignment Mechanisms}
\subsubsection{The General Case}
In the most general case, the treatment may vary both across units and over time, with units \textbf{switching} in and out of the treatment group:
\begin{equation*}
    \mathbf{W}^{\text{general}} = \begin{pmatrix}
        1&1&0&0&\cdots &1\\
        0&0&1&0&\cdots &0\\
        1&0&1&1&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        1&0&1&0&\cdots &0
    \end{pmatrix}
\end{equation*}
This is more relevant for the RCED configurations, and for panel data of products and promotions as treatments.
The assumption on the absence/presence of \textbf{dynamic treatment} effects is very important.

\subsubsection{Single Treated Period}
One special case arises when a substantial number of units is treated, but these units are only treated \textbf{in the last period}
\begin{equation*}
    \mathbf{W}^{\text{last}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&0&\cdots &1
    \end{pmatrix}
\end{equation*}
If $T$ is relatively small, this case is often analyzed as a cross-section problem, the lagged outcomes are used as exogenous covariates or pre-treatment variables to be adjusted.
Here, dynamic effects are not testable, nor do they matter since the shortness of the panel.
\begin{equation*}
    \mathbf{W}^{\text{last}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &0
    \end{pmatrix}
\end{equation*}
this setting is prominent in the original applications of the synthetic control literature, here $T$ is usually small.

\subsubsection{Single Treated Unit and Single Treated Period}
An extreme case is where only a single unit is treated, and it is only treated in a single period (typically the last). 
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&0&\cdots &1
    \end{pmatrix}
\end{equation*}
Normally, we focus on the effect for the single treated/time-period pair and construct prediction intervals.

\subsubsection{Block Assignment}
The case of block assignment is where a subset of units is treated every period after a common starting date:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &0\\
        0&0&1&1&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &1
    \end{pmatrix}
\end{equation*}
There is typically a sufficient number of treated unit/time-period pairs to allow for reasonable approximations. The presence of dynamic effects change the interpretation of the average effect of the treated: the average effect for the treated now is an average over short \textbf{and} medium term effects during different periods.

\subsubsection*{Staggered Adoption (a.k.a. absorbing treatment setting)}
The staggered adoption is the case where units adopt the treatment at various period, and remain in the treatment group once they adopt the treatment:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        0&0&0&0&\cdots &1\\
        0&0&0&1&\cdots &1\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&1&1&\cdots &1
    \end{pmatrix}
\end{equation*}
Here, with some assumptions, we can separate dynamic effects from heterogeneity across calendar time.

\subsubsection{Event Study Designs}
In the event-study design, units are exposed to the treatment in at most one period:
\begin{equation*}
    \mathbf{W}^{\text{block}} = \begin{pmatrix}
        0&0&0&0&\cdots &0\\
        1&0&0&0&\cdots &0\\
        0&1&0&0&\cdots &0\\
        \vdots &\vdots & \vdots & \vdots & \ddots & \vdots \\
        0&0&0&1&\cdots &0
    \end{pmatrix}
\end{equation*}
There are often dynamic effects of the treatment past hte time of initial treatment, however, the effects might be changing over time.

\subsubsection{Clustered Assignment}
In many applications, units are grouped together in clusters. Units within the same clusters are always assigned to the treatment:
\begin{equation*}
    \mathbf{W}^{\text{cluster}} = \begin{pmatrix}
        &&&&&&&\text{cluster}\\
        0&0&0&0&\cdots &0&0&1\\
        0&0&0&0&\cdots &0&0&1\\
        0&0&0&0&\cdots &0&0&2\\
        0&0&0&0&\cdots &0&0&2\\
        0&0&0&0&\cdots &1&1&3\\
        \vdots&\vdots&\vdots&\vdots&\ddots &\vdots&\vdots&\vdots\\
        0&0&0&1&\cdots &1&1&C\\
        0&0&0&1&\cdots &1&1&C
    \end{pmatrix}
\end{equation*}
Clustering creates complications for inference.

\subsection{Outcomes, Assumptions and Estimands}
For a treatment assignment matrix $\mathbf{W}$, denote:
\begin{itemize}
    \item the full $T-$component column vector of treatment assignments as $$ \underline{\mathbf{w}} \equiv \left(w_1,\cdots,w_T\right)' $$
    \item the $t$-component column vector of treatment assignments \myhl[myblue]{\textbf{up to} time $t$} as $$ \underline{\mathbf{w}}^{t} \equiv \left(w_1,\cdots,w_t\right)' $$ hence $\underline{\mathbf{w}}^T=\underline{\mathbf{w}}$
    \item the row vector of treatment values for unit $i$ as $\underline{\mathbf{W}}_i$
\end{itemize}
Then in general, we can index the potential outcomes for unit $i$ in period $t$ by the full $T-$component vector of assignments $\underline{\mathbf{w}}$
$$Y_{it}\left(\underline{\mathbf{w}}\right)$$
A key underlying assumption is the \myhl[myblue]{\textbf{Stable Unit Treatment Value Assumption (SUTVA)}}, which requires that there is no interference or spillovers between units\footnote{SUTVA can hold on a cluster/group level, where the spillover effects are within clusters/groups.}.

In this setup, there are $2^T$ potential outcomes for each unit and each time period, as a function of multi-valued treatment $\underline{\mathbf{w}}$. Then, define for each $t$-unit treatment effects for each pair of assignment vectors $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$: $$ \tau^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}_{it}\equiv Y_{it}\left(\underline{\mathbf{w}}'\right) - Y_{it}\left(\underline{\mathbf{w}}\right) $$
and the corresponding population average effect $$ \tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}\equiv \mathbb{E}\left[Y_{it}\left(\underline{\mathbf{w}}'\right)-Y_{it}\left(\underline{\mathbf{w}}\right) \right] $$
where the expectation is implicitly assumed to be taken over a \textbf{large} population.

Under completely random assignment, all $\tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}$ are identified, and are \textbf{just-identified}, given sufficient variation in the treatment paths. Dynamic treatment effects can also be identified\footnote{For example, consider that in the 2-period case $$ \tau_2^{(1,1),(0,1)} $$ is the average effect in the second period of being exposed to $(1,1)$, \textit{treated in both period}, rather than $(0,1)$, \textit{treated only in the second period}.}.
However, we have \myhl[myblue]{$2^{T-1}\times \left(2^T-1\right)$} distinct average effects of the form $\tau_t^{\underline{\mathbf{w}},\underline{\mathbf{w}}'}$,
in practice, we often need to focus on summary measures of these causal effects, which requires some addition assumptions:

\begin{assumption}{No Anticipation}{no_anticipation}
    The potential outcomes satisfy $$ Y_{it}\left(\underline{\mathbf{w}}\right) = Y_{it}\left(\underline{\mathbf{w}}'\right)$$ for all $i$, and for all combinations of $t$, $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$ such that $\underline{\mathbf{w}}t=\underline{\mathbf{w}}'^{t}$. 
\end{assumption}
This is a testable assumption with experimental data and sufficient variation in treatment paths, by comparing units that have the same treatment path up to and including $t$ and diverge after $t$.
\begin{itemize}
    \item \underline{Units \textbf{are not} active decision-makers}: the assumption can be guaranteed by design (random treatment assignment each period, or staggered adoption with randomly assigned adoption date)
    \item \underline{\textbf{Limited} antici}p\underline{ation}: assuming the treatment can be anticipated for a \textbf{fixed number} of periods, which shifts $\underline{\mathbf{w}}$ by that number of periods.
    \item \underline{Units \textbf{are} active decision-makers}: potential outcomes are functions of $\underline{\mathbf{w}}$ and the distribution of $\underline{\mathbf{w}}$ (experimental design itself):
    \begin{itemize}
        \item one can define potential outcomes for a given randomized experimental design: the beliefs about the future treatment paths are incorporated in the definition of the potential outcomes, the actual values are by construction unknown. This does change the interpretation of the casual effects\footnote{Think about the differences between a surprise deviation from a given policy rule versus the effect of a permanent chagne int he policy rule itself.}.
        \item In obserational studies, one cannot directly control the information about the future treatment paths. In this case, different units need to be gauranteed to face the \textbf{same information environment} for Assumption \ref{assump:no_anticipation} to hold.
    \end{itemize}
\end{itemize}
Under Assumption \ref{assump:no_anticipation}, the total number of potential treatment effects is reduced from $2^{T-1}\times \left(2^T-1\right)$ to $\left(\sum^T_{t=1}2^{t-1}\right)\left(\sum^T_{t=1}2^t-1\right)$. The unit-period specific treatment effects are now of the type 
\begin{equation*}
    \tau^{\underline{\mathbf{w}}^t,\underline{\mathbf{w}}^{t'}}_{it} \equiv Y_{it}\left(\underline{\mathbf{w}}^{t'}\right) - Y_{it}\left(\underline{\mathbf{w}}^t\right)
\end{equation*}
with the potential outcomes for period $t$ indexed by treatments up to period $t$ only. Here, one can still distinguish
\begin{itemize}
    \item static treatment effects: $\tau_{it}^{\left(\underline{\mathbf{w}}^{t-1},0\right),\left(\underline{\mathbf{w}}^{t-1},1\right)}$, which measures the response of current outcome to the current treatment, holding the past ones fixed.
    \item dynamic treatment effects: $\tau_{it}^{\left(\underline{\mathbf{w}}^{t-1},w^t\right),\left(\underline{\mathbf{w}}^{t-1},w^t\right)}$, which does the opposite.
\end{itemize}

\begin{assumption}{No Dynamic/Carry-Over Effects}{no_dynamic}
    The potential outcomes satisfy $$ Y_{it}\left(\underline{\mathbf{w}}\right) = Y_{it}\left(\underline{\mathbf{w}}'\right) $$
    for all $i$ and for all combinations of $t$, $\underline{\mathbf{w}}$ and $\underline{\mathbf{w}}'$ such that $w_{it}=w'_{it}$.
\end{assumption}
This assumption is \textbf{not} guaranteed by randomization. It restricts the treatment effects and the potential outcomes for the \textbf{post}-treatment periods.
It has testable restrictions given the random assignment of the treatment and sufficient variation in the treatment paths. It does \textbf{not} restrict the time path of the potential outcomes in the absence of any treatment $Y_{it}\left(\mathbf{0}\right)$.

This assumption greatly reduce the total number of treatment effects for each unit to $T$:
$$ \tau_{it}\equiv Y_{it}(1) - Y_{it}(0)$$
where $\tau_{it}$ has no superscripts because there are only 2 possible arguments of the potential outcomes $w\in \left\{0,1\right\}$.

\begin{assumption}{Staggered Adoption}{stag_adopt}
    In staggered adoption, $$W_{it}\leq W_{it-1},\ \forall t=2,\cdots,T$$
    define the adoption date $A_i$ as the date of the first treatment, $A_i\equiv T+1-\sum^T_{t=1}W_{it}$ for treated units, and $A_i\equiv \infty$ for never-treated units.
\end{assumption}
Under Assumption \ref{assump:stag_adopt}, the potential outcomes can be written in terms of the adoption date as $Y_{it}(a)$, for $a=1,\cdots,T,\infty$, and the realized outcome as $Y_{it}=Y_{it}\left(A_i\right)$. There are 2 broad classes of settings that are viewed as staggered adoption designs:
\begin{itemize}
    \item interventions adopted and remain in place 
    \item one-time interventions with a long-term, or even permanent, impact (where the post-intervention period effects are dynamic effects)
\end{itemize}
Under Assumption \ref{assump:stag_adopt}, but \textbf{not} Assumption \ref{assump:no_anticipation} and \ref{assump:no_dynamic}, we can write 
$$ \tau_{it}^{a,a'}\equiv Y_{it}\left(a'\right)-Y_{it}(a) $$
with the corresponding population average
$$ \tau_t^{a,a'}\equiv \mathbb{E}\left[ \equiv Y_{it}\left(a'\right)-Y_{it}(a) \right] $$
we can also denote the average for subpopulations conditional on the adoption dates as 
$$ \tau_{t\mid a''}^{a,a'}\equiv \mathbb{E}\left[\equiv Y_{it}\left(a'\right)-Y_{it}(a)\mid A_i=a''\right] $$
which explicitly depends on the details of the assignment process. This estimand is conceptually similar to the average effect on the treated in cross-sectional settings, but with selection operating over both unit and period dimensions.

\subsection{Conventional TWFE and DiD}
\subsubsection{TWFE Characterization}
First, consider a panel setting with no anticipation, no dynamics, and constant treatment effects:
\begin{assumption}{The TWFE Model}{twfe_model}
    The control outcome $Y_{it}(0)$ satisfies $$ Y_{it}(0) = \alpha_i + \beta_t + \epsilon_{it} $$
    The unobserved component $\epsilon_{it}$ is (mean-)independent of the treatment assignment $W_{it}$
\end{assumption}
And 
\begin{assumption}{Constant Static Treatment Effects}{const_stat_treat}
    The potential outcomes satisfy $$ Y_{it}(1) = Y_{it}(0) + \tau \ \ \forall (i,t) $$
\end{assumption}
Under Assumption \ref{assump:twfe_model} and \ref{assump:const_stat_treat}, for the realized $Y_{it}\equiv W_{it}Y_{it}(1) + \left(1-W_{it}\right)Y_{it}(0)$ we have a model 
$$ Y_{it} = \alpha_i + \beta_t +\tau W_{it}+\epsilon_{it} $$
then we can estimate the parameters of this model by least squares
\begin{equation*}
    \left(\hat{\tau}^{TWFE},\hat{\alpha},\hat{\beta}\right) = \arg\min_{\tau,\alpha,\beta} \sum^N_{i=1}\sum^T_{t=1} \left(Y_{it}-\alpha_i-\beta_t -\tau W_{it}\right)^2
\end{equation*}
one restriction on the $\alpha_i$ or $\beta_t$ needs to be imposed to avoid perfect collinearity, but this normalization does not affect the estimation of $\tau$.

Under a block assignment structure, we have $W_{it}=1$ only for a subset of the units\footnote{The \textit{treatment group} with $i\in \mathcal{J}$, where the cardinality for the set $\mathcal{J}$ is $N^{\mathrm{tr}}$ and $N^{\mathrm{co}}\equiv N-N^{\mathrm{tr}}$.}, and those units are treated only during periods $t$ with $t>T_0$.
Define the averages in the four groups as 
\begin{align*}
    \bar{Y}^{\mathrm{tr,post}} &\equiv \frac{\sum_{i\in\mathcal{J}}\sum_{t>T_0}Y_{it}}{N^{\mathrm{tr}}\left(T-T_0\right)} & \bar{Y}^{\mathrm{tr,pre}} &\equiv \frac{\sum_{i\in\mathcal{J}}\sum_{t\leq T_0}Y_{it}}{N^{\mathrm{tr}}T_0} \\
    \bar{Y}^{\mathrm{co,post}} &\equiv \frac{\sum_{i\not\in\mathcal{J}}\sum_{t>T_0}Y_{it}}{N^{\mathrm{co}}\left(T-T_0\right)} & \bar{Y}^{\mathrm{co,pre}} &\equiv \frac{\sum_{i\not\in\mathcal{J}}\sum_{t\leq T_0}Y_{it}}{N^{\mathrm{co}}T_0}
\end{align*}
and then write the estimator for the treatment effect as 
\begin{equation*}
    \hat{\tau}^{TWFE} = \left(\bar{Y}^{\mathrm{tr,post}}-\bar{Y}^{\mathrm{tr,pre}}\right) - \left(\bar{Y}^{\mathrm{co,post}}-\bar{Y}^{\mathrm{co,pre}}\right)
\end{equation*}

\subsubsection{DiD Estimator in the Grouped Repeated Cross-Section Setting}
In GRCS setting, we observe each physical unit only once. With blocked assignment, the notation only has a single index for the unit $i=1,\cdots,N$.
Let $G_i\in\mathcal{G}=\left\{1,\cdots,G\right\}$ denote the cluster or group unit $i$ belongs to, and $T_i\in \left\{1,\cdots,N\right\}$ the time period unit $i$ is observed in.

The set of clusters $\mathcal{G}$ is partitioned into two groups: control group $\mathcal{G}_C$ and treatment group $\mathcal{G}_T$, with cardinality $G_C$ and $G_T$. Only units with $G_i\in \mathcal{G}_T$, indicated by $D_i=\mathbf{1}_{G_i\in\mathcal{G}_T}$, are exposed to the treatment if they are observed after the treatment date $T_0$: $W_i=\mathbf{1}_{G_i\in\mathcal{G}_T,T_i>T_0}$

Assuming that the treatment within group and time period pairs is constant, the cluster/time-period average treatment $\bar{W}_{gt}$ is binary if the original treatment is. Then the DiD estimator is 
\begin{align*}
    \hat{\tau}^{DiD} =& \frac{1}{G_T\left(T-T_0\right)} \sum_{g\in\mathcal{G}_T,t>T_0} \bar{Y}_{gt} - \frac{1}{G_C\left(T-T_0\right)} \sum_{g\in\mathcal{G}_C,t>T_0}\bar{Y}_{gt}\\
    &- \frac{1}{G_TT_0}\sum_{g\in\mathcal{G}_T,t\leq T_0} \bar{Y}_{gt} + \frac{1}{G_CT_0}\sum_{g\in\mathcal{G}_C,t\leq T_0}\bar{Y}_{gt}
\end{align*}
and at the group level, we have a proper panel setup:
\begin{align*}
    \bar{Y}_{gt}(0) &= \alpha_g+\beta_t+\epsilon_{gt} & \bar{Y}_{gt}(1) &\bar{Y}_{gt}(0)+\tau
\end{align*}
and the potential outcomes $\bar{Y}_{gt}(0)$ and $\bar{Y}_{gt}(1)$ should be interpreted as the average of the potential outcomes if all units in a group/time-period pair are exposed to the control treatment.

\subsubsection{Inference}
There are two ways to conduct inference about $\hat{\tau}^{\mathrm{DiD}}$ and $\hat{\tau}^{\mathrm{TWFE}}$:
\begin{itemize}
    \item the assignment process is known: \myhl[myblue]{\textbf{design-based}} or \myhl[myblue]{\textbf{randomization-based}} inference
    \item otherwise: \myhl[myblue]{\textbf{sampling-based}} inference
\end{itemize}

\paragraph*{Design-Based Inference}


\paragraph*{Sampling-Based Inference}
\begin{itemize}
    \item \myhl[myblue]{\textbf{proper panel setting}}: it is often assumed that all units are randomly sampled from a large population and thus \textbf{exchangeable}. Inference about $\hat{\tau}^{\mathrm{TWFE}}$ reduces to joint inference about four means with i.i.d. observations.
    \item \myhl[myblue]{\textbf{GRCS setting}}: one can allow for non-vanishing errors at the group level, but it cannot be done in the two-group case.
\end{itemize}

\subparagraph*{Standard errors}
Regardless of the level of aggregation, inference for TWFE and DiD estimators typically takes into account the correlation in outcomes over time within units in applications with more than two periods. So it is \textbf{NOT} appropriate to use the robust Eicker-Huber-White standard errors. 
Instead, one should use clustered standard errors based on clustering observations by units. It can also be approximated by bootstrapping all observations for each unit.

\subsubsection{The Parallel Trend Assumption}

The \textbf{parallel trend assumption} is the fundamental justification for the DiD estimator. It states that the units who are treated would have followed a path that is parallel to the path followed by the control units on average, in the absence of the treatment.

\paragraph*{Proper panel settings} the assumption is that the expected difference in control outcomes in any period for units who later are exposed to the treatment and units who are always in the control group is \textbf{contstant}:
\begin{assumption}{Parallel Trend Assumption: Proper Panel}{parallel_trend_panel}
    For all $t,t'$,
    \begin{equation*}
        \mathbb{E}\left[Y_{it}\left(0\right)\mid D_i=1\right] - \mathbb{E}\left[Y_{it}\left(0\right)\mid D_i=0\right] = \mathbb{E}\left[Y_{it'}\left(0\right)\mid D_i=1\right] - \mathbb{E}\left[Y_{it'}\left(0\right)\mid D_i=0\right]
    \end{equation*}
    equivalently, we can formulate it in terms of changes over time\footnote{the expected change in control outcomes is the same for those who will eventually be exposed to the treatment and those who will not}:
    \begin{equation*}
        \mathbb{E}\left[Y_{it}(0)-Y_{it'}(0)\mid D_i=1\right] = \mathbb{E}\left[Y_{it}(0)-Y_{it'}(0)\mid D_i=0\right]
    \end{equation*}
    alternatively, postulate a TWFE model for the control outcomes, additionally assuming that the treatment assignment $D_i$ is independent of the vector of residuals $\epsilon_{it},t=1,\cdots,T$, conditional on FEs: 
    \begin{equation*}
        D_i \perp \left(\epsilon_{i1},\cdots,\epsilon_{iT}\right) \mid \alpha_i 
    \end{equation*}
\end{assumption}
From the point of view of the modern casual inference literature, the parallel trend assumption is non-standard in the sense that it combbines restrictions on the potential outcomes with restrictions on the assignment mechanism.

\paragraph*{GRCS settings} Suppose in the population, all groups are (infinitely) large in each period, and we have random samples from these populations for each period. Then the expectations are well defined as population averages.
The parallel trends assumption can the nbe formulated as requiring that the difference in expected control outcomes between two groups remains constant over time: 
\begin{assumption}{Parallel Trend Assumption: Grouped Repeated Cross-Section}{parallel_trend_grcs}
    For all pairs of groups $g,g'$ and for all pairs of time periods $t,t'$, the average difference between the groups remains the same over time, irrespective of their treatment status: 
    \begin{equation*}
        \mathbb{E}\left[Y_{gt}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{g't}(0)\mid D_i=0\right] = \mathbb{E}\left[Y_{gt'}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{g't'}(0)\mid D_i=0\right]
    \end{equation*}
    an alternative formulation is that expected change between periods $t'$ and $t$ is the same for all groups:
    \begin{equation*}
        \mathbb{E}\left[Y_{gt}(0)\mid D_i=1\right] - \mathbb{E}\left[Y_{gt'}(0)\mid D_i=1\right] = \mathbb{E}\left[Y_{g't}(0)\mid D_i=0\right] - \mathbb{E}\left[Y_{g't'}(0)\mid D_i=0\right]
    \end{equation*}
\end{assumption}
If $Y_{gt}(0)$ for all $g$ and $t$ are observed, the presence of the two groups and two time periods would be sufficient for the assumption to have testable implications. However, in the 2-group/2-period case, at least one of the four cells is exposed to the treatment, there are no testable restrictions implied by this assumption\footnote{If there are more than 2 periods, or more than 2 groups, there are testable restrictions by the parallel trend assumption.}.

\subsubsection{Pre-treatment Variables}

Time-invariant characteristics of the units in addition to the time path of the outcome are observed, these variables are colinear with the individual fixed effects $\alpha_i$ hence cannot be incorporated simply by adding them to the TWFE specification.
A reason one might want to include these pre-treatment variables is that the parallel trend and constant treatment effect assumptions hold only within subpopulations defined by them.

\paragraph*{Semi-parametric DiD}
\citet{abadie2005semiparametric} proposed a solution based on \textbf{re-weighting} the differences in outcomes by the propensity score for balance. TO estimate the average treatment effect on the treated (ATT):
\begin{equation*}
    ATT\equiv \mathbb{E}\left(\mathbf{y}_{1t}-\mathbf{y}_{0t}\mid \mathbf{d}=1\right)
\end{equation*}
where the 2 potential outcomes $\mathbf{y}_{1t}$ is the value of $\mathbf{y}$ if the participant received the treatment by $t$, $\mathbf{y}_{0t}$ is the value of $\mathbf{y}$ if the participant had not received the treatment by time $t$. $\mathbf{d}$ is an indicator of treatment.

ATT cannot be directly estimated since $\mathbf{y}_{0t}$, the counterfactual, is never observed. For a set of pretreatment characteristics $\mathbf{x}_{b}$, define the probability to be in the treatment group conditional on $\mathbf{x}_b$ as $$ \pi\left(\mathbf{x}_b\right)\equiv \mathbb{P}\left(\mathbf{d}=1\mid \mathbf{x}_b\right) $$
define the change of $\mathbf{y}$ from baseline $b$ to $t$ as $$\Delta \mathbf{y}_t \equiv \mathbf{y}_t - \mathbf{y}_b $$
then
\begin{equation}\label{eq:abadie_att_estimate}
    \mathbb{E}\left\{ \frac{\Delta \mathbf{y}_t}{\mathbb{P}\left(\mathbf{d}=1\right)} \times \frac{\mathbf{d}-\pi\left(\mathbf{x}_b\right)}{1-\pi\left(\mathbf{x}_b\right)} \right\}
\end{equation}
gives an unbiased estimate of the ATT if 
\begin{align*}
    \mathbb{E}\left(\mathbf{y}_{0t}-\mathbf{y}_{0b}\mid \mathbf{d}=1,\mathbf{x}_b\right) &= \mathbb{E}\left(\mathbf{y}_{0t}-\mathbf{y}_{0b}\mid \mathbf{d}=0,\mathbf{x}_b\right)\\
    \mathbb{P}\left(\mathbf{d}=1\right) &>0\\
    \pi\left(\mathbf{x}_b\right) &<1
\end{align*}
This estimator is a weighted average of the difference of trend, $\Delta \mathbf{y}_t$, across treatment groups: it reweights the trend of the untreated based on the propensity score $\pi\left(\mathbf{x}_b\right)$\footnote{$\frac{\pi\left(\mathbf{x}_b\right)}{1-\pi\left(\mathbf{x}_b\right)}$ is an increasing function of $\pi\left(\mathbf{x}_b\right)$, hence untreated participants with a higher propensity score are given a higher weight.}.

\citet{abadie2005semiparametric} suggests to approximate the propensity score $\pi\left(\mathbf{x}_b\right)$ semiparametrically using a polynomial series of the predictors and plug the predicted values into the sample analogue of the ATT estimates \ref{eq:abadie_att_estimate}.
There are two main ways to do the approximation:
\begin{itemize}
    \item \textbf{linear probability model (LPM)}: higher order improves the approximation, but less precise
    \item \textbf{series logit estimator (SLE)}: using a logit specification to constrain the estimated propensity score to vary between 0 and 1
\end{itemize}

consider $\hat{\pi}\left(\mathbf{x}_b\right)$, the approximated propensity score, and $k$, the order of the polynomial function for approximation. Then the \textbf{LPM} approximation is 
\begin{align*}
    \hat{\pi}\left(\mathbf{x}_b\right) = \hat{\gamma}_0 + \hat{\gamma}_1 \times \mathbf{x}_1 + \sum^k_{i=1}\hat{\gamma}_{2i}\times \mathbf{x}^i_2
\end{align*}
where $\mathbf{x}_1$ is a binary variable, $\mathbf{x}_2^i = \prod^i_{j=1}\mathbf{x}_2$, with $\mathbf{x}_2$ being a continuous variable. Then the coefficients $\hat{\gamma}_0,\hat{\gamma}_1,\hat{\gamma}_{21},\cdots,\hat{\gamma}_{2i},\cdots,\hat{\gamma}_{2k}$ are estimated using OLS estimators.

The \textbf{SLE} approximation is 
\begin{equation*}
    \hat{\pi}\left(\mathbf{x}_b\right) = \Lambda\left(\hat{\gamma}_0 + \hat{\gamma}_1\times \mathbf{x}_1 + \sum^K_{k=1}\hat{\gamma}_{2k}\times \mathbf{x}^k_2\right)
\end{equation*}
where $\Lambda(x) = \frac{\exp(x)}{1+\exp(x)}$ is the logistic function. Higher order binary variables are not considered here since $\mathbf{x}_1^k=\mathbf{x}_1$ for any value $k>1$.

\paragraph*{Doubly robust DiD} \citet{sant2020doubly} adjust for time-invariant covariates in a doubly robust way, by combining inverse-propensity score weighting with outcome modeling. 

\paragraph*{Timing varying covariates}
With finite $T$, strictly exogenous time-varying covariates $X_{it}$ can be converted to time invariant $X_i\equiv \left(X_{i1},\cdots,X_{iT}\right)$, in practice, applied researchers only rely on linear specifications with contemporaneous covariates instead.

\citet{sant2020doubly} also assume that covariates and treatment status are stationary as \citet{abadie2005semiparametric}. Let $T_i$ be a dummy variable that takes value one if the observation $i$ is only observed in the post-treatment period, and 0 if only observed in the pre-treatment period. Define $Y_i = T_iY_{i1}+\left(1-T_i\right)Y_{i0}$. Let $n_1$ and $n_0$ be the sample size of the post- and pre-treatment periods such that $n=n_1+n_0$, and let $\lambda = \mathbb{P}\left(T=1\right)\in \left(0,1\right)$:
\begin{assumption}{Main assumptions of \citet{sant2020doubly}}{santanna_zhao}
    Assume that
    \begin{itemize}
        \item[1] the data $\left\{Y_{i0},Y_{i1},D_i,X_i\right\}^n_{i=1}$ are \textbf{i.i.d.}, or the pooled repeated cross-section data $\left\{Y_i,D_i,X_i,T_i\right\}^n_{i=1}$ consisting of i.i.d. draws from the mixture distribution
        \begin{align*}
            \mathbb{P}\left(Y\leq y, D=d, X\leq x, T=t\right) =& t\cdot \lambda \cdot \mathbb{P}\left(Y_1\leq y, D=d, X\leq x \mid T=1\right)\\
            &+ \left(1-t\right)\cdot \left(1-\lambda\right) \mathbb{P}\left(Y_0\leq y, D=d, X\leq x \mid T=0\right)
        \end{align*}
        where $\left(y,d,x,t\right)\in \mathbb{R}\times \left\{0,1\right\}\times \mathbb{R}^k\times \left\{0,1\right\}$, with the joint distribution of $\left(D,X\right)$ invariant to $T$.
        \item[2] \textbf{Conditional Parallel Trend Assumption (PTA)}\footnote{It allows for covariate-specific time trends but not unit specific trends.}: $$ \mathbb{E}\left[Y_1(0)-Y_0(0)\mid D=1,X\right] \overset{a.s.}{=} \mathbb{E}\left[Y_1(0)-Y_0(0)\mid D=1,X\right] $$
        \item[3] $\exists \epsilon>0$, $\mathbb{P}\left(D=1\right)>\epsilon$ and $\mathbb{P}\left(D=1\mid X\right)\leq 1-\epsilon$ a.s.\footnote{This overlapping condition states that at least a small fraction of the population is treated and that for every value of $X$, at least a small probability that the unit is not treated.}
    \end{itemize}
\end{assumption}
Under Assumption \ref{assump:santanna_zhao}, there are 2 main flexible estimation procedures to estimate the ATT:
\begin{itemize}
    \item[1] outcome regression (\textbf{OR}) approach
    \begin{equation*}
        \hat{\tau}^{\mathrm{reg}} = \bar{Y}_{1,1}-\left[ \bar{Y}_{1,0} + n^{-1}_{\mathrm{treat}} \sum_{i\mid D_i=1}\left(\hat{\mu}_{0,1}\left(X_i\right) - \hat{\mu}_{0,0}\left(X_i\right) \right) \right]
    \end{equation*}
    where $\bar{Y}_{d,t}= \sum_{i\mid D_i=d,T_i=t}Y_{it}/n_{d,t}$ is the sample average outcome among units in treatment group $d$ and time $t$, $\hat{\mu}_{d,t}(x)$ is an estimator of the unknown $m_{d,t}(x)\equiv \mathbb{E}\left[Y_t\mid D=d,X=x\right]$
    \item[2] inverse propensity weighting (\textbf{IPW}) approach, as in \citet{abadie2005semiparametric}.
\end{itemize}

\citet{sant2020doubly} proposed to combine both the \textbf{OR} and \textbf{IPW} approaches to form the doubly robust (\textbf{DR}) moments/estimands for the ATT.

\subparagraph*{Notation} Let $\pi(X)$ be an arbitrary model for the true, unknown propensity score.
\begin{itemize}
    \item with proper panel data, let $\Delta Y= Y_1-Y_0$ and define $\mu^p_{d,\Delta}(X)\equiv \mu^p_{d,1}(X)-\mu^p_{d,0}(X)$ being a model for the true, unknown outcome regression $m^p_{d,t}(x)\equiv \mathbb{E}\left[Y_t\mid D=d,X=x\right]$, $d,t=0,1$. 
    \item with repeated cross-section data, let $\mu_{d,t}^{rc}(x)$ be an arbitrary model for the true, unknown regression $m^{rc}_{d,t}(x)\equiv \mathbb{E}\left[Y\mid D=d,T=t,X=x\right], d,t=0,1$, $\mu_{d,Y}^{rc}(T,X)\equiv T\cdot \mu^{rc}_{d,1}(X)+(1-T)\cdot \mu^{rc}_{d,0}(X)$, and $\mu^{rc}_{d,\Delta}(X)\equiv \mu^{rc}_{d,1}(X)-\mu^{rc}_{d,0}(X)$.
\end{itemize}

\paragraph*{Estimands} consider
\begin{itemize}
    \item for proper panel data: 
    \begin{equation*}
        \tau^{dr,p} = \mathbb{E}\left[ \left(w^p_1(D)-w^p_0\left(D,X;\pi\right)\right) \left(\Delta Y-\mu^p_{0,\Delta}(X)\right) \right]
    \end{equation*}
    where, for a generic $g$,
    \begin{align*}
        w^p_1(D) &= \frac{D}{\mathbb{E}\left[D\right]} & w^p_0\left(D,X;g\right)&= \frac{g(X)(1-D)}{1-g(X)}\cdot \left(\mathbb{E}\left[\frac{g(X)(1-D)}{1-g(X)}\right]\right)^{-1}
    \end{align*}
    \item for repeated cross-section data, consider 2 different estimands
    \begin{align*}
        \tau_1^{dr,rc} =& \mathbb{E}\left[\left(w^{rc}_1(D,T)-w_0^{rc}\left(D,T,X;\pi\right)\right) \cdot \left(Y-\mu^{rc}_{0,Y}(T,X)\right)\right] \\
        \tau^{dr,rc}_2 =& \tau^{dr,rc}_1 + \left(\mathbb{E}\left[\mu^{rc}_{1,1}(X)-\mu^{rc}_{0,1}(X)\mid D=1\right] -\mathbb{E}\left[\mu^{rc}_{1,1}(X)-\mu^{rc}_{0,1}(X)\mid D=1,T=1\right] \right)\\
        &- \left( \mathbb{E}\left[\mu^{rc}_{1,0}(X)-\mu^{rc}_{0,0}(X)\mid D=1\right] - \mathbb{E}\left[\mu^{rc}_{1,0}(X)-\mu^{rc}_{0,0}(X)\mid D=1,T=0\right] \right)
    \end{align*}
    where for a generic $g$,
    \begin{align*}
        w^{rc}_1(D,T)&=w^{rc}_{1,1}(D,T)-w^{rc}_{1,0}(D,T) & w^{rc}_0\left(D,T,X;g\right) &=w^{rc}_{0,1}\left(D,T,X;g\right) - w^{rc}_{0,0}\left(D,T,X;g\right) 
    \end{align*}
    and for $t=0,1$
    \begin{align*}
        w^{rc}_{1,t}\left(D,T\right) &= \frac{D\cdot 1\left\{T=t\right\}}{\mathbb{E} \left[D\cdot 1\left\{T=t\right\}\right]}\\
        w^{rc}_{0,t}\left(D,T,X;g\right) &= \frac{g(X)\left(1-D\right)\cdot 1\left\{T=t\right\}}{1-g(X)} \left(\mathbb{E}\left[\frac{g(X)\left(1-D\right)\cdot 1\left\{T=t\right\}}{1-g(X)}\right]\right)^{-1}
    \end{align*}
\end{itemize}
Then if at least 
\begin{itemize}
    \item for \textbf{panel} data, either $\pi(X)\overset{a.s.}{=} p(X)$ or $\mu^p_{\Delta}(X) \overset{a.s.}{=} m^p_{0,1}(X)-m^p_{0,0}(X)$
    \item for \textbf{repeated cross-section} data, either $\pi(X)\overset{a.s.}{=} p(X)$ or $\mu^p_{0,\Delta}(X) \overset{a.s.}{=} m^{rc}_{0,1}(X)-m^{rc}_{0,0}(X)$\footnote{For repeated cross-section data, $\tau^{dr,rc}_1$ does not reply on OR models for the treated group but $\tau_2^{dr,rc}$ does, however, $\tau^{dr,rc}_1$ is not more robust against model misspecification than $\tau_2^{dr,rc}$ since they identify the ATT under the same conditions. Given that $\mathbb{E}\left[g(X)\mid D=1\right] = \mathbb{E}\left[g(X)\mid D=1,T=t\right],t=0,1$ holds for any $g(\cdot)$, it must hold for $\mu^{rc}_{1,t}(\cdot)-\mu^{rc}_{0,t}(\cdot),t=0,1$, even when $\mu^{rc}_{d,t}(\cdot)$ are misspecified.} 
\end{itemize}
that is, at least one of the working nuisance models is correctly specified, the ATT can be estimated. This is less demanding than both OR and IPW approach.

\paragraph*{Semiparametric efficiency bound} 
Let $m^p_{0,\Delta} \equiv m^p_{0,1}(x)-m^p_{0,0}(x)$ and $m^{rc}_{d,\Delta}(X)\equiv m^{rc}_{0,1}(X)-m^{rc}_{0,0}(X)$ for $d=0,1$. Then 
\begin{itemize}
    \item for \textbf{panel} data, the \textbf{efficient influence function} for the ATT is
    \begin{align*}
        \eta^{e,p}\left(Y_1,Y_0,D,X\right) =& w^p_1(D)\left(m^p_{1,\Delta}(X) - m^p_{0,\Delta}(X) -\tau \right) \\
        &+ w_1^p(D)\left(\Delta Y-m^p_{1,\Delta}(X)\right) - w^p_0(D,X;p)\left(\Delta Y-m^p_{0,\Delta}(X)\right)
    \end{align*}
    and the \textbf{semiparametric efficiency bound} for all regular ATT estimators is
    \begin{align*}
        \mathbb{E}\left[\eta^{e,p}\left(Y_1,Y_0,D,X\right)\right]^2 =& \frac{1}{\mathbb{E}\left[D\right]^2} \left[ D\left(m^p_{1,\Delta}(X)-m^p_{0,\Delta}(X)-\tau\right)^2 \right. \\
        & \left. + D\left(\Delta-m^p_{1,\Delta}(X)\right)^2 + \frac{(1-D)p(X)^2}{\left(1-p(X)\right)^2} \left(\Delta Y-m^p_{0,\Delta}(X)\right)^2 \right]
    \end{align*}
    \item for \textbf{repeated cross-section} data, the \textbf{efficient influence function} for the ATT is
    \begin{align*}
        \eta^{e,rc}\left(Y,D,T,X\right) =& \frac{D}{\mathbb{E}[D]}\left(m^{rc}_{1,\Delta}(X)-m^{rc}_{0,\Delta}(X)-\tau\right) \\
        &+\left( w^{rc}_{1,1}(D,T)\left(Y-m^{rc}_{1,1}(X)\right)-w^{rc}_{1,0}(D,T)\left(Y-m^{rc}_{1,0}(X)\right) \right)\\
        &- \left(w^{rc}_{0,1}(D,T,X;p)\left(Y-m^{rc}_{0,1}(X)\right) - w^{rc}_{0,0}(D,T,X;p)\left(Y-m^{rc}_{0,0}(X)\right) \right)
    \end{align*}
    and the \textbf{semiparametric efficiency bound} for all regular ATT estimators is
    \begin{align*}
        \mathbb{E}\left[\eta^{e,rc}\left(Y,D,T,X\right)^2\right] = \frac{1}{\mathbb{E}[D]^2} \mathbb{E}& \left[ D \left(m^{rc}_{1,\Delta}(X)-m^{rc}_{0,\Delta}(X)-\tau\right)^2 \right. \\
        & + \frac{DT}{\lambda^2}\left(Y-m^{rc}_{1,1}(X)\right)^2 + \frac{D(1-T)}{(1-\lambda)^2}\left(Y-m^{rc}_{1,0}(X)\right)^2 \\
        &+ \left. \frac{(1-D)p(X)^2T}{\left(1-p(X)\right)^2\lambda^2}\left(Y-m^{rc}_{0,1}(X)\right)^2 + \frac{(1-D)p(X)^2 (1-T)}{\left(1-p(X)\right)^2(1-\lambda)^2}\left(Y-m^{rc}_{0,0}(X)\right)^2 \right]
    \end{align*}
\end{itemize}

Both $\eta^{e,p}$ and $\eta^{e,rc}$ depends on the true, unknown, outcome regression functions for the treated group, $m_{1,1}(\cdot)$ and $m_{1,0}(\cdot)$ in an asymmetric manner.

The key difference between the two estimators is that for panel data, 
\begin{align*}
    \eta^{e,p}\left(Y_1,Y_0,D,X\right) =& w^p_1(D)\left(m^p_{1,\Delta}(X) - m^p_{0,\Delta}(X) -\tau \right) \\
    &+ w_1^p(D)\left(\Delta Y-m^p_{1,\Delta}(X)\right) - w^p_0(D,X;p)\left(\Delta Y-m^p_{0,\Delta}(X)\right) \\
    =& \left[w^p_1(D) - w^p_0(D,X;p)\right] Y \left[\Delta Y - m_{0,\Delta}(X)\right] - w_1^p(D)\cdot\tau 
\end{align*}
which ends up \textbf{not} depending are $m_{1,1}(\cdot)$ or $m_{1,0}(\cdot)$. 

Comparing the efficiency bound of the two cases, we have, if $T$ is independent of $\left(Y_1,Y_0,D,X\right)$,
\begin{align*}
    &\mathbb{E}\left[\eta^{e,rc}\left(Y,D,T,X\right)^2\right] - \mathbb{E}\left[\eta^{e,p}\left(Y_1,Y_0,D,X\right)^2\right]\\
    =& \frac{1}{\mathbb{E}\left[D\right]^2} \left[ D\left(\sqrt{\frac{1-\lambda}{\lambda}}\left(Y_1-m_{1,1}(X)\right) + \sqrt{\frac{\lambda}{1-\lambda}}\left(Y_0-m_{1,0}(X)\right)\right)^2 \right.\\
    & \left. + \frac{(1-D)P(X)^2}{\left(1-p(X)\right)^2} \left(\sqrt{\frac{1-\lambda}{\lambda}}\left(Y_1-m_{0,1}(X)\right)+\sqrt{\frac{\lambda}{1-\lambda}}\left(Y_0-m_{0,0}(X)\right)  \right)^2 \right] \geq 0
\end{align*}
which gives that under the DiD framework, it is possible to form more efficient estimators for the ATT when the panel data are available.

This result also gives an efficiency-loss-minimizing $\lambda$:
\begin{align*}
    \lambda &= \frac{\tilde{\sigma}_1}{\tilde{\sigma}_0+ \tilde{\sigma}_1} & \text{where }\tilde{\sigma}^2_t &=\mathbb{E}\left[D\left(Y_t-m_{1,t}(X)\right)^2 + \frac{(1-D)p(X)^2}{\left(1-p(X)\right)^2}\left(Y_t-m_{0,t}(X)\right)^2 \right], t=0,1
\end{align*}
hence, in principle, one may benefit from \textit{oversampling} from either the pre- or post-treamtent period. But it's generally infeasible to do so during the design stage since $\tilde{\sigma}_1^2$ depends on post-treatment data. \citet{sant2020doubly} recommand $\lambda=0.5$ for DiD with repeated cross-section units as a reasonable choice.

\paragraph*{Estimation and inference} 
\citet{sant2020doubly} proposed a two-step procedure for estimation:
\begin{itemize}
    \item first, estimate the true, unknown $p(\cdot)$ with $\pi(\cdot)$, the true unknown $m^p_{d,t}(\cdot)$ and $m^{rc}_{d,t}(\cdot)$ with $\mu^p_{d,t}(\cdot)$ and $\mu^{rc}_{d,t}(\cdot)$, $d,t=0,1$
    \item second, plug the fitted values of the estimated propensity score and regression models into the sample analogue of $\tau^{dr,p}$, $\tau^{dr,rc}_1$, $\tau^{dr,rc}_2$
\end{itemize}
instead of using semi-parametric estimators as \citet{abadie2005semiparametric}, \citet{sant2020doubly} use generic parametric estimators for the first step, assuming:
\begin{itemize}
    \item $\pi\left(x;\gamma^*\right)$ is a parametric model for $p(x)$ s.t. $\pi(\cdot)$ is known up to the \textbf{finite} dimensional pseudo-true $\gamma^*$
    \item for $d,t=0,1$, $\mu^p_{d,t}\left(x;\beta^{*,p}_{d,t}\right)$ and $\mu^{rc}_{d,t}\left(x;\beta^{*,rc}_{d,t}\right)$ s.t. they are known up to the finite dimensional pseudo-true parameter $\beta^{*,p}_{d,t}$ and $\beta^{*,rc}_{d,t}$
\end{itemize}
this approach is most suitable when the sample size is moderate and the dimension of covariates is high. The estimations are 
\begin{itemize}
    \item for \textbf{panel data}
    \begin{equation*}
        \hat{\tau}^{dr,p} = \mathbb{E}_n\left[\left(\hat{w}^p_1(D)-\hat{w}^p_0\left(D,X;\hat{\gamma}\right)\right) \left(\Delta Y- \mu^p_{0,\Delta}\left(X;\hat{\beta}^p_{0,0},\hat{\beta}^p_{0,1}\right)\right) \right]
    \end{equation*}
    where 
    \begin{align*}
        \hat{w}^p_1(D)&=\frac{D}{\mathbb{E}_n[D]} & \hat{w}^p_0\left(D,X;\gamma\right)&= \frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)} \left(\mathbb{E}_n\left[\frac{\pi(X;\gamma)(1-D)}{1-\pi(X;\gamma)}\right]\right)^{-1}
    \end{align*}
    \item for \textbf{repeated cross-section data},
    \begin{align*}
        \hat{\tau}^{dr,rc}_1 =& \mathbb{E}_n \left[ \left(\hat{w}^{rc}_1(D,T) - \hat{w}_0^{rc}(D,T,X;\hat{\gamma}) \right) \left(Y-\mu^{rc}_{0,Y}\left(T,X;\hat{\beta}^{rc}_{0,0},\hat{\beta}^{rc}_{0,1}\right)\right) \right] \\
        \hat{\tau}^{dr,rc}_2 =& \hat{\tau}_1^{dr,rc} + \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,1}(D,T) \right) \left(\mu^{rc}_{1,1}\left(X;\hat{\beta}^{rc}_{1,1}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{rc}_{0,1}\right)\right) \right]\right) \\
        & - \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,0}(D,T) \right) \left(\mu^{rc}_{1,0}\left(X;\hat{\beta}^{rc}_{1,0}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{rc}_{0,0}\right)\right) \right]\right)
    \end{align*}
    where
    \begin{align*}
        \mu^{rc}_{0,Y}\left(Y,X;\beta^{rc}_{0,0},\beta^{rc}_{0,1}\right) &= T\cdot \mu^{rc}_{0,1}\left(\cdot;\beta^{rc}_{0,1}\right)+(1-T)\mu^{rc}_{0,0}\cdot \left(\cdot; \beta^{rc}_{0,0}\right) \\
        \mu^{rc}_{d,\Delta}\left(\cdot;\beta^{rc}_{d,1},\beta^{rc}_{d,0}\right) &= \mu^{rc}_{d,1}\left(\cdot;\beta^{rc}_{d,1}\right) - \mu^{rc}_{d,0}\left(\cdot;\beta^{rc}_{d,0}\right)
    \end{align*}
\end{itemize}
These estimators can be improved to achieve not only \textbf{consistency} doubly robustness, but also \textbf{inference} doubly robustness\footnote{This way, there is no estimation effect from first-step estimators, the asymptotic variance of the results DR DiD estimator for the ATT is invariant to which working modesl for the nuisance functions are correctly specified. This in practice usually translates to simpler and more stable inference procedures.}. 
For the improvement, \citet{sant2020doubly} assume, in addition, 
\begin{itemize}
    \item linear regression working models, for the outcome of interest
    \item a logistic working model, for the propensity score
    \item covariates $X$ entering all nuisance models in a symemtric manner 
\end{itemize}
which are more stringent than the generic DR DiD estimators, but weaker than TWFE estimators. Under such assumptions, we have the improved DR DiD estimators 
\begin{itemize}
    \item for \textbf{panel data}, the 3-step estimator is given as 
    \begin{equation*}
        \hat{\tau}^{dr,p}_{imp} = \mathbb{E}_n\left[ \left(\hat{w}^p_1(D)-\hat{w}^p_0 \left(D,X;\hat{\gamma}^{ipt}\right)\right) \left( \Delta Y-\mu^{lin,p}_{0,\Delta} \left(X;\hat{\beta}^{wls,p}_{0,\Delta}\right) \right) \right]
    \end{equation*}
    the first two steps compute 
    \begin{align*}
        \hat{\gamma}^{ipt} &= \arg\max_{\gamma\in \Gamma}\mathbb{E}_n\left[DX'\gamma -(1-D)\exp(X'\gamma)\right], & \hat{\beta}^{wls,p}_{0,\Delta} & \arg\min_{b\in\Theta}\mathbb{E}_n \left[ \frac{\Lambda\left(X'\hat{\gamma}^{ipt}\right)}{1-\Lambda\left(X'\hat{\gamma}^{ipt}\right)} \left(\Delta Y-X'b\right)^2 \mid D=0 \right]
    \end{align*}
    where $\hat{\gamma}^{ipt}$ is the inverse probability tilting estimator, $\hat{\beta}^{wls,p}_{0,\Delta}$ is the weighted least squares estimator for $\beta^{*,p}_{0,\Delta}$.
    In the last step, plug the fitted value of the working models for the nuisance functions 
    \begin{align*}
        \pi(X,\gamma) &= \Lambda\left(X'\gamma\right) \equiv \frac{\exp(X'\gamma)}{1+\exp (X'\gamma)} & \mu^p_{0,\Delta}\left(X;\beta^p_{0,1},\beta^p_{0,1}\right)&= \mu^{lin,p}_{0,\Delta}\left(X;\beta^p_{0,\Delta}\right)\equiv X'\beta^p_{0,\Delta}
    \end{align*}
    into the sample analogue of $\tau^{dr,p}$. \citet{sant2020doubly} show that if 
    \begin{align*}
        \mathbb{E}\left[ \left(\frac{D}{\mathbb{E}\left[D\right]} - \frac{\exp(X'\gamma^*)(1-D)}{\mathbb{E}\left[\exp(X'\gamma^*)(1-D)\right]}\right) X \right] &= 0 & \mathbb{E}\left[\exp (X'\gamma^*) \left(\Delta Y-\mu^{lin,p}_{0,\Delta}\left(X;\beta^*_{0,\Delta}\right)\right)X \mid D=0 \right] &=0
    \end{align*}
    there will be no estimation effect from the first stage, with the linear outcome models and logistic propensity score models assumed. 
    
    As $n\rightarrow \infty$, these 2 moment conditions follow from the first-order conditions of the optimization problems associated with $\hat{\gamma}^{ipt}$ and $\hat{\beta}^{wls,p}_{0,\Delta}$, even when the working models are misspecified. Hence, replacing the pseudo-true parameters $\gamma^{*,ipt}$ and $\beta^{*,wls,p}_{0,\Delta}$ with their estimators $\hat{\gamma}^{ipt}$ and $\hat{\beta}^{wls,p}_{0,\Delta}$ guarantee that $\hat{\tau}^{dr,p}_{imp}$ is doubly robust:
    $$
    \hat{\tau}^{dr,p}_{imp} \xrightarrow{p} \tau,\ \  \text{ if \textbf{either} $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{or} $X'\beta^{*,wls,p}_{0,\Delta}\overset{a.s.}{=}m^p_{0,\Delta}(X)$}
    $$
    As for inference, $\hat{\tau}^{dr,p}_{imp}$ is $\sqrt{n}-$consistent and asymptotically normal
    \begin{align*}
        \sqrt{n}\left(\hat{\tau}^{dr,p}_{imp}-\tau^{dr,p}_{imp}\right) &= \frac{1}{\sqrt{n}} \sum^n_{i=1}\eta^{dr,p}_{imp} \left(W; \gamma^{*,ipt},\beta^{*,wls,p}_{0,\Delta},\tau^{dr,p}_{imp}\right) + o_p(1) \xrightarrow{d} \mathcal{N}\left(0,V^p_{imp}\right)
    \end{align*}
    and if \textbf{both}  $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{and} $X'\beta^{*,wls,p}_{0,\Delta}\overset{a.s.}{=}m^p_{0,\Delta}(X)$, $V^p_{imp}$ equals to the semiparametrically efficiency bound, and it can be estimated as 
    \begin{equation*}
        \hat{V}^p_{imp} = \mathbb{E}_n\left[\eta^{dr,p}_{imp}\left(W;\hat{\gamma}^{ipt},\hat{\beta}^{wls,p}_{0,\Delta},\hat{\tau}^{dr,p}_{imp}\right)^2\right]
    \end{equation*}
    \item for \textbf{repeated cross-section data}, again assume the working models
    \begin{align*}
        \pi(X,\gamma) &= \Lambda\left(X'\gamma\right) \equiv \frac{\exp(X'\gamma)}{1+\exp (X'\gamma)} & \mu^{rc}_{d,t}\left(X;\beta^{rc}_{d,t}\right)&= \mu^{lin,rc}_{d,t}\left(X;\beta^{rc}_{d,t}\right)\equiv X'\beta^{rc}_{d,t}
    \end{align*}
    then the two improved estimators are given as 
    \begin{align*}
        \hat{\tau}^{dr,rc}_{1,imp} =& \mathbb{E}_n \left[ \left(\hat{w}^{rc}_1(D,T) - \hat{w}_0^{rc}\left(D,T,X;\hat{\gamma}^{ipt}\right) \right) \left(Y-\mu^{lin,rc}_{0,Y}\left(X;\hat{\beta}^{wls,rc}_{0,0},\hat{\beta}^{wls,rc}_{0,1}\right)\right) \right] \\
        \hat{\tau}^{dr,rc}_{2,imp} =& \hat{\tau}_{1,imp}^{dr,rc} + \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,1}(D,T) \right) \left(\mu^{rc}_{1,1}\left(X;\hat{\beta}^{ols,rc}_{1,1}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{wls,rc}_{0,1}\right)\right) \right]\right) \\
        & - \left(\mathbb{E}_n \left[ \left( \frac{D}{\mathbb{E}_n[D]} -\hat{w}^{rc}_{1,0}(D,T) \right) \left(\mu^{rc}_{1,0}\left(X;\hat{\beta}^{ols,rc}_{1,0}\right) - \mu^{rc}_{0,1}\left(X;\hat{\beta}^{wls,rc}_{0,0}\right)\right) \right]\right)
    \end{align*}
    where 
    \begin{align*}
        \hat{\gamma} &= \arg\max_{\gamma\in\Gamma} \mathbb{E}_n \left[DX'\gamma - (1-D)\exp(X'\gamma)\right]\\
        \hat{\beta}^{wls,rc}_{0,t} &= \arg\min_{b\in\Theta} \mathbb{E}_n \left[ \frac{\Lambda \left(X'\hat{\gamma}^{ipt}\right)}{1-\Lambda \left(X'\hat{\gamma}^{ipt}\right)} (Y-X'b)^2 \mid D=0,T=t \right]\\
        \hat{\beta}^{ols,rc}_{1,t} &= \arg\min_{b\in\Theta} \mathbb{E}_n \left[ (Y-X'b)^2 \mid D=1, T=t \right]
    \end{align*}
    OLS is adopted to estimate $\beta^{*,rc}_{1,t},t=0,1$ as there is no estimation effect. 
    
    Let 
    \begin{equation*}
        \tau^{dr,rc}_{imp} = \mathbb{E}\left[ \left(w^{rc}_1(D,T) - w^{rc}_0 \left(D,T,X;\gamma^{*,ipt}\right)\right) \left(Y-\mu^{lin,rc}_{0,Y}\left(T,X;\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right)\right) \right]
    \end{equation*}
    and for $\beta^{*,rc}_{imp}=\left(\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0},\beta^{*,ols,rc}_{1,1},\beta^{*,ols,rc}_{1,0}\right)$, define 
    \begin{align*}
        \eta^{dr,rc}_{1,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) &= \eta^{rc,1}_1 \left(W;\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right) -  \eta^{rc,1}_0 \left(W;\gamma^{*,ipt},\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right) \\
        \eta^{dr,rc}_{2,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) &= \eta^{rc,2}_1 \left(W;\beta^{*,rc}_{imp}\right) - \eta^{rc,1}_1 \eta^{rc,2}_0\left(W;\gamma^{*,ipt},\beta^{*,wls,rc}_{0,1},\beta^{*,wls,rc}_{0,0}\right)
    \end{align*}
    
    Let $n=n_1+n_0$, where $n_1$ and $n_0$ are the sample sizes of the post- and pre-treatment periods respectively. If $n_1/n\xrightarrow{p}\lambda\in(0,1)$ as $n_0,n_1\rightarrow\infty$, then
    $$
    \hat{\tau}^{dr,rc}_{j,imp} \xrightarrow{p} \tau,\ \  \text{ if \textbf{either} $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{or} $X'\beta^{*,wls,rc}_{0,1}-X'\beta^{*,wls,rc}_{0,0}\overset{a.s.}{=}m^{rc}_{0,\Delta}(X)$}
    $$
    As for inference, $\hat{\tau}^{dr,p}_{imp}$ is $\sqrt{n}-$consistent and asymptotically normal
    \begin{align*}
        \sqrt{n}\left(\hat{\tau}^{dr,rc}_{j,imp}-\tau\right) &= \frac{1}{\sqrt{n}} \sum^n_{i=1}\eta^{dr,rc}_{j,imp} \left(W; \gamma^{*,ipt},\beta^{*,rc}_{imp}\right) + o_p(1) \xrightarrow{d} \mathcal{N}\left(0,V^{rc}_{j,imp}\right)
    \end{align*}
    and if \textbf{both}  $\Lambda(X'\gamma^{*,ipt})\overset{a.s.}{=}p(X)$ \textbf{and} $X'\beta^{*,wls,rc}_{d,t}\overset{a.s.}{=}m^{rc}_{d,t}(X)$, $\eta^{dr,rc}_{2,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right) \overset{a.s.}{=} \eta^{e,rc}\left(Y,D,T,X\right)$, $V^{rc}_{2,imp}$ equals to the semiparametrically efficiency bound, $V^{rc}_{1,imp}$ does \textbf{not}, where
    \begin{equation*}
        {V}^{rc}_{j,imp} = \mathbb{E}_n\left[\eta^{dr,rc}_{j,imp}\left(W;\gamma^{*,ipt},\beta^{*,rc}_{imp}\right)^2\right]
    \end{equation*}
    and the efficiency loss of using $\hat{\tau}^{dr,rc}_{1,imp}$ instaed of $\hat{\tau}^{dr,rc}_{2,imp}$ is 
    \begin{align*}
        V^{rc}_{1,imp} - V^{rc}_{2,imp} = \mathbb{E} [D]^{-1} \cdot \mathrm{Var} \left[ \sqrt{\frac{1-\lambda}{\lambda}} \left(m^{rc}_{1,1}(X)-m^{rc}_{0,1}(X)\right) + \sqrt{\frac{\lambda}{1-\lambda}} \left(m^{rc}_{1,0}(X)-m^{rc}_{0,0}(X)\right) \mid D=1\right]\geq 0
    \end{align*}
\end{itemize}

\subsubsection{Unconfoundedness}

Viewing the pre-treatment outcomes as covariates, then one can assume \myhl[myblue]{\textbf{unconfoundedness}}:
\begin{equation*}
    D_i \bot \left(Y_{iT}(0),Y_{iT}(1)\right) \mid Y_{i1},\cdots,Y_{iT-1}
\end{equation*}

under this assumption, one can apply the large literature of treatment effect estimation under unconfoundedness \citep{imbens2004nonparametric} or modern approaches \citep{bang2005doubly,chernozhukov2017double,athey2018approximate}.

\citet{imbens2004nonparametric} pointed out 3 arguments for the assumption of unconfoundedness
\begin{itemize}
    \item statistical motivation: the unconfoundedness assumption is logically nature in program evaluation 
    \item purpose: the unconfoundedness assumption asserts that all variables that need to be adjusted for are observed by the researcher
    \item even when agents choose their treatment optimally, two agents with the same values for observed characteristics may differ in their treatment choices without invalidating the unconfoundedness assumption if the difference in their choices is driven by differences in unobserved characteristics that are themselves unrelated to the outcomes of interest.
\end{itemize}

\citet{imbens2004nonparametric} proposes that is is sufficient to assume a weaker version of unconfoundedness, \textbf{mean independence} $$ \mathbb{E}\left[Y_{iT}(0),Y_{iT}(1) \mid D_i, Y_{i1},\cdots,Y_{iT-1}\right] = \mathbb{E}\left[ Y_{iT}(0),Y_{iT}(1) \mid Y_{i1},\cdots,Y_{iT-1} \right] $$
for population ATE. 

Denote 
$$\mu_d = \mathbb{E}_d(Y_{i1},\cdots,Y_{iT-1}) = \mathbb{E}\left[Y\mid D_i=d,Y_{i1},\cdots,Y_{iT-1} \right]$$
for $d=0,1$, \citet{imbens2004nonparametric} reviews 5 groups of estimation for ATEs under unconfoundedness, 

\paragraph*{A. Regression for population/sample/conditional ATE}
we have the estimand
\begin{equation*}
    \hat{\tau}_{reg} = \frac{1}{N} \sum^N_{i=1}\left(\hat{\mu}_1-\hat{\mu}_0\right) = \frac{1}{N}\sum^N_{i=1}D_i\cdot\left(Y_i - \hat{\mu}_0\right) + (1-D_i)\cdot\left(\hat{\mu}_1-Y_i\right)
\end{equation*}
\begin{itemize}
    \item \textbf{early estimators} for $\mu_d$ included parametric regression functions including least-square estimators with the regression function $\mu_d = \beta x+\tau\cdot d$, then one can use the regression $$ Y_i =\alpha + \beta'\left(Y_{i1},\cdots,Y_{iT-1}\right) + \tau W_i +\epsilon_i $$
    more generally, one can specify separate regressions for the two regimes $\mu_d = \beta_d x$, and estimate the two regressions separately.

    \underline{\textit{cons}}: the regression estimators may rely heavily on extrapolation, hence sensitive to changes in the specification of the models.
    \item \textbf{non-parametric estimators}
    \begin{itemize}
        \item \citet{hahn1998role} proposes to estimate first the 3 conditional expectations
        \begin{align*}
            g_1(x) &= \mathbb{E}\left[DY_t\mid Y_{1},\cdots,Y_{T-1}\right] & g_0(x) &= \mathbb{E}\left[(1-D)Y_t\mid Y_{1},\cdots,Y_{T-1}\right] & e(x)&= \mathbb{E}\left[D\mid Y_{1},\cdots,Y_{T-1}\right]
        \end{align*}
        nonparametrically using series methods, then estimate\footnote{For simplicity, let $X_i \equiv \left(Y_{i1},\cdots,Y_{iT-1}\right)$.}
        \begin{align*}
            \hat{\mu}_1(x) &= \frac{\hat{g}_1(x)}{\hat{e}(x)} & \hat{\mu}_0(x) &= \frac{\hat{g}_0(x)}{1-\hat{e}(x)}
        \end{align*}
        and show that the estimators for population ATE and ATT achieve the semiparametric efficiency bounds. Alternatively, one can use this series approach to directly estimate $\mu_d$.
        \item \citet{heckman1998characterizing,heckman1998matching} propose (local-linear) kernel methods, one simple form is 
        \begin{equation*}
            \hat{\mu}_d = \sum_{i:D_i=d} Y_{iT}\cdot K\left(\frac{X_i-x}{h}\right)/\sum_{i:D_i=d}K\left(\frac{X_i-x}{h}\right)
        \end{equation*}
        with kernel $K(\cdot)$ and bandwidth $h$. In the local linear kernel regression, $\mu_d$ is estimated as the intercept $\beta_0$ in the minimization problem 
        \begin{equation*}
            \min_{\beta_0,\beta_1} \sum_{i:D_i=d} \left[Y_i-\beta_0-\beta_1'(X_i-x)\right]^2 \cdot K\left(\frac{X_i-x}{h}\right)
        \end{equation*}
        for bias control, the order of the kernel should be at least as large as the dimension of the covariates: $\int_z z^{r}K(z)\mathrm{d}z=0$, for $r\leq \mathrm{dim}(X)$.
    \end{itemize}
\end{itemize}
for the population ATT, it is important to note that with the propensity score known, the average $\sum D_iY_i/N_T$ is not efficient for the population expectation $\mathbb{E}\left[Y(1)\mid D=1\right]$. The efficient estimator can instead be obtained by weighting all the estimated treatment effects $\hat{\mu}_1-\hat{\mu}_0$ by the probability of receiving treatment:
\begin{equation*}
    \tilde{\tau}_{reg,T} = \frac{\sum^N_{i=1}e(X_i)\cdot \left[\hat{\mu}_1 - \hat{\mu}_0\right]}{\sum^N_{i=1}e(X_i)}
\end{equation*}
this allows one to exploit the control observations to adjust for imbalances in the sampling of the covariates.

\paragraph*{B. Matching}
similar to nonparametric kernel regression methods, matching estimators also impute the missing potential outcomes, but using \textbf{only} the outcomes of \textbf{nearest neighbors} of the opposite treatment group\footnote{What makes matching estimators more attractive is that the researcher only has to choose the \textbf{number of matches}, instead of smoothing parameters.}. Matching estimators often apply in settings where 
\begin{itemize}
    \item the interest is in the ATT
    \item there is a large reservoir of potential controls
\end{itemize}
the estimator is essentially the difference between 2 sample means, the variance is calculated using standard methods for differences in means or paired randomized experiments. The biggest challenge mostly comes from computation.

\citet{abadie2002simple} propose that: again, for a sample $\left\{ \left(Y_i,X_i,D_i\right) \right\}^N_{i=1}$, let $l_m(i)$ be the index that satisfies $D_l\neq D_i$, and 
    \begin{equation*}
        \sum_{j\mid D_j\neq D_i} \mathbf{1} \left\{ \left\Vert X_j-X_i \right\Vert \leq \left\Vert X_l-X_i \right\Vert \right\} = m
    \end{equation*}
    then $l_m(i)$ is the index of the unit in the opposite treatment group that is the $m^{th}$ closest to unit $i$ based on norm $\left\Vert \cdot \right\Vert$ distance, and $l_1(i)$ is the nearest match. Let the set of indices for the first $M$ matches for unit $i$ be $\mathcal{L}_M(i) = \left\{l_1(i),\cdots,l_M(i)\right\}$, then define the imputed potential outcomes as 
    \begin{align*}
        \hat{Y}_i(0) &= \begin{cases}
            Y_i, & D_i=0\\
            \frac{1}{M} \sum_{j\in\mathcal{L}_M(i)} Y_j, & D_i=1
        \end{cases}
        & \hat{Y}_i(1) &= \begin{cases}
            \frac{1}{M} \sum_{j\in\mathcal{L}_M(i)} Y_j, & D_i=0 \\
            Y_i, & D_i=1
        \end{cases}
    \end{align*}
    the simple matching estimator is 
    \begin{equation*}
        \hat{\tau}^{sm}_M = \frac{1}{N} \sum^N_{i=1} \left[hat{Y}_i(1) - \hat{Y}_i(0)\right]
    \end{equation*}
    the bias of this estimator is $O\left(N^{-1/k}\right)$, where $k$ is the dimension of the covariates. \citet{imbens2004nonparametric} listed 3 caveats to \cite{abadie2002simple}'s result:
    \begin{itemize}
        \item[\textbf{1}] only continuous covariates should be counted in $k$: matching with discrete covariates will be exact in large samples
        \item[\textbf{2}] if only matching the treated and the number of potential controls is much larger, the bias can be ignored asymptotically 
        \item[\textbf{3}] the order of the bias may be high, but the actual bias may be small if the coefficients in the leading term are small\footnote{One such case is that biases for different units are at least partially offsetting.}
    \end{itemize}
    and these matching estimators are generally not efficient.

One key aspect of matching is the choice of distance metrics, one can choose 
\begin{itemize}
    \item standard Euclidean metric: $d_E\left(x,z\right)=\left(x-z\right)'\left(x-z\right)$
    \item the diagonal matrix of the inverse of the covariate variances $d_{AI}\left(x,z\right) = \left(x-z\right)'\mathrm{diag}\left(\Sigma^{-1}_X\right)\left(x-z\right)$
    \item Mahalanobis metric: $d_M(x,z) = \left(x-z\right)' \Sigma^{-1}_X \left(x-z\right)$, which reduces differences in covariates within matched pairs in all directions
    \item metrics depending on the correlation between covariates, treatment assignemnt and outcomes:
    \begin{itemize}
        \item weighting absolute differences by the coefficients in the propensity score $$d_{Z1}\left(x,z\right) = \sum^K_{k=1}\left\vert x_k-z_k \right\vert \cdot \left\vert \gamma_k \right\vert$$ where the propensity score has a logistic form $e(x) = \frac{\exp(x'\gamma)}{1+\exp(x'\gamma)}$
        \item weighting absolute differences by the coefficients in the regression function $$ d_{Z2}\left(x,z\right) = \sum^K_{k=1} \left\vert x_k-z_k \right\vert \left\vert \beta_k \right\vert $$ where the regression functions are linear $\mu_{d}(x) = \alpha_d  +x'\beta $
    \end{itemize}
    \citet{imbens2004nonparametric} comments that when the regression function is misspecified, matching with the particular metrics may lead to inconsistency.
\end{itemize}

\paragraph*{C. Propensity scores}
There are 3 main ways to use propensity scores in estimation:
\begin{itemize}
    \item \myhl[myblue]{\textbf{weighting}}: the units by the reciprocal of the probability of receiving the treatment can undo the imbalance of the covariate distributions conditional on the treatment assignment. Formally,
    \begin{align*}
        \mathbb{E} \left[\frac{DY}{e(X)}\right] = \mathbb{E} \left[\frac{DY(1)}{e(X)}\right] = \underbrace{\mathbb{E}\left[\mathbb{E}\left[ \frac{DY(1)}{e(X)} \mid X \right]\right] = \mathbb{E} \left[ \frac{e(X)\cdot \mathbb{E}\left[Y(1)\mid X\right]}{e(X)} \right]}_{\text{unconfoundedness}} = \mathbb{E}\left[Y(1)\right]
    \end{align*}
    and similarly $\mathbb{E}\left[\frac{(1-D)Y}{1-e(X)}\right]=\mathbb{E}\left[Y(0)\right]$, which imply 
    \begin{equation*}
        \tau^p = \mathbb{E} \left[\frac{D\cdot Y}{e(X)} - \frac{\left(1-D\right)\cdot Y}{1-e(X)}\right]
    \end{equation*} 
    it can be directly estimated as 
    \begin{equation*}
        \tilde{\tau} = \frac{1}{N} \sum^N_{i=1} \left[ \frac{D_i Y_i}{e(X_i)} - \frac{(1-D_i)\cdot Y_i}{1-e(X_i)} \right]
    \end{equation*}
    here, the weights do not necessarily add to 1\footnote{For the treated units, the weights added up to $\frac{1}{N}\sum^N_{i=1}W_i/e(X_i)$, which equals to 1 in expectation, but not so in any given sample.}. One can normalize the weights within subpopulations, which in the limits leads to the estimator proposed by \citet{hirano2003efficient}
    \begin{equation*}
        \hat{\tau}_{weight} = \frac{\sum^N_{i=1} \frac{D_i\cdot Y_i}{\hat{e}(X_i)}}{\sum^N_{i=1}\frac{D_i}{\hat{e}(X_i)}} - \frac{\sum^N_{i=1} \frac{(1-D_i)\cdot Y_i}{1-\hat{e}(X_i)} }{\sum^N_{i=1} \frac{1-D_i}{1-\hat{e}(X_i)}}
    \end{equation*}
    where they specify a sequence of functions of the covariates, such as power series $h_l(x),l=1,\cdots,\infty$, and choose a number of terms $L(N)$ as a function of the sample size, then esitmate the $L-$dimensional vector $\gamma_L$ in 
    \begin{equation*}
        \Pr \left(D=1\mid X=x\right) = \frac{\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \gamma_L \right]}{1+\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \gamma_L \right]}
    \end{equation*}
    by maximizing the associated likelihood function, and calculate the estimated propensity score as 
    \begin{equation*}
        \hat{e}(x) = \frac{\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \hat{\gamma}_L \right]}{1+\exp \left[ \left(h_1(x),\cdots,h_L(x)\right) \hat{\gamma}_L \right]}
    \end{equation*}
    a nonparametric estimator for $e(x)$ is efficient, ignoring the 2 regression functions\footnote{The finite-sample properties of the two approaches (nonparametrically estimating propensity scores versus regression functions) may be different, except for when there are only discrete covariates.}.
    
    For the treatment effects of the treated, weight the contribution for unit $i$ by the propensity score $e(x_i)$,
    \begin{align*}
        \hat{\tau}_{weight,tr} &= \frac{\sum^N_{i=1} D_i\cdot Y_i\cdot \frac{e(X_i)}{\hat{e}(X_i)} }{ \sum^N_{i=1} D_i \frac{e(X_i)}{\hat{e}(X_i)} } - \frac{\sum^N_{i=1} (1-D_i)\cdot Y_i\cdot \frac{e(X_i)}{\left(1-\hat{e}(X_i)\right)} }{ \sum^N_{i=1} (1-D_i) \frac{e(X_i)}{ \left(1-\hat{e}(X_i)\right) } } & \text{propensity scores known} \\
        \hat{\tau}_{weight,tr} &= \left[\frac{1}{N_1} \sum_{D_i=1}Y_i \right] - \left[ \frac{\sum_{i:D_i=0} Y_i \cdot \frac{\hat{e}(X_i)}{\left(1-\hat{e}(X_i)\right)} }{ \sum_{i:D_i=0} \frac{\hat{e}(X_i)}{\left(1-\hat{e}(X_i)\right)} }  \right] & \text{propensity scores unknown}
    \end{align*}

    \underline{\textbf{CAUTIOUS}}: the problem of choosing the smoothing parameters is relevant here too\footnote{\citet*{hirano2003efficient}'s series estimators require choosing the \textbf{number of terms} in the series, a kernel-version alternative requires choosing a bandwidth.}, but here one want to use nonparametric regression methods even if the propensity scores are known.
    
    \item \myhl[myblue]{\textbf{blocking}}: \citet{rosenbaum1983central} suggest using the (estimated) propsensity score divide the sample into $M$ blocks of units of approximately equal probability of treatment, letting $J_{im}$ be an indicator for unit $i$ being in block $m$. One way of implementing this is by dividing the unit interval into $M$ blocks with boundary values equal to $m/M$ for $m=1,\cdots,M-1$, s.t. 
    \begin{equation*}
        J_{im} = \mathbf{1} \left\{ \frac{m-1}{M} < e(X_i) < \leq \frac{m}{M} \right\},\ m=1,\cdots,M 
    \end{equation*}
    within each block there are $N_{dm}$ observations with treatment equal to $d$, $N_{dm}=\sum_i \mathbf{1}\left\{D_i=d, J_{im}=1\right\}$. Within each block, estimate the average treatment effect as if random assignment held:
    \begin{equation*}
        \hat{\tau}_m = \frac{1}{N_{1m}} \sum^N_{i=1} J_{im}W_iY_i - \frac{1}{N_{0m}}\sum^N_{i=1} J_{im}\left(1-D_i\right)Y_i
    \end{equation*}
    then estimate the overall average treatment effect as 
    \begin{equation*}
        \hat{\tau}_{block} = \sum^M_{m=1}\hat{\tau}_m \cdot \frac{N_{1m}+N_{0m}}{N}
    \end{equation*}
    for the treatment effect of the treated, one can weight the within-block average treatment effects by the number of treated units: 
    \begin{equation*}
        \hat{\tau}_{T,block} = \sum^M_{m=1} \hat{\tau}_m \cdot \frac{N_{1m}}{N_T}
    \end{equation*}
    \underline{\textbf{CAUTIOUS}}: The asymptotic properties of such estimators require establishing the relative relationship between the number of blocks and the sample size, so choosing the number of blocks becomes essential 
    \begin{itemize}
        \item \textit{starting point}: a single covariate, assuming normality, \textbf{5 blocks} removes $\geq 95\%$ of the bias
        \item \textit{balance check}: covariates whould be balanced within blocks 
        \item \textit{unbalanced blocks}: if the distributions of the covariates among treated and controled are different, one can 
        \begin{itemize}
            \item split the blocks into a number of subblocks if the propensity score itself is unbalanced.
            \item generalize the specification of the propensity score, if the score is balanced but covariates not
        \end{itemize}
        \item \textit{weighting with modified propensity score estimators}: discretize $\hat{e}(x)$ to 
        \begin{equation*}
            \tilde{e}(x) = \frac{1}{M} \sum^M_{m=1} \sum^M_{m=1} \mathbf{1} \left\{ \frac{m}{M} \leq \hat{e}(x) \right\}
        \end{equation*}
        then use $\tilde{e}(x)$ as the propensity score in the weighting estimator leads to an estimator for the ATE \textbf{identical} to that obtained by using the blocking estimator with $\hat{e}(x)$ as the propensity score and $M$ blocks\footnote{With sufficiently large $M$, the blocking estimator is sufficiently close to the oiriginal weighting estimator, sharing first-order asymptotic properties. Hence a large number of blocks does little harm, with regard to asymptotic properties.}. 
    \end{itemize}
    \item \myhl[myblue]{\textbf{propensity scores as regressors}}: estimate the conditional expectation of $Y$ given $D$ and $e(X)$, define $$ v_d(e) = \mathbb{E}\left[ Y(d) \mid e(X)=e \right] \overset{\text{unconfoundedness}}{=} \mathbb{E}\left[Y \mid D=d, e(X)=e\right] $$
    given an estimator $\hat{v}_w(e)$, one can estimate the ATE as 
    \begin{equation*}
        \hat{\tau}_{regprop} = \frac{1}{N} \sum^N_{i=1} \left[ \hat{v}_1\left(e(X_i)\right) - \hat{v}_0 \left(e(X_i)\right) \right]
    \end{equation*}
\end{itemize}

There are 2 cases in practice when considering propensity score approaches
\begin{itemize}
    \item \textbf{propensity scores known}: all 3 methods are efficient, do not rely on high-dimensional nonparametric regressions, have attractive finite-sample properties
    \item \textbf{propensity scores unknown}: require high-dimensional nonparametric regression of the treatment indicator on the covariates. The relative merits of the 3 approaches will depend on whether the propensity score is more or less smooth than the regression functions, and whether additional information is available about either the propensity score or the regression functions.
\end{itemize}

\paragraph*{D. Mixed} Neither matching nor the propensity score methods directly address the correlation between the covariates and the outcome, incorporating the regression method may eliminate remaining bias and improve precision.

\begin{itemize}
    \item \myhl[myblue]{\textbf{Weighting and Regression}}: estimating $$ Y_i = \alpha + \tau\cdot D_i + \epsilon_i $$ with weights $$ \lambda_i = \sqrt{ \frac{D_i}{e(X_i)} + \frac{1-D_i}{1-e(X_i)} } $$
    the covariates are uncorrelated with the treatment indicator, making the weighted estimator consistent. To improve precision, add covariates 
    $$ Y_i = \alpha + \beta'X_i + \tau\cdot D_i + \epsilon_i $$
    which is doubly robust: consistent if either the regression model or the propensity score are specified correctly.
    \item \myhl[myblue]{\textbf{Blocking and Regression}}: least square estimator in block $m$ as $$ Y_i = \alpha_m + \tau_m\cdot D_i + \epsilon_i $$ using only units in block $m$. One can again add covariates and estimate $Y_i = \alpha_m + \beta_m' X_i + \tau_m\cdot D_i + \epsilon_i$.
    \item \myhl[myblue]{\textbf{Matching and Regression}}: the bias of the simple matching estimator can dominate the variance if the dimension of the covariates is too large, regression can help in this situation.
    
    Let $\hat{Y}_i(0)$ and $\hat{Y}_i(1)$ be the observed or imputed potential outcomes for unit $i$, the \textit{estimated potential} outcomes equal the \textit{observed} outcomes for some unit $i$ for its match $l(i)$, the bias $$ \mathbb{E}\left[\hat{Y}_i(1) - \hat{Y}_i(0)\right] - \left[Y_i(1)-Y_i(0)\right] $$
    arises from the fact that the covariates $X_i$ and $X_[l(i)]$ for units $i$ and $l(i)$ are not equal, although they are close because of the matching process. Focusing on the single-match case, define for unit $i$ 
    \begin{align*}
        \hat{X_i}(0) &= \begin{cases}
            X_i &D_i=0\\
            X_{l_1(i)}&D_1=1
        \end{cases} &
        \hat{X_i}(1) &= \begin{cases}
            X_{l_1(i)} &D_i=0\\
            X_i &D_1=1
        \end{cases}
    \end{align*}
    if the matching is exact, $\hat{X}_i(0)=\hat{X}_i(1)$ for each unit $i$. Suppose $D_1=1$, then $\hat{Y}_i(1)=Y_i(1)$, and $\hat{Y}_i(0)$ is an imputed value for $Y_i(0)$. This value is unbiased for $\mu_0\left(X_{l_1(i)}\right)$, but not necessarily for $\mu_0\left(X_i\right)$. Hence, $\hat{Y}_i(0)$ should be adjusted by an estimate of $\mu_0\left(X_i\right) - \mu_0\left(X_{l_1(i)}\right)$. Typically, these corrections are taken be to linear in the difference in the covariates for unit $i$ and its match, of the form $$\beta_0'\left[\hat{X}_i(1)-\hat{X}_i(0)\right] = \beta'_0\left(X_i - X_{l_1(i)}\right)$$
    then \citet{rubin1973use} proposed 3 modifications
    \begin{itemize}
        \item[1] using least squares, estimate $$ \hat{Y}_i\left(1\right) -\hat{Y}_(0) = \tau+\left[\hat{X}_i(1)-\hat{X}_i(0)\right]'\beta + \epsilon_i $$
        \item[2] estimate $\mu_0(x)$ directly by taking all control units and use lease squares to estimate $$ Y_i = \alpha_0 + \beta_0'X_i + \epsilon_i $$
        if unit $i$ is a control unit, or $Y_i=\alpha_1+\beta_1'X_i+\epsilon_i$ for the treated units\footnote{If the correction is done nonparametrically, the resulting matching estimator is consistent and asymptotically normal, with its bias dominated by the variance.}.
        \item[3] estimate the same regression function for the controls, but using \textbf{only those that are used as matches} for the treated units with weights corresponding to the number of times a control observation is used as a match.
        
        \textbf{\underline{CAUTIOUS}}: This approach can be less efficient due to dropping some control observations, but can likely avoid including outliners. 
    \end{itemize}
\end{itemize}

\paragraph*{E. Bayesian}
Bayesian approaches have not been applied in estimating ATE under unconfoundedness. They can be useful for a number of reasons,
\begin{itemize}
    \item \textbf{high-dimensionality}: Bayesian methods would allow researchers to include covariates with more or less informative prior distributions.
    \item \textbf{missing-at-random (MAR)}: multiple imputation methods often rely on a Bayesian approach for missing data.
\end{itemize}

\paragraph*{Estimating variances}
\begin{itemize}
    \item for population ATE, the variance of efficient estamators is $$ V^P = \mathbb{E} \left[ \frac{\sigma^2_1(X)}{e(X)} + \frac{\sigma^2_0(X)}{1-e(X)} + \left(\mu_1(X)-\mu_0(X)-\tau\right)^2 \right] $$
    which can be estimated by
    \begin{itemize}
        \item[1] brute force: estimate all 5 components $\sigma^2_0(x)$, $\sigma^2_1(x)$, $\mu_0(x)$, $\mu_1(x)$ and $e(x)$ using kernel methods or series.
        \item[2] either estimate regression functions $\mu_0(x),\mu_1(x)$ or the propensity score $e(x)$ using series/sieves. This can be interpreted as parametric estimators
        \item[3] bootstrapping: given the asymptotic linearity of the estimators, bootstrapping will lead to valid standard errors and CIs at least for the regression and propensity score methods. For matching, it's more complicated since bootstrapping introduces discreteness in the distribution, which will lead to ties.
    \end{itemize}
    \item for sample ATE, the appropriate (conservative) variance is $$ V^S = \mathbb{E}\left[\frac{\sigma^2_1(X)}{e(X)} + \frac{\sigma^2_0(X)}{1-e(X)}\right] $$
    which can be estimated by 
    \begin{itemize}
        \item[1] estimating the conditional moments of the outcome distributions, with the accompanying inherent difficulties.
        \item[2] matching variance estimator\footnote{The idea is that one need not actually estimate this variance consistently at all values of the covariates. One needs only the average of this variance over the distribution, weighted by the inverse of either $e(x)$ ot its complement $1-e(x)$.}: the key is to obtain a close-to-unbiased estimator for $\sigma^2_w(x)$. Suppose units $i$ and $j$ have $X=x$, then an unbiased estimator for $\sigma^2_1(x)$ is $$ \hat{\sigma}^2_1(x) = \frac{1}{2}(Y_i-Y_j)^2 $$
        this matching doesn't need to be exact, one can use the closest match within the set of units with the same treatment indicator. Let $v_m(i)$ be the $m$-th closest unit to $i$ with the same treatment indicator $W_{v_m(i)}=W_i$ and 
        $$
        \sum_{l\mid W_l=l,l\neq i} \mathbf{1} \left\{ \left\Vert X_l-x \right\Vert \leq \left\Vert X_{v_m(i)}-x \right\Vert \right\} = m
        $$
        which gives $M$ units with the same treatment indicator and approximately the same values for the covariates. The sample variance of the outcome variable for these $M$ units can then be used to estimate $\sigma^2_1(x)$, and similarly for the control variance function $\sigma^2_0(x)$. 
        \item[3] estimate the variance of the ATE as
        \begin{equation*}
            \hat{V}^S = \frac{1}{N} \sum^N_{i=1}\left( \frac{\hat{\sigma}^2_1(X_i)}{\hat{e}(X_i)} + \frac{\hat{\sigma^2_{0}(X_i)}}{1-\hat{e}(X_i)} \right)
        \end{equation*}
        for matching estimators, even estimation of the propensity score can be avoided:
        \begin{equation*}
            \hat{V}^E = \frac{1}{N}\sum^N_{i=1} \left(1+\frac{K_M(i)}{M}\right)\hat{\sigma}^2_{W_i}(X_i)
        \end{equation*}
        where $M$ is the number of matches and $K_M(i)$ is the number of times unit $i$ is used as a match.
    \end{itemize}
\end{itemize}

\paragraph*{Assessing the assumptions}
There assumption of unconfoundedness is not directly testable, since the distribution of $Y(0)$ for those who received the treatment and of $Y(1)$ for those who received the control are never in the data. But there are still 2 groups of ways to indirectly test the assumption:
\begin{itemize}
    \item estimating the casual effect of a treatment that is known \textbf{not to have} an effect: postulate a three-valued indicator $T_i\in\left\{-1,0,1\right\}$ for the groups of ineligibles, eligible non-participants and participants, and treatment indicator $W_i=\mathbf{1}\left\{T_i=1\right\}$. The extended unconfoundedness assumption is 
    $$ Y_i(0),Y_i(1) \bot T_i\mid X_i $$
    and a testable implication is 
    $$Y_i \bot \mathbf{1}\left\{T_i=0 \right\} \mid X_i,T_i\leq 0 $$
    \item estimating the casual effect of the treatment on a variable known to be \textbf{unaffected} by it, typically a pre-determined variable, which could eitehr me time-invariant, or more interestingly, a lagged outcome. One can test 
    $$ Y_{i,-1}\bot W_i\mid Y_{i,-2},\cdots,Y_{i,-T},Z_i $$
    which combines 2 assumptions\footnote{The test depends on the link between the 2 assumptions and the original unconfoundedness assumption. With a sufficient number of lags, unconfoundedness given all lags but one appears plausible, conditional on unconfoundedness given all lags, so the relevance of the test depends largely on the plausiblity of the stationarity and exchangeability assumption.}
    \begin{align*}
        Y_i(1),Y_i(0)\bot W_i &Y_{i,-1},\cdots,Y_{i,-(T-1)},Z_i &\text{unconfoundedness given only $T-1$ lags}\\
        f_{Y_{i,s(0)\mid Y_{i,s-1}(0)},\cdots,Y_{i,s-(T-1)}(0),Z_i,W_i}&\left(y_s\mid y_{s-1},\cdots, y_{s-(T-1)},z,w\right) &\text{stationarity and exchangeability}
    \end{align*}
\end{itemize}

next, the important question is how to \textbf{choose the covariates}, some concerns are 
\begin{itemize}
    \item some variables should not be adjusted for: the unconfoundedness assumption may apply with one set of covariates, but \textbf{not} with an expanded set. A covariate measured before the treatment was chosen should be safe to include in theory, but in practice, the covariates are often recorded as the same time as the outcomes
    \item the expected mean squared error may be reduced by ignoring those covariates that have only weak correlation with the treatment indicator and the outcomes: including a covariate in teh adjustment procedure will not lower the asymptotically precision of ATE but could reduce precision \textbf{in finite samples} if the covariates are not or only weakly correlated with outcomes and treatment indicators.
\end{itemize}

the second key assumption is the \textbf{overlapping}: the propensity score, i.e., the probability of receiving the active treatment, should be strictly between 0 and 1. In practice, there are 2 main issues
\begin{itemize}
    \item[\textbf{1}] \myhl[myblue]{\textbf{detect} the lack of overlapping}: there are several methods popularly used
    \begin{itemize}
        \item plot distributions of covariates by treatment groups: can be very difficult in \textbf{high-dimensional} cases
        \item non-parametrically estimate the distribution of the propensity score: one may wish to \textbf{undersmooth} the estimation by choosing a bandwidth smaller than optimal or including higher-order terms
        \item inspect the \textbf{worst} matches: for each component $k$ of the covariate vector, check $$\max_i\left\vert x_{i,k} - x_{l_1(i),k} \right\vert$$
        the maximum over all observations of the matching discrepancy. If it's large relative to the \textbf{sample standard deviation} of the $k$-th component of the coviarates, there would be a lack of overlapping.
    \end{itemize}
    \item[\textbf{2}] \myhl[myblue]{\textbf{address} the lack of overlapping}: given a lack of overlap, one can 
    \begin{itemize}
        \item conclude the ATE \textbf{cannot} be estimated with sufficient precision 
        \item focus on an average treatment effect that is estimable with greater accuracy, by \textit{\underline{dropping observations} with a \textbf{cutoff} of propensity scores}: one principle is to evaluate the weight of each unit as
        \begin{align*}
            \frac{1}{N\cdot \left[1-e(X_i)\right]}, &\text{ for treated units} & \frac{1}{N\cdot e(X_i)}, &\text{ for control units}
        \end{align*}
        and make sure the weights of all units are \textbf{upper-bounded} by a reasonable number\footnote{For example, set the cutoff as 0.05, s.t. no unit will have a weight of more than 5\% in the average. In a sample with 1000 units, with such a cutoff, only units with a propensity score outside $[0.02,0.98]$ will be ignored.}.
    \end{itemize}
    \item[\textbf{3}] \myhl[myblue]{\textbf{comparing} the 3 approaches}: in general, variance estimates \textbf{increase} when adding treated observations to a sufficiently-overlapping data set, approximately \textbf{unchanged} when adding control observations.
    \begin{itemize}
        \item \textbf{regression}: in general, adding observations with outlier values of the regressors leads to more precise estimates. If the added observations are \textbf{treated} units, the precision of the estimated control regression at thes outlier values will be lower; if in \textbf{control} units, such precision will increase.
        \item \textbf{matching}: adding \textbf{control} observations with outlier covariate values will likely not change the results, such they won't be useed as matches; but adding \textbf{treated} will bias the estimates, while the standard error would largely unaffected.
        \item \textbf{propensity-score}: if the propensity score of a \textbf{control} unit is \textbf{close to 0}, adding it would not cause much trouble; but adding a \textbf{control} unit with a propensity score \textbf{close to 1} would increase variance of an ATE estimator
    \end{itemize}
    over all, \textbf{matching} and \textbf{propensity-score}, as well as \textbf{kernel-based} regression methods, are better in coping with limited overlap than (semi-)parametric regression models. In practice, one should \textit{always} inspect histograms of the estimated propensity scores in both groups to assess whether limited overlap is an issue.
\end{itemize}

\citet{chernozhukov2017double} propose to a machine-learning based approach. They consider the case where treatment effects are fully heterogeneous, and the treatment variable $D\in \left\{0,1\right\}$, and model random vector $\left(Y,D,Z\right)$ as 
\begin{align*}
    Y&= g_0\left(D,Z\right)+\zeta & \mathbb{E}\left[\zeta \mid Z,D\right]&=0 & \text{outcome variable}\\
    D&= m_0\left(Z\right) + \nu & \mathbb{E}\left[\nu\mid Z\right] &=0 & \text{treatment assignment}
\end{align*}
which allows for general heterogeneity in treatment effect. The confounding factors $Z$ affect the treatment variable $D$ via propensity score $m_0(Z)\coloneq \mathbb{E}\left[D\mid Z\right]$, and the outcome variable via function $g_0(D,Z)$, both of these functions will be estimated via ML methods.

Then the ATE or ATT are 
\begin{align*}
    \theta_0 &= \mathbb{E}\left[g_0\left(1,Z\right),g_0\left(0,Z\right)\right] & \text{ATE}\\
    \theta_0 &= \mathbb{E}\left[g_0\left(1-Z\right)-g_0\left(0,Z\right)\mid D=1\right] & \text{ATT}
\end{align*}
conisder a score $\psi(W;\theta,\eta)$ that satisfies
\begin{align*}
    \mathbb{E}\psi \left(W;\theta_0,\eta_0\right) &=0 & \text{identification condition}\\
    \left.\partial_n\mathbb{E}\psi \left(W;\theta_0,\eta\right)\right\vert_{\eta=\eta_0} &=0 & \text{Neyman orthogonality condition}
\end{align*}
\citet{chernozhukov2017double} suggest employing 
\begin{itemize}
    \item for \myhl[myblue]{\textbf{ATE}}
    \begin{align*}
        \psi\left(W;\theta,\eta\right)&= \frac{D\left(Y-g(0,Z)\right)}{m} - \frac{m(Z)(1-D)\left(Y-g(0,Z)\right)}{\left(1-m(Z)\right)m} - \theta\frac{D}{m} \\
        \eta(Z) &\coloneq \left(g(0,Z),g(1,Z),m(Z)\right)\\
        \eta_0(Z) &\coloneq \left(g_0(0,Z),g_0(1,Z),m_0(Z)\right)
    \end{align*}
    \item for \myhl[myblue]{\textbf{ATT}}
    \begin{align*}
        \psi\left(W;\theta,\eta\right) &= \frac{D\left(Y-g(0,Z)\right)}{m} - \frac{m(Z)(1-D)\left(Y-g(0,Z)\right)}{\left(1-m(Z)\right)m} - \theta\frac{D}{m} \\
        \eta(Z) &\coloneq \left(g(0,Z),g(1,Z),m(Z),m\right)\\
        \eta_0(Z) &\coloneq \left(g_0(0,Z),g_0(1,Z),m_0(Z),\mathbb{E}[D]\right)
    \end{align*}
\end{itemize}
where $\eta(Z)$ is the nuisance parameter with true value of $\eta_0(Z)$ consisting of $P-$square integrable functions, mapping the support of $Z$ to $\mathbb{R}\times \mathbb{R} \times\left(\epsilon,1-\epsilon \right)$ for ATE and to $\mathbb{R}\times \mathbb{R} \times\left(\epsilon,1-\epsilon \right) \times\left(\epsilon,1-\epsilon\right)$ for ATT\footnote{All semiparametrically efficient scores satisfy Neyman orthogonality, but not vice versa. In some problems, one may consider inefficient orthogonal scores for robustness.}. 

\citet{chernozhukov2017double} propose to use cross-fitting to avoid overfitting and the Neyman orthogonality to reduce regularization and modeling biases of ML estimators $\hat{\eta}_0$, hence, \textbf{double debiasing}
\begin{algorithm}{Estimation with Orthogonal Scores by $K-$fold Cross-Fitting}{chernozhukov_doublyrobust_alg}
    \begin{itemize}
        \item[\textbf{S1}] Let $K$ be a fixed integer. Form a $K-$fold random partition of $\left\{1,\cdots,N\right\}$ by dividing it into equal parts $\left(I_k\right)^K_{k=1}$, each of size $n\coloneq N/K$. For each $I_k$, let $I^C_k$ denote all indices that are \textbf{not} in $I_k$
        \item[\textbf{S2}] Construct $K$ estimators
        $\check{\theta}_0\left(I_k,I^C_k\right)$, $k=1,\cdots,K$
        that employ the machine learning estimators 
        $$
        \hat{\eta}_0\left(I^C_k\right) = \left( \hat{g}_0\left(0,Z;I^C_k\right),\hat{g}_0\left(1,Z;I^C_k\right),\hat{m}_0\left(Z;I^C_k\right),\frac{1}{N-n}\sum_{i=\in I^C_k}D_i \right)'
        $$
        of the nuisance parameters $$ \eta_0(Z) = \left(g_0(0,Z),g_0(1,Z),m_0(Z),\mathbb{E}\left[D\right]\right)' $$
        and where each estimator $\check{\theta}_0\left(I_k,I^C_k\right)$ is defined as the root $\theta$ of $$ \frac{1}{n}\sum_{i\in I_k}\psi \left(W;\theta,\hat{\eta}_0\left(I^C_k\right)\right) =0 $$
        for the score $\psi$ for ATE and ATT respectively.
        \item{\textbf{S3}} Average the $K$ estimators to obtain the final estimator $$ \tilde{\theta}_0 = \frac{1}{K}\sum^K_{k=1}\check{\theta}_0\left(I_k,I^C_k\right) $$
        and an approximate standard error for this estimator is $\hat{\sigma}/\sqrt{N}$, where
        $\hat{\sigma}^2 = \frac{1}{N}\sum^N_{i=1}\hat{\psi}^2_i$, $\hat{\psi}\coloneq \psi\left(W_i;\tilde{\theta}_0,\hat{\eta}_0\left(I^C_{k(i)}\right)\right)$, $k(i) \coloneq \left\{k\in \left\{1,\cdots,K\right\}:i\in I_k\right\}$
        an approximate $\left(1-\alpha\right)\times 100\%$ Confidence interval is $$ \mathrm{CI}_n \coloneq \left[\tilde{\theta}_0 \pm \Phi^{-1}\left(1-\frac{\alpha}{2}\right)\frac{\hat{\sigma}}{\sqrt{N}} \right] $$
    \end{itemize}
    The \textbf{key assumptions} on the rate of estimating are 
    \begin{itemize}
        \item $\forall d\in\left\{0,1\right\}$, $\left\Vert \zeta \right\Vert _{P,2}\geq c$, $\left\Vert v \right\Vert _{P,2}\geq c$, and
        \begin{align*}
            \left\Vert g(d,Z) \right\Vert _{P,q} &\leq C & \left\Vert Y \right\Vert _{P,q} &\leq C & P\left(\epsilon \leq m_0 (Z) \leq 1-\epsilon\right) &=1 & P\left(\mathbb{E}_P\left[\zeta^2\mid Z\right]\leq C\right)&=1 
        \end{align*}
        \item the ML estimators based on a random subset $I^C_k$ of $\left\{1,\cdots,N\right\}$ of size $N-n$, $\forall N\geq 2K, d\in\left\{0,1\right\}$
        \begin{align*}
            \left\Vert \hat{g}_0\left(d,Z;I^C_k\right)-g_0(d,Z) \right\Vert _{P,2}\cdot \left\Vert \hat{m}_0\left(Z;I^C_k\right) -m_0(Z) \right\Vert _{P,2} &\leq \frac{\delta_n}{\sqrt{n}} \\
            \left\Vert \hat{g}_0\left(d,Z;I^C_k\right)-g_0(d,Z) \right\Vert _{P,2} + \left\Vert \hat{m}_0\left(Z;I^C_k\right) -m_0(Z) \right\Vert _{P,2} &\leq \delta_n
        \end{align*}
        and $P\left(\epsilon\leq \hat{m}_0\left(Z;I^C_k\right)\leq 1-\epsilon\right)=1$ with $P_P-$probability no less than $1-\Delta_n$\footnote{The conditions are fairly sharp, in the sense that for a sparse regression function $g_0$ and a propsensity function $m_0$ with sparsity indices $s^g\ll n,s^m\ll n$, and estimators $\hat{g}_0$ and $\hat{m}_0$ have sparsity indices of order $s^g$ and $s^m$, converging to $g_0$ and $m_0$ at the rates $\sqrt{s^g/n}$ and $\sqrt{s^m/n}$. The rate conditions in the assumption require $$ \sqrt{s^g/n} \cdot \sqrt{s^m/n} \Leftrightarrow s^gs^m\ll \sqrt{n} $$ which is much weaker than the without-sample-splitting condition $\left(s^g\right)^2 + \left(s^m\right)^2\ll n$}.
    \end{itemize}
\end{algorithm}

the estimator from Algorithm \ref{algm:chernozhukov_doublyrobust_alg} achieves asymptotic normality:
$\sigma^{-1}\sqrt{N}\left(\tilde{\theta}_0-\theta_0\right) \rightarrow \mathcal{N}\left(0,1\right)$, where $\sigma^2 = \mathbb{E}_P\left[\psi^2\left(W;\theta_0,\eta_0\left(Z\right)\right)\right]$, the results hold with $\hat{\sigma}^2$. The confidence regions based upon $\tilde{\theta}_0$ have uniform asymptotic validity $$ \sup_{P\in \mathcal{P}}\left\vert P\left(\theta_0\in CI_n\right)-\left(1-\alpha\right) \right\vert \rightarrow 0$$.

\textbf{Uncertainty due to sample-splitting}: although sample partitions have no impact on estimation results asymptotically, but may be important in finite samples. \citet{chernozhukov2017double} proposes to repeat the procedure $S$ times and repartition each time, get a set of estimates $\tilde{\theta}^s_0$.
Then, consider 2 quantities:
\begin{itemize}
    \item sample \textbf{average} of the $S$ estimates: $\tilde{\theta}_0^{\text{mean}}$
    \item sample \textbf{median} of the $S$ estimates: $\tilde{\theta}_0^{\text{median}}$
\end{itemize}
$\tilde{\theta}_0^{\text{median}}$ is less affected by extreme estimates, hence more robust. But asymptotically, the specific random partition is irrelevant, hence $\tilde{\theta}_0^{\text{mean}}$ and $\tilde{\theta}_0^{\text{median}}$ should be close.
\citet{chernozhukov2017double} also introduce the repsective standard error measures:
\begin{align*}
    \hat{\sigma}^{\text{mean}} &= \sqrt{\frac{1}{S}\sum^S_{s=1}\left(\hat{\sigma}^2_s+\underbrace{\left(\tilde{\theta}_0^s-\frac{1}{S}\sum^S_{j=1}\tilde{\theta}^j_0\right)^2}_{\text{variation of sample splitting}}\right)} & \hat{\sigma}^{\text{median}} &= \text{median} \left\{ \sqrt{\hat{\sigma}^2_i + \underbrace{\left(\hat{\theta}_i-\hat{\theta}^{\text{median}}\right)^2}_{\text{variation of sample splitting}} } \right\}^S_{i=1}
\end{align*}

\textbf{In practice}, one might want to consider the following
\begin{itemize}
    \item for \textbf{extreme propensity score}, set cutoffs close to 0 and 1 
    \item for \textbf{nuisance function estimation}, some typical methods are tree-based methods (Random Forest, Regression Tree, Boosting), or $l_1-$penalization method (Lasso), or neural network.
    One can also use the weighted averages of these methods s.t. the average gives the lowest mean squared out-of-sample prediction errors.
\end{itemize}

\citet{athey2018approximate} proposed a similar framework, focusing on high dimensional linear models. They showed that in linear models, it is enough to correct for linear biases in two steps:
\begin{itemize}
    \item[\textbf{S1}] Fit a regularized linear model for the outcome given the features separately in the 2 treatment groups 
    \item[\textbf{S2}] reweight the first-stage residuals by using weights that approximately balance all the features between the treatment and control groups
\end{itemize}
\citet{athey2018approximate} showed that it is often possible to achieve approximate balance under reasonable assumptions and that approximate balance suffices for eliminating bias due to regularization, combined with a lasso regression adjustment.

Consider the conditional ATE for the treated sample
\begin{equation*}
    \tau = \frac{1}{n_t} \sum_{i:D_i=1} \mathbb{E}\left[Y_i(0)-Y_i(1)\mid X_i\right]
\end{equation*}
In addition to unconfoundedness, they also assume \textbf{linearity}:
\begin{align*}
\mu_c(x) &= \mathbb{E}\left[Y_i(0)\mid X=x\right] = x\beta_c & \mu_t(x) = \mathbb{E}\left[Y_i(1)\mid X=x\right] = x\beta_t
\end{align*}
for all $x\in \mathbb{R}^p$. This assumption is strong, but generally used in high-dimension cases. Given linearity, we have 
\begin{align*}
    \tau &=\mu_t-\mu_c & \mu_t &=\bar{X}_t\beta_t & \mu_c&=\bar{X}_t\beta_c &\bar{X}_t &= \frac{1}{n_t}\sum^n_{i=1}\mathbf{1}\left\{D_i=1\right\}X_i
\end{align*}
the main focus of this paper is to estimate $\mu_c$ when $p$ is large, combining two approaches:
\begin{itemize}
    \item \textbf{weighted} estimation: weighting control to mimic treatment
    \item \textbf{regression} adjustment: estimate $\beta_c$ using control observations and get $\hat{\mu}_c=\bar{X}_t\hat{\beta}_c$
\end{itemize}
both of them perform poorly in high-dimension setting alone. \citet{athey2018approximate} propose a meta-algorithm that estimate 
\begin{equation*}
    \hat{\mu}_c = \bar{X}_t \hat{\beta}_c + \sum_{i:W_i=0}\gamma_i \left(Y_i^{obs}-X_i\hat{\beta}_c\right)
\end{equation*}
where $\bar{X}_t \hat{\beta}_c$ captures the strong signals, and $\gamma_i$ rebalance the residuals and effectively extract left-over signals. It satisfies 
\begin{equation*}
    \left\vert \hat{\mu}_c-\mu_c \right\vert \leq \underbrace{\left\Vert \bar{X}_t-X^T_c\gamma \right\Vert _{\infty}\left\Vert \hat{\beta}_c-\beta_c \right\Vert _1}_{\text{dimensionality bias}} + \underbrace{\left\vert \Sigma_{i:W_i=0}\gamma_i\epsilon_i \right\vert}_{\text{variance}} 
\end{equation*}
the dimensionality bias should be expected to scale as 
\begin{align*}
    \left\Vert \bar{X}_t-X^T_c\gamma \right\Vert _{\infty} &=O\left(\sqrt{\log (p)/n}\right) & \left\Vert \hat{\beta}_c-\beta_c \right\Vert _1 &=O\left(k\cdot \sqrt{\log (p)/n} \right)
\end{align*}
then in a sufficiently sparse setting, i.e., $k$ is sufficiently small, the bias could be \textbf{variance dominated}. The estimation procedure is 
\begin{itemize}
    \item[S1] compute the \textbf{positive} approximately balancing weights $\gamma$ to make the mean of the reweighted control sample $X^T_c\gamma$ match the treated sample mean $\bar{X}_t$ as closely as possible, 
    \begin{equation*}
        \gamma = \arg\min_{\tilde{\gamma}} \left\{ \left(1-\zeta\right)\left\Vert \tilde{\gamma} \right\Vert^2_2 + \zeta \left\Vert \bar{X}_t- X^T_c\tilde{\gamma} \right\Vert^2_{\infty}\text{ s.t. }\sum_{i:W_i=0}\tilde{\gamma}_i=1,0\leq \tilde{\gamma}_i\leq n_c^{-2/3} \right\}
    \end{equation*}
    $\gamma$ is constrained to be positive to prevent aggresive extrapolating.
    \item[S2] fit $\beta_c$ in the linear model by using a lasso or an elastic net to achieve sufficiently good risk bounds under $L_1-$risk
    \begin{equation*}
        \hat{\beta}_c = \arg \min_{\beta} \left[ \sum_{i:W_i=0} \left(Y_i^{obs}-X_i\beta\right)^2 + \lambda\left\{(1-\alpha)\left\Vert \beta \right\Vert^2_2 + \alpha\left\Vert \beta \right\Vert _1 \right\} \right]
    \end{equation*}
    \item[S3] estimate the ATE as 
    \begin{equation*}
        \hat{\tau} = \bar{Y}_t - \left\{ \bar{X}_t\hat{\beta}_c + \sum_{i:W_i=0} \gamma_i\left(Y^{obs}_i-X_i\hat{\beta}_c\right) \right\}
    \end{equation*}
\end{itemize}
an analogous estimator for the ATE $\mathbb{E}\left[Y(1)-Y(0)\right]$ can also be constructed as 
\begin{equation*}
    \hat{\tau}_{ATE} = \bar{X}\left(\hat{\beta}_t-\hat{\beta}_c\right) + \sum_{i:W_i=1} \gamma_{t,i}\left(Y^{obs}_i-X_i\hat{\beta}_t\right) -\sum_{i:W_i=0} \gamma_{c,i}\left(Y_i^{obs}-X_i\hat{\beta}_c\right)
\end{equation*}
where 
\begin{equation*}
    \gamma_t = \arg\min_{\tilde{\gamma}} \left\{ \left(1-\zeta\right)\left\Vert \tilde{\gamma} \right\Vert^2_2 + \zeta \left\Vert \bar{X}- X^T_t\tilde{\gamma} \right\Vert^2_{\infty}\text{ s.t. }\sum_{i:W_i=0}\tilde{\gamma}_i=1,0\leq \tilde{\gamma}_i\leq n_c^{-2/3} \right\}
\end{equation*}
and $\gamma_c$ is constructed similarly.

There are 3 key features of this algorithm:
\begin{itemize}
    \item[(a)] direct covariance adjustment based on the outcome data with regularization to deal with the \textbf{high dimensionality}
    \item[(b)] \textbf{weighting} using the relationship between the treatment and the features
    \item[(c)] the weights are based on \textbf{direct measures of imbalance} rather than on propensity score estimates
\end{itemize}

In comparison with doubly robust estimators,
\begin{itemize}
    \item doubly robust estimators estimate the outcome model and the propensity score, this method instead relies on linearity assumption twice
    \item doubly robust estimators are preferable given sufficient sparsity and well-specified propensity models
    \item this method is not estimating propensity score, but just balancing, hence substantially easier
\end{itemize}

\paragraph{Unconfoundedness versus DiD/TWFE}
The unconfoundedness assumption and TWFE model validate different non-nested comparisons:
\begin{itemize}
    \item \myhl[myblue]{\textbf{TWFE}} model assumes: the treated differs from the control in unobserved characteristics that are potentially \textbf{correlated} with a persistent component of the outcomes (the fixed effects $\alpha_i$)
    \item \myhl[myblue]{\textbf{Unconfoundedness}} assumes: the selection is based \textbf{solely on past} rather than future outcomes
\end{itemize}
The literature in general does not provide a lot of guidance on the choice between the two strategies\footnote{\citet{xu2023causal} touches on this topic by comparing \textbf{strict exogeneity} (TWFE) and \textbf{sequential ignorability} (unconfoundedness).}.
In two cases, unconfoundedness and TWFE lead to similar results:
\begin{itemize}
    \item the averages of previous period outcomes are similar for the control and the treatment group 
    \item the avearge in the control group does not change much over time
\end{itemize}
when the control group changes over time, and the control group and treatment group differ in the initial period, then TWFE and unconfoundedness give different results. The differences can be bounded under additional assumptions. Consider the case of 2 periods, where everyone in the first period with treatment equal to $0$, and a positive true treatment effect $\beta$. 
\begin{itemize}
    \item If treatment is correlated with an unobserved individual fixed effects $a_i$, and outcomes are 
    \begin{align*}
        Y_{it} =& a_i+\beta D_{it}+\epsilon_{it} & Y_{it-1} &=a_i + \epsilon_{it-1}
    \end{align*}
    where $\epsilon_{it}$ is serially uncorrelated, and uncorrelated with $a_i$ and $D_{it}$. If one controls for $Y_{it-1}$ but ignore fixed effects, that is, estimate $Y_{it}$ on the residual from a regression of $D_{it}$ on $Y_{it-1}$, the resulting estimator is $\frac{\mathrm{Cov}\left(Y_{it},D_{it}-\gamma Y_{it-1}\right)}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)}$.
    Substituting $a_i\equiv Y_{it-1}-\epsilon_{it-1}$, get the real $$ Y_{it} = Y_{it-1} + \beta D_{it}+\epsilon_{it}-\epsilon_{it-1} $$
    then the estimator controlling unconfoundedness only is 
    \begin{align*}
        \frac{\mathrm{Cov}\left(Y_{it},D_{it}-\gamma Y_{it-1}\right)}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)} = \beta - \frac{\mathrm{Cov}\left( \epsilon_{it-1},D_{it}-\gamma Y_{it-1} \right)}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)} = \beta + \gamma\cdot \frac{\sigma^2_{\epsilon}}{\mathrm{Var}\left(D_{it}-\gamma Y_{it-1}\right)}
    \end{align*}
    if $ Y_{it-1} $ is larger in the control group, meaning that $\gamma<0$, just assumming unconfoundedness and adjusting for the lagged outcome will overestimate the true effects.
    \item If the correct specification is instead $$ Y_{it}=\alpha + \theta Y_{it-1}+\beta D_{it}+\epsilon_{it} $$
    where $\epsilon_{it}$ is serially uncorrelated. Just run a TWFE (in this case, first-difference) model, get an estimator $ \frac{\mathrm{Cov}\left(Y_{it}-Y_{it-1},D_{it}\right)}{\mathrm{Var}\left(D_{it}\right)} $, plug in $Y_{it}-Y_{it-1}=\alpha + \left(\theta-1\right)Y_{it-1}+\beta D_{it}+\epsilon_{it}$, get 
    \begin{align*}
        \frac{\mathrm{Cov}\left(Y_{it}-Y_{it-1},D_{it}\right)}{\mathrm{Var}\left(D_{it}\right)} = \beta + \left(\theta-1\right)\cdot \frac{\mathrm{Cov}\left(Y_{it-1},D_{it}\right)}{\mathrm{Var}\left(D_{it}\right)}
    \end{align*}
    in general $\theta\in \left(0,1\right)$ for stationary, if $Y_{it-1}$ is larger in control group, TWFE will overestimate the true effects.
\end{itemize}

In the more general setting, to estimate $\mu_0 = \mathbb{E}\left[Y_{i,t+1}(0)\mid G_i=1\right]$, 
we are comparing 
\begin{align*}
    \tilde{\mu}_{0,TWFE} &= \mathbb{E}\left(Y_{it}\mid G_i=1\right) + \mathbb{E}\left(Y_{i,t+1}\mid G_i=0\right) - \mathbb{E}\left(Y_it\mid G_i=0\right) & \text{TWFE}\\
    \tilde{\mu}_{0,uncf} &= \mathbb{E}\left[\mathbb{E}\left(Y_{t+1}\mid G=0,Y_t\right)\mid G=1\right] = \int \mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)F_{Y_t}\left(\mathrm{d}y\mid G=1\right) & \text{unconfoundedness}
\end{align*}
the difference between the two is then
\begin{equation*}
    \tilde{\mu}_{0,uncf}-\tilde{\mu}_{0,TWFE} = \int \Delta (y) F_{Y_t}\left(\mathrm{d}y\mid =1\right) - \int \Delta(y)F_{Y_t}\left(\mathrm{d}y\mid G=0\right)
\end{equation*}
where $\Delta (y)=\mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)-y = \mathbb{E}\left(Y_{t+1}-Y_t\mid G=0,Y_t=y\right)$ equals the expectation of change in the outcome conditioning on the lagged outcome in the \textbf{control} group.
\citet{ding2019bracketing} establishes that the relative magnitude of $\tilde{\mu}_{0,uncf}$ and $\tilde{\mu}_{0,TWFE}$ depends 
\begin{itemize}
    \item[(a)] $\mathbb{E}\left(Y_{t+1}-Y_t\right)$ conditional on $Y_t$ in the \textbf{control} group: the dependence of the outcome on the lagged outcome
    \item[(b)] difference between the distribution of $Y_t$ in the \textbf{treated} versus \textbf{control} group: the dependence of the treatment assignment on the lagged outcome
\end{itemize}
Assuming stationarity, $$ \frac{\partial \mathbb{E}\left(Y_{t+1}\mid G=0,Y_t=y\right)}{\partial y}<1,\forall y $$we have
\begin{itemize}
    \item if $Y_t \bot G $, or equivalently, $F_{Y_t}\left(y\mid G=1\right) = F_{Y_t}\left(y\mid G=0\right)$
    \item if $F_{Y_t}(y\mid G=1)\geq F_{Y_t}(y\mid G=0), \forall y$, $\tilde{\mu}_{0,TWFE}\leq \tilde{\mu}_{0,uncf}$, thus $\tilde{\tau}_{TWFE}\geq \tilde{\tau}_{uncf}$
    \item if $F_{Y_t}(y\mid G=1)\leq F_{Y_t}(y\mid G=0), \forall y$, $\tilde{\mu}_{0,TWFE}\geq \tilde{\mu}_{0,uncf}$, thus $\tilde{\tau}_{TWFE}\leq \tilde{\tau}_{uncf}$
\end{itemize}
and both estimation could be biased for the true ${\tau}_{ATT}$. 

\newpage
\bibliographystyle{plainnat}
\bibliography{ref.bib}

\end{document}